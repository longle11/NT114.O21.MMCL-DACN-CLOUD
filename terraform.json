{
  "version": 4,
  "terraform_version": "1.3.0",
  "serial": 1,
  "lineage": "20f58a92-82ea-2125-4513-8a67e2e0a701",
  "outputs": {
    "aws_iam_openid_connect_provider_extract_from_arn": {
      "value": "oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358",
      "type": "string"
    },
    "cluster_certificate_authority_data": {
      "value": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJQUtDMVpabTdERFl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRFeU1qQXhOREE0TUROYUZ3MHpOREV5TVRneE5ERXpNRE5hTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUM5TWVLbFM4dzBlb1RiR0Z5aVFXT0hpcU5CU3hoeHUzS3ZMTnBSRkFGSlBLMDZHelNJeGYrVEQxMXQKQTJiSWN2NDFyM3JXQXZXb2MvSHlPSk9IcDFyMlNTNS9weXJXb0tDRkpVSVZJTDFrN0ZOUE1nem5jWHJIbC9aTgpBMUVLMGlCbkhPS1FpeGNjTWQ1U1dPQWNobWNYVXdOUlJyZG9ENEN4YzlsMjBvUk5GekNjQStlRWxiYXNqL2R4CnE1QlVCNGFsVHE3VkE5NzNFQ2xNSm5NbkFYcldOcmFqazNSc2g2SHNFbU5Lek9YZ0MzUndiV1UxUWxaYW9kR0QKcHNzOHBZa1h2ekJ6WXkrVW8raDdDaXh6MjFVRFQ5M3c0RGRhUUUrWkxlRVR4bGVjMU5LRi9TTm81d0JiYkFFVQpEQjhSR0VmUjhGeE1kTTV2S0hTRWI3NnpIaUNwQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJUam42MGZjWVlkUlFmQ3VOYW51ODNtSy9Udk16QVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQUJld3pubnV0OApuekJ5a1orYTFoaUprcWNhZ2NMWmdyZ0lVYXR5NDNFT2V5ZGowTTFPV3BIbHBVejh1dm45K1RQYVRqZHRiUnN1Cmh1N0ExQnhSRk1jT1cyUFY0VDFzNzM0OHpnaXJOSnoyRno2S0d3SVZyWXRkUko3TnBpUUxKM3lpZmNyKzZPNS8KWEJXcnpjQ2UzdU5rWjdIYnVJdjNqWDlYWFVaMWVoRnJibVF2QTBIYlpTSFlOcTJhWlJ2L3JtQVN2ZVNNRjMwWQpOWFF2NmtBK1pSQnptUWsxYWZsL0xLUURnM1NnZnZ5L1lQMmp1SlNHWkE1clR4ZHJaeUJ4aVJ3QVpkTGFlUFc2CmJkaEdpYU41V1VSdldkVFdLOVVVY3ZLMldOWDBtbGlacDY3TWpZVTAzWVlaZ2QvbGZIVkMxUnVFcS9MU2RPK3AKNXZyNVVHWDRZNzJ4Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K",
      "type": "string"
    },
    "cluster_endpoint": {
      "value": "https://BD9B8CBB7DD9D935201147689578F358.gr7.us-east-1.eks.amazonaws.com",
      "type": "string"
    },
    "cluster_iam_role_arn": {
      "value": "arn:aws:iam::905418291613:role/dev-eks-cluster-role",
      "type": "string"
    },
    "cluster_oidc_issuer_url": {
      "value": "https://oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358",
      "type": "string"
    },
    "cluster_primary_security_group_id": {
      "value": "sg-07f57fddd34441d9f",
      "type": "string"
    },
    "cluster_version": {
      "value": "1.30",
      "type": "string"
    },
    "ebs_iam_role_arn_output": {
      "value": "{\"Statement\":[{\"Action\":\"sts:AssumeRoleWithWebIdentity\",\"Condition\":{\"StringEquals\":{\"oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358:sub\":\"system:serviceaccount:kube-system:ebs-csi-controller-sa\"}},\"Effect\":\"Allow\",\"Principal\":{\"Federated\":\"arn:aws:iam::905418291613:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358\"},\"Sid\":\"\"}],\"Version\":\"2012-10-17\"}",
      "type": "string"
    },
    "ebs_iam_role_output": {
      "value": "arn:aws:iam::905418291613:role/dev-amazonEks-ebs-iam-role",
      "type": "string"
    },
    "eks_cluster_arn": {
      "value": "arn:aws:eks:us-east-1:905418291613:cluster/ekscluster",
      "type": "string"
    },
    "eks_cluster_id": {
      "value": "ekscluster",
      "type": "string"
    },
    "ingress_controller_iam_role_output_arn": {
      "value": "arn:aws:iam::905418291613:role/dev-ingress-kong-controller-role",
      "type": "string"
    },
    "ingress_controller_iam_role_output_assume_role_policy": {
      "value": "{\"Statement\":[{\"Action\":\"sts:AssumeRoleWithWebIdentity\",\"Condition\":{\"StringEquals\":{\"oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358:aud\":\"sts.amazonaws.com\",\"oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358:sub\":\"system:serviceaccount:kong:kong-ingress-controller\"}},\"Effect\":\"Allow\",\"Principal\":{\"Federated\":\"arn:aws:iam::905418291613:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358\"},\"Sid\":\"\"}],\"Version\":\"2012-10-17\"}",
      "type": "string"
    },
    "key_name": {
      "value": "keypair",
      "type": "string"
    },
    "node_group_private_arn": {
      "value": "arn:aws:eks:us-east-1:905418291613:nodegroup/ekscluster/dev-eks-nodegroup-private/26c9f237-1ac7-c5b2-7f1d-878a8564a0eb",
      "type": "string"
    },
    "node_group_private_id": {
      "value": "ekscluster:dev-eks-nodegroup-private",
      "type": "string"
    },
    "private_key": {
      "value": "-----BEGIN RSA PRIVATE KEY-----\nMIIEpAIBAAKCAQEAxRp96pEIgFFsFfqkpeS9x++Wm9MUOqjTMwQGBAFCvaLWvuIQ\n/760rYxmVzQp01kwNFmrFojyQiDcNDmDCQb4X6Nt4PKXPJOnOgJy1fAwY9lIRRO/\ndeQUTMZGrAZLhiaf77rEM7ZhEzekiomqk4stpKJkAFaLuMZVRLf94of9Il+9Tboi\nyfmsdv1nY34lEZ09ij2d9M44BnaKppOP6qJnk4sTaIbEVqvVsRgHlcZF4emf/2nA\nKfFZLys8zTwh0p3pLkAg3tyrUVDzdOzz0QeZeCpsciiukO1LP0j5VSkygV55OW+Y\nVpAtqAjGBHtrR2BI15AqozXZxwp//hheu7U//wIDAQABAoIBAG2OyCa17w/EMlwV\nMae0eXjEx+ZdcYUZiit1NRLM0JgwB0WayvAcxLkirVATgpwjmxO+M8cW3/G56ixt\nO1StNymR4eWDcmUNfYpVChe4pm0ZvX4H+AhDerUiEO5VoW0z0vqhyaUk3tibuS34\nT4QrfHdifEtjz84pEARecRSVmjKcIqDuQNz56ui93hHXg3s8heXC/wKDIAqnfQib\nxidT1qgJnq9yz/U1Tglamhf2fdF4qvRPTekPuSZeYMn0tVGPk+TnPDCANJBoFs5Z\nkmuOfolTi4WtEqHJnQDIBS5Gs2kmNsJihnU8irMc1JeePxiB7UkGFtIT7S5FTnJ9\nT1gbzmECgYEA7tLxOkPs3uQGWrUb6u+oki7zHVkoKmg5YAdpLlM5BxT7h8QeJ026\nur2m3Q/FnXjiipIXUGGmD6tl4/RsiVpa6KUcHdEzpiEFGiKyiw0G0t1+NaUisuWB\nwWvhX5kFfrg725fI7vQNlWwXLq9j9Hvmr3AbSPJ0nMNaqT/Ul4ThmmUCgYEA00dr\n0pph3YzU4DZ7fSf/2WnKJURoVVCaC4DFz7vUGPuX2NNQaie95Mq74xPJWjxmNK3D\nT9GH58nv99YLrBpWz0FMCIIz2WqtOnP8ZINX0YpTdsygKmWXrhI5UTjzJN/PC20F\nBZaMmjRNEF84MYMHF/a9J3V6qKISFD/3libduJMCgYA14b2vCZgtiYCtgmL0FHIR\n2SncmzrmpnEba1CdtQUOxfsh3Gt5Lp5Md3FoOqC5MIbcAK3l4sIWkvMABStfqdUM\n3AOF7qcaeiSuitmBacT6WiKZc9JqTkGCJBbK/Lkyp9pJZutcjg9qgOKSE9wXUsTv\nG/nr8VfB7olL2izaqo5DyQKBgQDSU/V40QvAHKUHF/XTYHCJJZGBjBo5ZGYWMXm7\nFYdN35kI10TkiO+3xkNJCbeXRy2QH7Oh0Dt1ekiT9tfj2sy0wpJIniWzuUj+Odz2\nyvIvvX7dc1O3IS/gsG+y9fjHqq5Y94zaRUdvc8WMGN6+G2yCZLVlhMQ5AC7zTcLS\nDZsIVQKBgQCrroJCXmldFZ1SKN5iURzTImMypI37fF3cCDCui+4uGLTbmmGrvixz\nwN8HIraLDaITQOdQy6XDKltQfOwbdNRf37+uCqT1+4ujqv6IKdinKyeh5RW0JVy5\nN2NUVnkulUKRVdqWJu9HKHC93KaRnydv+7cUzRdC2y14RI9tpxv6Eg==\n-----END RSA PRIVATE KEY-----\n",
      "type": "string",
      "sensitive": true
    }
  },
  "resources": [
    {
      "mode": "data",
      "type": "aws_availability_zones",
      "name": "available",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "all_availability_zones": null,
            "exclude_names": null,
            "exclude_zone_ids": null,
            "filter": [
              {
                "name": "opt-in-status",
                "values": [
                  "opt-in-not-required"
                ]
              }
            ],
            "group_names": [
              "us-east-1"
            ],
            "id": "us-east-1",
            "names": [
              "us-east-1a",
              "us-east-1b",
              "us-east-1c",
              "us-east-1d",
              "us-east-1e",
              "us-east-1f"
            ],
            "state": null,
            "timeouts": null,
            "zone_ids": [
              "use1-az1",
              "use1-az2",
              "use1-az4",
              "use1-az6",
              "use1-az3",
              "use1-az5"
            ]
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_caller_identity",
      "name": "current",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "account_id": "905418291613",
            "arn": "arn:aws:iam::905418291613:user/cloud_user",
            "id": "905418291613",
            "user_id": "AIDA5FTZCXGO2OPJDELLC"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_eks_cluster_auth",
      "name": "eks_cluster_auth",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "ekscluster",
            "name": "ekscluster",
            "token": "k8s-aws-v1.aHR0cHM6Ly9zdHMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vP0FjdGlvbj1HZXRDYWxsZXJJZGVudGl0eSZWZXJzaW9uPTIwMTEtMDYtMTUmWC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBNUZUWkNYR09XR05MV0lFUCUyRjIwMjQxMjIwJTJGdXMtZWFzdC0xJTJGc3RzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEyMjBUMTQyOTI5WiZYLUFtei1FeHBpcmVzPTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JTNCeC1rOHMtYXdzLWlkJlgtQW16LVNpZ25hdHVyZT04NWRmYTk5ZjU0OTA1NmM1MmMzODZkMGExNWM5NDc0MDEyOTU0ZWU5YjZjN2JlMDQ1ZjU0YmU2ZWEzZDdkNDQ4"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_elb_hosted_zone_id",
      "name": "elb_hosted_zone",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "Z35SXDOTRQ7X7K",
            "region": "us-east-1"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_elb_hosted_zone_id",
      "name": "main",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "Z35SXDOTRQ7X7K",
            "region": null
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_partition",
      "name": "current",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "dns_suffix": "amazonaws.com",
            "id": "aws",
            "partition": "aws",
            "reverse_dns_prefix": "com.amazonaws"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "http",
      "name": "iam_policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/http\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "body": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateSnapshot\",\n        \"ec2:AttachVolume\",\n        \"ec2:DetachVolume\",\n        \"ec2:ModifyVolume\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeSnapshots\",\n        \"ec2:DescribeTags\",\n        \"ec2:DescribeVolumes\",\n        \"ec2:DescribeVolumesModifications\",\n        \"ec2:EnableFastSnapshotRestores\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateTags\"\n      ],\n      \"Resource\": [\n        \"arn:*:ec2:*:*:volume/*\",\n        \"arn:*:ec2:*:*:snapshot/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteTags\"\n      ],\n      \"Resource\": [\n        \"arn:*:ec2:*:*:volume/*\",\n        \"arn:*:ec2:*:*:snapshot/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateVolume\"\n      ],\n      \"Resource\": \"arn:*:ec2:*:*:volume/*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"aws:RequestTag/ebs.csi.aws.com/cluster\": \"true\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateVolume\"\n      ],\n      \"Resource\": \"arn:*:ec2:*:*:volume/*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"aws:RequestTag/CSIVolumeName\": \"*\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateVolume\"\n      ],\n      \"Resource\": \"arn:*:ec2:*:*:snapshot/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteVolume\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteVolume\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/CSIVolumeName\": \"*\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteVolume\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/kubernetes.io/created-for/pvc/name\": \"*\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteSnapshot\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/CSIVolumeSnapshotName\": \"*\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteSnapshot\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n        }\n      }\n    }\n  ]\n}\n",
            "ca_cert_pem": null,
            "id": "https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json",
            "insecure": null,
            "method": null,
            "request_body": null,
            "request_headers": {
              "Accept": "application/json"
            },
            "request_timeout_ms": null,
            "response_body": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateSnapshot\",\n        \"ec2:AttachVolume\",\n        \"ec2:DetachVolume\",\n        \"ec2:ModifyVolume\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeSnapshots\",\n        \"ec2:DescribeTags\",\n        \"ec2:DescribeVolumes\",\n        \"ec2:DescribeVolumesModifications\",\n        \"ec2:EnableFastSnapshotRestores\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateTags\"\n      ],\n      \"Resource\": [\n        \"arn:*:ec2:*:*:volume/*\",\n        \"arn:*:ec2:*:*:snapshot/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteTags\"\n      ],\n      \"Resource\": [\n        \"arn:*:ec2:*:*:volume/*\",\n        \"arn:*:ec2:*:*:snapshot/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateVolume\"\n      ],\n      \"Resource\": \"arn:*:ec2:*:*:volume/*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"aws:RequestTag/ebs.csi.aws.com/cluster\": \"true\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateVolume\"\n      ],\n      \"Resource\": \"arn:*:ec2:*:*:volume/*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"aws:RequestTag/CSIVolumeName\": \"*\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateVolume\"\n      ],\n      \"Resource\": \"arn:*:ec2:*:*:snapshot/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteVolume\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteVolume\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/CSIVolumeName\": \"*\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteVolume\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/kubernetes.io/created-for/pvc/name\": \"*\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteSnapshot\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/CSIVolumeSnapshotName\": \"*\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DeleteSnapshot\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n        }\n      }\n    }\n  ]\n}\n",
            "response_body_base64": "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",
            "response_headers": {
              "Accept-Ranges": "bytes",
              "Access-Control-Allow-Origin": "*",
              "Cache-Control": "max-age=300",
              "Content-Security-Policy": "default-src 'none'; style-src 'unsafe-inline'; sandbox",
              "Content-Type": "text/plain; charset=utf-8",
              "Cross-Origin-Resource-Policy": "cross-origin",
              "Date": "Fri, 20 Dec 2024 14:29:26 GMT",
              "Etag": "W/\"39407821656c9d288093bd2e93ad0989232fbfd851a6b80d135f7cde617de83c\"",
              "Expires": "Fri, 20 Dec 2024 14:34:26 GMT",
              "Source-Age": "18",
              "Strict-Transport-Security": "max-age=31536000",
              "Vary": "Authorization,Accept-Encoding,Origin",
              "Via": "1.1 varnish",
              "X-Cache": "HIT",
              "X-Cache-Hits": "1",
              "X-Content-Type-Options": "nosniff",
              "X-Fastly-Request-Id": "99aa952062711638b26637aa8d25c82be7a75a11",
              "X-Frame-Options": "deny",
              "X-Github-Request-Id": "E957:312663:D66861:E81A5C:676579F2",
              "X-Served-By": "cache-bur-kbur8200068-BUR",
              "X-Timer": "S1734704966.496945,VS0,VE1",
              "X-Xss-Protection": "1; mode=block"
            },
            "retry": null,
            "status_code": 200,
            "url": "https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "kubernetes_service_v1",
      "name": "admin_kong_ingress_controller",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "kong/kong-kong-admin",
            "metadata": [
              {
                "annotations": {
                  "meta.helm.sh/release-name": "kong",
                  "meta.helm.sh/release-namespace": "kong"
                },
                "generation": 0,
                "labels": {
                  "app.kubernetes.io/instance": "kong",
                  "app.kubernetes.io/managed-by": "Helm",
                  "app.kubernetes.io/name": "kong",
                  "app.kubernetes.io/version": "3.6",
                  "helm.sh/chart": "kong-2.46.0"
                },
                "name": "kong-kong-admin",
                "namespace": "kong",
                "resource_version": "1506",
                "uid": "8b9c9cc3-0418-45bd-87a6-7d112cb14982"
              }
            ],
            "spec": [
              {
                "allocate_load_balancer_node_ports": true,
                "cluster_ip": "172.16.32.255",
                "cluster_ips": [
                  "172.16.32.255"
                ],
                "external_ips": [],
                "external_name": "",
                "external_traffic_policy": "Cluster",
                "health_check_node_port": 0,
                "internal_traffic_policy": "Cluster",
                "ip_families": [
                  "IPv4"
                ],
                "ip_family_policy": "SingleStack",
                "load_balancer_class": "",
                "load_balancer_ip": "",
                "load_balancer_source_ranges": [],
                "port": [
                  {
                    "app_protocol": "",
                    "name": "kong-admin",
                    "node_port": 31423,
                    "port": 8001,
                    "protocol": "TCP",
                    "target_port": "8001"
                  },
                  {
                    "app_protocol": "",
                    "name": "kong-admin-tls",
                    "node_port": 32369,
                    "port": 8444,
                    "protocol": "TCP",
                    "target_port": "8444"
                  }
                ],
                "publish_not_ready_addresses": false,
                "selector": {
                  "app.kubernetes.io/component": "app",
                  "app.kubernetes.io/instance": "kong",
                  "app.kubernetes.io/name": "kong"
                },
                "session_affinity": "None",
                "session_affinity_config": [],
                "type": "LoadBalancer"
              }
            ],
            "status": [
              {
                "load_balancer": [
                  {
                    "ingress": [
                      {
                        "hostname": "a8b9c9cc3041845bd87a67d112cb1498-1887389677.us-east-1.elb.amazonaws.com",
                        "ip": ""
                      }
                    ]
                  }
                ]
              }
            ]
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "kubernetes_service_v1",
      "name": "manager_kong_ingress_controller",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "kong/kong-kong-manager",
            "metadata": [
              {
                "annotations": {
                  "meta.helm.sh/release-name": "kong",
                  "meta.helm.sh/release-namespace": "kong"
                },
                "generation": 0,
                "labels": {
                  "app.kubernetes.io/instance": "kong",
                  "app.kubernetes.io/managed-by": "Helm",
                  "app.kubernetes.io/name": "kong",
                  "app.kubernetes.io/version": "3.6",
                  "helm.sh/chart": "kong-2.46.0"
                },
                "name": "kong-kong-manager",
                "namespace": "kong",
                "resource_version": "1486",
                "uid": "97cafaba-82c3-4d19-a8ee-5c86c47757f5"
              }
            ],
            "spec": [
              {
                "allocate_load_balancer_node_ports": true,
                "cluster_ip": "172.16.131.184",
                "cluster_ips": [
                  "172.16.131.184"
                ],
                "external_ips": [],
                "external_name": "",
                "external_traffic_policy": "Cluster",
                "health_check_node_port": 0,
                "internal_traffic_policy": "Cluster",
                "ip_families": [
                  "IPv4"
                ],
                "ip_family_policy": "SingleStack",
                "load_balancer_class": "",
                "load_balancer_ip": "",
                "load_balancer_source_ranges": [],
                "port": [
                  {
                    "app_protocol": "",
                    "name": "kong-manager",
                    "node_port": 30537,
                    "port": 8002,
                    "protocol": "TCP",
                    "target_port": "8002"
                  },
                  {
                    "app_protocol": "",
                    "name": "kong-manager-tls",
                    "node_port": 30922,
                    "port": 8445,
                    "protocol": "TCP",
                    "target_port": "8445"
                  }
                ],
                "publish_not_ready_addresses": false,
                "selector": {
                  "app.kubernetes.io/component": "app",
                  "app.kubernetes.io/instance": "kong",
                  "app.kubernetes.io/name": "kong"
                },
                "session_affinity": "None",
                "session_affinity_config": [],
                "type": "LoadBalancer"
              }
            ],
            "status": [
              {
                "load_balancer": [
                  {
                    "ingress": [
                      {
                        "hostname": "a97cafaba82c34d19a8ee5c86c47757f-1089699414.us-east-1.elb.amazonaws.com",
                        "ip": ""
                      }
                    ]
                  }
                ]
              }
            ]
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "kubernetes_service_v1",
      "name": "proxy_kong_ingress_controller",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "kong/kong-kong-proxy",
            "metadata": [
              {
                "annotations": {
                  "meta.helm.sh/release-name": "kong",
                  "meta.helm.sh/release-namespace": "kong"
                },
                "generation": 0,
                "labels": {
                  "app.kubernetes.io/instance": "kong",
                  "app.kubernetes.io/managed-by": "Helm",
                  "app.kubernetes.io/name": "kong",
                  "app.kubernetes.io/version": "3.6",
                  "enable-metrics": "true",
                  "helm.sh/chart": "kong-2.46.0"
                },
                "name": "kong-kong-proxy",
                "namespace": "kong",
                "resource_version": "1465",
                "uid": "a30f0335-b39b-43ed-aafd-701d6f046073"
              }
            ],
            "spec": [
              {
                "allocate_load_balancer_node_ports": true,
                "cluster_ip": "172.16.10.110",
                "cluster_ips": [
                  "172.16.10.110"
                ],
                "external_ips": [],
                "external_name": "",
                "external_traffic_policy": "Cluster",
                "health_check_node_port": 0,
                "internal_traffic_policy": "Cluster",
                "ip_families": [
                  "IPv4"
                ],
                "ip_family_policy": "SingleStack",
                "load_balancer_class": "",
                "load_balancer_ip": "",
                "load_balancer_source_ranges": [],
                "port": [
                  {
                    "app_protocol": "",
                    "name": "kong-proxy",
                    "node_port": 32438,
                    "port": 8000,
                    "protocol": "TCP",
                    "target_port": "8000"
                  },
                  {
                    "app_protocol": "",
                    "name": "kong-proxy-tls",
                    "node_port": 32257,
                    "port": 443,
                    "protocol": "TCP",
                    "target_port": "8443"
                  }
                ],
                "publish_not_ready_addresses": false,
                "selector": {
                  "app.kubernetes.io/component": "app",
                  "app.kubernetes.io/instance": "kong",
                  "app.kubernetes.io/name": "kong"
                },
                "session_affinity": "None",
                "session_affinity_config": [],
                "type": "LoadBalancer"
              }
            ],
            "status": [
              {
                "load_balancer": [
                  {
                    "ingress": [
                      {
                        "hostname": "aa30f0335b39b43edaafd701d6f04607-351278094.us-east-1.elb.amazonaws.com",
                        "ip": ""
                      }
                    ]
                  }
                ]
              }
            ]
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "tls_certificate",
      "name": "authentication",
      "provider": "provider[\"registry.terraform.io/hashicorp/tls\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "certificates": [
              {
                "cert_pem": "-----BEGIN CERTIFICATE-----\nMIIEdTCCA12gAwIBAgIJAKcOSkw0grd/MA0GCSqGSIb3DQEBCwUAMGgxCzAJBgNV\nBAYTAlVTMSUwIwYDVQQKExxTdGFyZmllbGQgVGVjaG5vbG9naWVzLCBJbmMuMTIw\nMAYDVQQLEylTdGFyZmllbGQgQ2xhc3MgMiBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0\neTAeFw0wOTA5MDIwMDAwMDBaFw0zNDA2MjgxNzM5MTZaMIGYMQswCQYDVQQGEwJV\nUzEQMA4GA1UECBMHQXJpem9uYTETMBEGA1UEBxMKU2NvdHRzZGFsZTElMCMGA1UE\nChMcU3RhcmZpZWxkIFRlY2hub2xvZ2llcywgSW5jLjE7MDkGA1UEAxMyU3RhcmZp\nZWxkIFNlcnZpY2VzIFJvb3QgQ2VydGlmaWNhdGUgQXV0aG9yaXR5IC0gRzIwggEi\nMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDVDDrEKvlO4vW+GZdfjohTsR8/\ny8+fIBNtKTrID30892t2OGPZNmCom15cAICyL1l/9of5JUOG52kbUpqQ4XHj2C0N\nTm/2yEnZtvMaVq4rtnQU68/7JuMauh2WLmo7WJSJR1b/JaCTcFOD2oR0FMNnngRo\nOt+OQFodSk7PQ5E751bWAHDLUu57fa4657wx+UX2wmDPE1kCK4DMNEffud6QZW0C\nzyyRpqbn3oUYSXxmTqM6bam17jQuug0DuDPfR+uxa40l2ZvOgdFFRjKWcIfeAg5J\nQ4W2bHO7ZOphQazJ1FTfhy/HIrImzJ9ZVGif/L4qL8RVHHVAYBeFAlU5i38FAgMB\nAAGjgfAwge0wDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAYYwHQYDVR0O\nBBYEFJxfAN+qAdcwKziIorhtSpzyEZGDMB8GA1UdIwQYMBaAFL9ft9HO3R+G9FtV\nrNzXEMIOqYjnME8GCCsGAQUFBwEBBEMwQTAcBggrBgEFBQcwAYYQaHR0cDovL28u\nc3MyLnVzLzAhBggrBgEFBQcwAoYVaHR0cDovL3guc3MyLnVzL3guY2VyMCYGA1Ud\nHwQfMB0wG6AZoBeGFWh0dHA6Ly9zLnNzMi51cy9yLmNybDARBgNVHSAECjAIMAYG\nBFUdIAAwDQYJKoZIhvcNAQELBQADggEBACMd44pXyn3pF3lM8R5V/cxTbj5HD9/G\nVfKyBDbtgB9TxF00KGu+x1X8Z+rLP3+QsjPNG1gQggL4+C/1E2DUBc7xgQjB3ad1\nl08YuW3e95ORCLp+QCztweq7dp4zBncdDQh/U90bZKuCJ/Fp1U1ervShw3WnWEQt\n8jxwmKy6abaVd38PMV4s/KCHOkdp8Hlf9BRUpJVeEXgSYCfOn8J3/yNTd126/+pZ\n59vPr5KW7ySaNRB6nJHGDn2Z9j8Z3/VyVOEVqQdZe4O/Ui5GjLIAZHYcSNPYeehu\nVsyuLAOQ1xk4meTKCRlb/weWsKh/NEnfVqn3sF/tM+2MR7cwA130A4w=\n-----END CERTIFICATE-----\n",
                "is_ca": true,
                "issuer": "OU=Starfield Class 2 Certification Authority,O=Starfield Technologies\\, Inc.,C=US",
                "not_after": "2034-06-28T17:39:16Z",
                "not_before": "2009-09-02T00:00:00Z",
                "public_key_algorithm": "RSA",
                "serial_number": "12037640545166866303",
                "sha1_fingerprint": "9e99a48a9960b14926bb7f3b02e22da2b0ab7280",
                "signature_algorithm": "SHA256-RSA",
                "subject": "CN=Starfield Services Root Certificate Authority - G2,O=Starfield Technologies\\, Inc.,L=Scottsdale,ST=Arizona,C=US",
                "version": 3
              },
              {
                "cert_pem": "-----BEGIN CERTIFICATE-----\nMIIEkjCCA3qgAwIBAgITBn+USionzfP6wq4rAfkI7rnExjANBgkqhkiG9w0BAQsF\nADCBmDELMAkGA1UEBhMCVVMxEDAOBgNVBAgTB0FyaXpvbmExEzARBgNVBAcTClNj\nb3R0c2RhbGUxJTAjBgNVBAoTHFN0YXJmaWVsZCBUZWNobm9sb2dpZXMsIEluYy4x\nOzA5BgNVBAMTMlN0YXJmaWVsZCBTZXJ2aWNlcyBSb290IENlcnRpZmljYXRlIEF1\ndGhvcml0eSAtIEcyMB4XDTE1MDUyNTEyMDAwMFoXDTM3MTIzMTAxMDAwMFowOTEL\nMAkGA1UEBhMCVVMxDzANBgNVBAoTBkFtYXpvbjEZMBcGA1UEAxMQQW1hem9uIFJv\nb3QgQ0EgMTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALJ4gHHKeNXj\nca9HgFB0fW7Y14h29Jlo91ghYPl0hAEvrAIthtOgQ3pOsqTQNroBvo3bSMgHFzZM\n9O6II8c+6zf1tRn4SWiw3te5djgdYZ6k/oI2peVKVuRF4fn9tBb6dNqcmzU5L/qw\nIFAGbHrQgLKm+a/sRxmPUDgH3KKHOVj4utWp+UhnMJbulHheb4mjUcAwhmahRWa6\nVOujw5H5SNz/0egwLX0tdHA114gk957EWW67c4cX8jJGKLhD+rcdqsq08p8kDi1L\n93FcXmn/6pUCyziKrlA4b9v7LWIbxcceVOF34GfID5yHI9Y/QCB/IIDEgEw+OyQm\njgSubJrIqg0CAwEAAaOCATEwggEtMA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/\nBAQDAgGGMB0GA1UdDgQWBBSEGMyFNOy8DJSULghZnMeyEE4KCDAfBgNVHSMEGDAW\ngBScXwDfqgHXMCs4iKK4bUqc8hGRgzB4BggrBgEFBQcBAQRsMGowLgYIKwYBBQUH\nMAGGImh0dHA6Ly9vY3NwLnJvb3RnMi5hbWF6b250cnVzdC5jb20wOAYIKwYBBQUH\nMAKGLGh0dHA6Ly9jcnQucm9vdGcyLmFtYXpvbnRydXN0LmNvbS9yb290ZzIuY2Vy\nMD0GA1UdHwQ2MDQwMqAwoC6GLGh0dHA6Ly9jcmwucm9vdGcyLmFtYXpvbnRydXN0\nLmNvbS9yb290ZzIuY3JsMBEGA1UdIAQKMAgwBgYEVR0gADANBgkqhkiG9w0BAQsF\nAAOCAQEAYjdCXLwQtT6LLOkMm2xF4gcAevnFWAu5CIw+7bMlPLVvUOTNNWqnkzSW\nMiGpSESrnO09tKpzbeR/FoCJbM8oAxiDR3mjEH4wW6w7sGDgd9QIpuEdfF7Au/ma\neyKdpwAJfqxGF4PcnCZXmTA5YpaP7dreqsXMGz7KQ2hsVxa81Q4gLv7/wmpdLqBK\nbRRYh5TmOTFffHPLkIhqhBGWJ6bt2YFGpn6jcgAKUj6DiAdjd4lpFw85hdKrCEVN\n0FE6/V1dN2RMfjCyVSRCnTawXZwXgWHxyvkQAiSr6w10kY17RSlQOYiypok1JR4U\nakcjMS9cmvqtmg5iUaQqqcT5NJ0hGA==\n-----END CERTIFICATE-----\n",
                "is_ca": true,
                "issuer": "CN=Starfield Services Root Certificate Authority - G2,O=Starfield Technologies\\, Inc.,L=Scottsdale,ST=Arizona,C=US",
                "not_after": "2037-12-31T01:00:00Z",
                "not_before": "2015-05-25T12:00:00Z",
                "public_key_algorithm": "RSA",
                "serial_number": "144918191876577076464031512351042010504348870",
                "sha1_fingerprint": "06b25927c42a721631c1efd9431e648fa62e1e39",
                "signature_algorithm": "SHA256-RSA",
                "subject": "CN=Amazon Root CA 1,O=Amazon,C=US",
                "version": 3
              },
              {
                "cert_pem": "-----BEGIN CERTIFICATE-----\nMIIEXjCCA0agAwIBAgITB3MSSkvL1E7HtTvq8ZSELToPoTANBgkqhkiG9w0BAQsF\nADA5MQswCQYDVQQGEwJVUzEPMA0GA1UEChMGQW1hem9uMRkwFwYDVQQDExBBbWF6\nb24gUm9vdCBDQSAxMB4XDTIyMDgyMzIyMjUzMFoXDTMwMDgyMzIyMjUzMFowPDEL\nMAkGA1UEBhMCVVMxDzANBgNVBAoTBkFtYXpvbjEcMBoGA1UEAxMTQW1hem9uIFJT\nQSAyMDQ4IE0wMjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALtDGMZa\nqHneKei1by6+pUPPLljTB143Si6VpEWPc6mSkFhZb/6qrkZyoHlQLbDYnI2D7hD0\nsdzEqfnuAjIsuXQLG3A8TvX6V3oFNBFVe8NlLJHvBseKY88saLwufxkZVwk74g4n\nWlNMXzla9Y5F3wwRHwMVH443xGz6UtGSZSqQ94eFx5X7Tlqt8whi8qCaKdZ5rNak\n+r9nUThOeClqFd4oXych//Rc7Y0eX1KNWHYSI1Nk31mYgiK3JvH063g+K9tHA63Z\neTgKgndlh+WI+zv7i44HepRZjA1FYwYZ9Vv/9UkC5Yz8/yU65fgjaE+wVHM4e/Yy\nC2osrPWE7gJ+dXMCAwEAAaOCAVowggFWMBIGA1UdEwEB/wQIMAYBAf8CAQAwDgYD\nVR0PAQH/BAQDAgGGMB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEFBQcDAjAdBgNV\nHQ4EFgQUwDFSzVpQw4J8dHHOy+mc+XrrguIwHwYDVR0jBBgwFoAUhBjMhTTsvAyU\nlC4IWZzHshBOCggwewYIKwYBBQUHAQEEbzBtMC8GCCsGAQUFBzABhiNodHRwOi8v\nb2NzcC5yb290Y2ExLmFtYXpvbnRydXN0LmNvbTA6BggrBgEFBQcwAoYuaHR0cDov\nL2NydC5yb290Y2ExLmFtYXpvbnRydXN0LmNvbS9yb290Y2ExLmNlcjA/BgNVHR8E\nODA2MDSgMqAwhi5odHRwOi8vY3JsLnJvb3RjYTEuYW1hem9udHJ1c3QuY29tL3Jv\nb3RjYTEuY3JsMBMGA1UdIAQMMAowCAYGZ4EMAQIBMA0GCSqGSIb3DQEBCwUAA4IB\nAQAtTi6Fs0Azfi+iwm7jrz+CSxHH+uHl7Law3MQSXVtR8RV53PtR6r/6gNpqlzdo\nZq4FKbADi1v9Bun8RY8D51uedRfjsbeodizeBB8nXmeyD33Ep7VATj4ozcd31YFV\nfgRhvTSxNrrTlNpWkUk0m3BMPv8sg381HhA6uEYokE5q9uws/3YkKqRiEz3TsaWm\nJqIRZhMbgAfp7O7FUwFIb7UIspogZSKxPIWJpxiPo3TcBambbVtQOcNRWz5qCQdD\nslI2yayq0n2TXoHyNCLEH8rpsJRVILFsg0jc7BaFrMnF462+ajSehgj12IidNeRN\n4zl+EoNaWdpnWndvSpAEkq2P\n-----END CERTIFICATE-----\n",
                "is_ca": true,
                "issuer": "CN=Amazon Root CA 1,O=Amazon,C=US",
                "not_after": "2030-08-23T22:25:30Z",
                "not_before": "2022-08-23T22:25:30Z",
                "public_key_algorithm": "RSA",
                "serial_number": "166129353110899469622597955040406457904926625",
                "sha1_fingerprint": "414a2060b738c635cc7fc243e052615592830c53",
                "signature_algorithm": "SHA256-RSA",
                "subject": "CN=Amazon RSA 2048 M02,O=Amazon,C=US",
                "version": 3
              },
              {
                "cert_pem": "-----BEGIN CERTIFICATE-----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\nqQ3LlAF6Op6n7JLa2rlXkf6LienZDVlSuQ==\n-----END CERTIFICATE-----\n",
                "is_ca": false,
                "issuer": "CN=Amazon RSA 2048 M02,O=Amazon,C=US",
                "not_after": "2025-04-26T23:59:59Z",
                "not_before": "2024-03-28T00:00:00Z",
                "public_key_algorithm": "RSA",
                "serial_number": "11397038078078022747295509550808604104",
                "sha1_fingerprint": "9451ad2b53c7f41fab22886cc07d482085336561",
                "signature_algorithm": "SHA256-RSA",
                "subject": "CN=*.eks.us-east-1.amazonaws.com",
                "version": 3
              }
            ],
            "content": null,
            "id": "99d41e43229a4cdaf4141f3e8310e6d95c31dab9",
            "url": "https://oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358",
            "verify_chain": true
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_eks_cluster",
      "name": "eks_cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "access_config": [
              {
                "authentication_mode": "CONFIG_MAP",
                "bootstrap_cluster_creator_admin_permissions": true
              }
            ],
            "arn": "arn:aws:eks:us-east-1:905418291613:cluster/ekscluster",
            "bootstrap_self_managed_addons": true,
            "certificate_authority": [
              {
                "data": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJQUtDMVpabTdERFl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRFeU1qQXhOREE0TUROYUZ3MHpOREV5TVRneE5ERXpNRE5hTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUM5TWVLbFM4dzBlb1RiR0Z5aVFXT0hpcU5CU3hoeHUzS3ZMTnBSRkFGSlBLMDZHelNJeGYrVEQxMXQKQTJiSWN2NDFyM3JXQXZXb2MvSHlPSk9IcDFyMlNTNS9weXJXb0tDRkpVSVZJTDFrN0ZOUE1nem5jWHJIbC9aTgpBMUVLMGlCbkhPS1FpeGNjTWQ1U1dPQWNobWNYVXdOUlJyZG9ENEN4YzlsMjBvUk5GekNjQStlRWxiYXNqL2R4CnE1QlVCNGFsVHE3VkE5NzNFQ2xNSm5NbkFYcldOcmFqazNSc2g2SHNFbU5Lek9YZ0MzUndiV1UxUWxaYW9kR0QKcHNzOHBZa1h2ekJ6WXkrVW8raDdDaXh6MjFVRFQ5M3c0RGRhUUUrWkxlRVR4bGVjMU5LRi9TTm81d0JiYkFFVQpEQjhSR0VmUjhGeE1kTTV2S0hTRWI3NnpIaUNwQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJUam42MGZjWVlkUlFmQ3VOYW51ODNtSy9Udk16QVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQUJld3pubnV0OApuekJ5a1orYTFoaUprcWNhZ2NMWmdyZ0lVYXR5NDNFT2V5ZGowTTFPV3BIbHBVejh1dm45K1RQYVRqZHRiUnN1Cmh1N0ExQnhSRk1jT1cyUFY0VDFzNzM0OHpnaXJOSnoyRno2S0d3SVZyWXRkUko3TnBpUUxKM3lpZmNyKzZPNS8KWEJXcnpjQ2UzdU5rWjdIYnVJdjNqWDlYWFVaMWVoRnJibVF2QTBIYlpTSFlOcTJhWlJ2L3JtQVN2ZVNNRjMwWQpOWFF2NmtBK1pSQnptUWsxYWZsL0xLUURnM1NnZnZ5L1lQMmp1SlNHWkE1clR4ZHJaeUJ4aVJ3QVpkTGFlUFc2CmJkaEdpYU41V1VSdldkVFdLOVVVY3ZLMldOWDBtbGlacDY3TWpZVTAzWVlaZ2QvbGZIVkMxUnVFcS9MU2RPK3AKNXZyNVVHWDRZNzJ4Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K"
              }
            ],
            "cluster_id": null,
            "compute_config": [],
            "created_at": "2024-12-20T14:07:30Z",
            "enabled_cluster_log_types": [
              "api",
              "audit",
              "authenticator",
              "controllerManager",
              "scheduler"
            ],
            "encryption_config": [],
            "endpoint": "https://BD9B8CBB7DD9D935201147689578F358.gr7.us-east-1.eks.amazonaws.com",
            "id": "ekscluster",
            "identity": [
              {
                "oidc": [
                  {
                    "issuer": "https://oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358"
                  }
                ]
              }
            ],
            "kubernetes_network_config": [
              {
                "elastic_load_balancing": [
                  {
                    "enabled": false
                  }
                ],
                "ip_family": "ipv4",
                "service_ipv4_cidr": "172.16.0.0/16",
                "service_ipv6_cidr": ""
              }
            ],
            "name": "ekscluster",
            "outpost_config": [],
            "platform_version": "eks.23",
            "remote_network_config": [],
            "role_arn": "arn:aws:iam::905418291613:role/dev-eks-cluster-role",
            "status": "ACTIVE",
            "storage_config": [],
            "tags": {
              "Name": "eks-cluster"
            },
            "tags_all": {
              "Name": "eks-cluster"
            },
            "timeouts": null,
            "upgrade_policy": [
              {
                "support_type": "EXTENDED"
              }
            ],
            "version": "1.30",
            "vpc_config": [
              {
                "cluster_security_group_id": "sg-07f57fddd34441d9f",
                "endpoint_private_access": false,
                "endpoint_public_access": true,
                "public_access_cidrs": [
                  "0.0.0.0/0"
                ],
                "security_group_ids": [],
                "subnet_ids": [
                  "subnet-01085b2d3412f3ebf",
                  "subnet-089c4360de112d825"
                ],
                "vpc_id": "vpc-008c185615b74dff3"
              }
            ],
            "zonal_shift_config": []
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxODAwMDAwMDAwMDAwLCJkZWxldGUiOjkwMDAwMDAwMDAwMCwidXBkYXRlIjozNjAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "dependencies": [
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_eks_node_group",
      "name": "eks_nodegroup_private",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "ami_type": "AL2_x86_64",
            "arn": "arn:aws:eks:us-east-1:905418291613:nodegroup/ekscluster/dev-eks-nodegroup-private/26c9f237-1ac7-c5b2-7f1d-878a8564a0eb",
            "capacity_type": "ON_DEMAND",
            "cluster_name": "ekscluster",
            "disk_size": 20,
            "force_update_version": null,
            "id": "ekscluster:dev-eks-nodegroup-private",
            "instance_types": [
              "t3.large"
            ],
            "labels": {},
            "launch_template": [],
            "node_group_name": "dev-eks-nodegroup-private",
            "node_group_name_prefix": "",
            "node_role_arn": "arn:aws:iam::905418291613:role/dev-eks-nodegroup-role",
            "release_version": "1.30.7-20241213",
            "remote_access": [
              {
                "ec2_ssh_key": "keypair",
                "source_security_group_ids": [
                  "sg-069cd5f52d5c13bb1"
                ]
              }
            ],
            "resources": [
              {
                "autoscaling_groups": [
                  {
                    "name": "eks-dev-eks-nodegroup-private-26c9f237-1ac7-c5b2-7f1d-878a8564a0eb"
                  }
                ],
                "remote_access_security_group_id": "sg-0ba887284feaaa09d"
              }
            ],
            "scaling_config": [
              {
                "desired_size": 1,
                "max_size": 4,
                "min_size": 1
              }
            ],
            "status": "ACTIVE",
            "subnet_ids": [
              "subnet-00c603222b651f8da",
              "subnet-0a00bb84a17478e9a"
            ],
            "tags": {
              "Name": "Private-Node-Group",
              "k8s.io/cluster-autoscaler/ekscluster": "owned",
              "k8s.io/cluster-autoscaler/enabled": "TRUE"
            },
            "tags_all": {
              "Name": "Private-Node-Group",
              "k8s.io/cluster-autoscaler/ekscluster": "owned",
              "k8s.io/cluster-autoscaler/enabled": "TRUE"
            },
            "taint": [],
            "timeouts": null,
            "update_config": [
              {
                "max_unavailable": 1,
                "max_unavailable_percentage": 0
              }
            ],
            "version": "1.30"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozNjAwMDAwMDAwMDAwLCJkZWxldGUiOjM2MDAwMDAwMDAwMDAsInVwZGF0ZSI6MzYwMDAwMDAwMDAwMH19",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_admin_role",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role.eks_dev_role",
            "aws_iam_role.eks_nodegroup_role",
            "aws_iam_role.eks_readonly_role",
            "aws_iam_role_policy_attachment.eks-AmazonEC2ContainerRegistryReadOnly",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "aws_iam_role_policy_attachment.eks-AmazonEKSWorkerNodePolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKS_CNI_Policy",
            "aws_security_group.allow_tls",
            "data.aws_availability_zones.available",
            "data.aws_caller_identity.current",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "kubernetes_cluster_role_binding_v1.eks_dev_cluster_role_binding",
            "kubernetes_cluster_role_binding_v1.eks_readonly_cluster_role_binding",
            "kubernetes_cluster_role_v1.eks_dev_cluster_role",
            "kubernetes_cluster_role_v1.eks_readonly_cluster_role",
            "kubernetes_config_map_v1.aws-auth",
            "kubernetes_namespace.k8s_dev",
            "kubernetes_role_binding_v1.eksdev_role_binding",
            "kubernetes_role_v1.eksdev_role",
            "module.vpc.aws_subnet.private",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_group",
      "name": "eks_admin_group",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:group/eksAdmins",
            "id": "eksAdmins",
            "name": "eksAdmins",
            "path": "/",
            "unique_id": "AGPA5FTZCXGOWOHC4KIFP"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_group",
      "name": "eks_dev_group",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:group/eksDev",
            "id": "eksDev",
            "name": "eksDev",
            "path": "/",
            "unique_id": "AGPA5FTZCXGO5XYBKAUKR"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_group",
      "name": "eks_readonly_group",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:group/eksReadonly",
            "id": "eksReadonly",
            "name": "eksReadonly",
            "path": "/",
            "unique_id": "AGPA5FTZCXGOTH2VBJWQH"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_group_membership",
      "name": "eksadmins_group",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "group": "eksAdmins",
            "id": "eksadmin_group_members",
            "name": "eksadmin_group_members",
            "users": [
              "eksadmin"
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_group.eks_admin_group",
            "aws_iam_user.eksadmin_user"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_group_membership",
      "name": "eksdev_group",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "group": "eksDev",
            "id": "eksdev_group_members",
            "name": "eksdev_group_members",
            "users": [
              "eksdev"
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_group.eks_dev_group",
            "aws_iam_user.eks_dev_user"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_group_membership",
      "name": "eksreadonly_group",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "group": "eksReadonly",
            "id": "eksreadonly_group_members",
            "name": "eksreadonly_group_members",
            "users": [
              "eksreadonly"
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_group.eks_readonly_group",
            "aws_iam_user.eks_readonly_user"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_group_policy",
      "name": "iam_group_admin_assume_role_policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "group": "eksAdmins",
            "id": "eksAdmins:eksAdmins-group-policy",
            "name": "eksAdmins-group-policy",
            "name_prefix": "",
            "policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Action\":[\"sts:AssumeRole\"],\"Effect\":\"Allow\",\"Resource\":\"arn:aws:iam::905418291613:role/eks-admin-role\",\"Sid\":\"AllowAssumeOrganizationAccountRole\"}]}"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_group.eks_admin_group",
            "aws_iam_role.eks_admin_role",
            "data.aws_caller_identity.current"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_group_policy",
      "name": "iam_group_dev_assume_role_policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "group": "eksDev",
            "id": "eksDev:eksdev-group-policy",
            "name": "eksdev-group-policy",
            "name_prefix": "",
            "policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Action\":[\"sts:AssumeRole\"],\"Effect\":\"Allow\",\"Resource\":\"arn:aws:iam::905418291613:role/eks-dev-role\",\"Sid\":\"AllowAssumeOrganizationAccountRole\"}]}"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_group.eks_dev_group",
            "aws_iam_role.eks_dev_role",
            "data.aws_caller_identity.current"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_group_policy",
      "name": "iam_group_readonly_assume_role_policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "group": "eksReadonly",
            "id": "eksReadonly:eksreadonly-group-policy",
            "name": "eksreadonly-group-policy",
            "name_prefix": "",
            "policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Action\":[\"sts:AssumeRole\"],\"Effect\":\"Allow\",\"Resource\":\"arn:aws:iam::905418291613:role/eks-readonly-role\",\"Sid\":\"AllowAssumeOrganizationAccountRole\"}]}"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_group.eks_readonly_group",
            "aws_iam_role.eks_readonly_role",
            "data.aws_caller_identity.current"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_openid_connect_provider",
      "name": "oidc_provider",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358",
            "client_id_list": [
              "sts.amazonaws.com"
            ],
            "id": "arn:aws:iam::905418291613:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358",
            "tags": {
              "Name": "dev-eks-irsa"
            },
            "tags_all": {
              "Name": "dev-eks-irsa"
            },
            "thumbprint_list": [
              "9e99a48a9960b14926bb7f3b02e22da2b0ab7280"
            ],
            "url": "oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "data.aws_partition.current",
            "data.tls_certificate.authentication"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_policy",
      "name": "cluster_autoscaler_iam_policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:policy/dev-AmazonEKSClusterAutoscalerPolicy",
            "attachment_count": 1,
            "description": "EKS Cluster Autoscaler Policy",
            "id": "arn:aws:iam::905418291613:policy/dev-AmazonEKSClusterAutoscalerPolicy",
            "name": "dev-AmazonEKSClusterAutoscalerPolicy",
            "name_prefix": "",
            "path": "/",
            "policy": "{\"Statement\":[{\"Action\":[\"autoscaling:DescribeAutoScalingGroups\",\"autoscaling:DescribeAutoScalingInstances\",\"autoscaling:DescribeInstances\",\"autoscaling:DescribeLaunchConfigurations\",\"autoscaling:DescribeTags\",\"autoscaling:SetDesiredCapacity\",\"autoscaling:TerminateInstanceInAutoScalingGroup\",\"ec2:DescribeLaunchTemplateVersions\",\"ec2:DescribeInstanceTypes\"],\"Effect\":\"Allow\",\"Resource\":\"*\"}],\"Version\":\"2012-10-17\"}",
            "policy_id": "ANPA5FTZCXGOTSVZ5ECQZ",
            "tags": {},
            "tags_all": {}
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_policy",
      "name": "ebs_iam_policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:policy/dev-amazonEks-ebs-iam-policy",
            "attachment_count": 1,
            "description": "",
            "id": "arn:aws:iam::905418291613:policy/dev-amazonEks-ebs-iam-policy",
            "name": "dev-amazonEks-ebs-iam-policy",
            "name_prefix": "",
            "path": "/",
            "policy": "{\"Statement\":[{\"Action\":[\"ec2:CreateSnapshot\",\"ec2:AttachVolume\",\"ec2:DetachVolume\",\"ec2:ModifyVolume\",\"ec2:DescribeAvailabilityZones\",\"ec2:DescribeInstances\",\"ec2:DescribeSnapshots\",\"ec2:DescribeTags\",\"ec2:DescribeVolumes\",\"ec2:DescribeVolumesModifications\",\"ec2:EnableFastSnapshotRestores\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"ec2:CreateTags\"],\"Effect\":\"Allow\",\"Resource\":[\"arn:*:ec2:*:*:volume/*\",\"arn:*:ec2:*:*:snapshot/*\"]},{\"Action\":[\"ec2:DeleteTags\"],\"Effect\":\"Allow\",\"Resource\":[\"arn:*:ec2:*:*:volume/*\",\"arn:*:ec2:*:*:snapshot/*\"]},{\"Action\":[\"ec2:CreateVolume\"],\"Condition\":{\"StringLike\":{\"aws:RequestTag/ebs.csi.aws.com/cluster\":\"true\"}},\"Effect\":\"Allow\",\"Resource\":\"arn:*:ec2:*:*:volume/*\"},{\"Action\":[\"ec2:CreateVolume\"],\"Condition\":{\"StringLike\":{\"aws:RequestTag/CSIVolumeName\":\"*\"}},\"Effect\":\"Allow\",\"Resource\":\"arn:*:ec2:*:*:volume/*\"},{\"Action\":[\"ec2:CreateVolume\"],\"Effect\":\"Allow\",\"Resource\":\"arn:*:ec2:*:*:snapshot/*\"},{\"Action\":[\"ec2:DeleteVolume\"],\"Condition\":{\"StringLike\":{\"ec2:ResourceTag/ebs.csi.aws.com/cluster\":\"true\"}},\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"ec2:DeleteVolume\"],\"Condition\":{\"StringLike\":{\"ec2:ResourceTag/CSIVolumeName\":\"*\"}},\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"ec2:DeleteVolume\"],\"Condition\":{\"StringLike\":{\"ec2:ResourceTag/kubernetes.io/created-for/pvc/name\":\"*\"}},\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"ec2:DeleteSnapshot\"],\"Condition\":{\"StringLike\":{\"ec2:ResourceTag/CSIVolumeSnapshotName\":\"*\"}},\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"ec2:DeleteSnapshot\"],\"Condition\":{\"StringLike\":{\"ec2:ResourceTag/ebs.csi.aws.com/cluster\":\"true\"}},\"Effect\":\"Allow\",\"Resource\":\"*\"}],\"Version\":\"2012-10-17\"}",
            "policy_id": "ANPA5FTZCXGOQNUGDZE3H",
            "tags": {},
            "tags_all": {}
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "data.http.iam_policy"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_policy",
      "name": "ingress_kong_controller_policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:policy/dev-ingress-kong-controller-policy",
            "attachment_count": 1,
            "description": "",
            "id": "arn:aws:iam::905418291613:policy/dev-ingress-kong-controller-policy",
            "name": "dev-ingress-kong-controller-policy",
            "name_prefix": "",
            "path": "/",
            "policy": "{\"Statement\":[{\"Action\":[\"acm:DescribeCertificate\",\"acm:ListCertificates\",\"acm:GetCertificate\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"ec2:AuthorizeSecurityGroupIngress\",\"ec2:CreateSecurityGroup\",\"ec2:CreateTags\",\"ec2:DeleteTags\",\"ec2:DeleteSecurityGroup\",\"ec2:DescribeAccountAttributes\",\"ec2:DescribeAddresses\",\"ec2:DescribeInstances\",\"ec2:DescribeInstanceStatus\",\"ec2:DescribeInternetGateways\",\"ec2:DescribeNetworkInterfaces\",\"ec2:DescribeSecurityGroups\",\"ec2:DescribeSubnets\",\"ec2:DescribeTags\",\"ec2:DescribeVpcs\",\"ec2:ModifyInstanceAttribute\",\"ec2:ModifyNetworkInterfaceAttribute\",\"ec2:RevokeSecurityGroupIngress\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"elasticloadbalancing:AddListenerCertificates\",\"elasticloadbalancing:AddTags\",\"elasticloadbalancing:CreateListener\",\"elasticloadbalancing:CreateLoadBalancer\",\"elasticloadbalancing:CreateRule\",\"elasticloadbalancing:CreateTargetGroup\",\"elasticloadbalancing:DeleteListener\",\"elasticloadbalancing:DeleteLoadBalancer\",\"elasticloadbalancing:DeleteRule\",\"elasticloadbalancing:DeleteTargetGroup\",\"elasticloadbalancing:DeregisterTargets\",\"elasticloadbalancing:DescribeListenerCertificates\",\"elasticloadbalancing:DescribeListeners\",\"elasticloadbalancing:DescribeLoadBalancers\",\"elasticloadbalancing:DescribeLoadBalancerAttributes\",\"elasticloadbalancing:DescribeRules\",\"elasticloadbalancing:DescribeSSLPolicies\",\"elasticloadbalancing:DescribeTags\",\"elasticloadbalancing:DescribeTargetGroups\",\"elasticloadbalancing:DescribeTargetGroupAttributes\",\"elasticloadbalancing:DescribeTargetHealth\",\"elasticloadbalancing:ModifyListener\",\"elasticloadbalancing:ModifyLoadBalancerAttributes\",\"elasticloadbalancing:ModifyRule\",\"elasticloadbalancing:ModifyTargetGroup\",\"elasticloadbalancing:ModifyTargetGroupAttributes\",\"elasticloadbalancing:RegisterTargets\",\"elasticloadbalancing:RemoveListenerCertificates\",\"elasticloadbalancing:RemoveTags\",\"elasticloadbalancing:SetIpAddressType\",\"elasticloadbalancing:SetSecurityGroups\",\"elasticloadbalancing:SetSubnets\",\"elasticloadbalancing:SetWebAcl\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"iam:CreateServiceLinkedRole\",\"iam:GetServerCertificate\",\"iam:ListServerCertificates\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"cognito-idp:DescribeUserPoolClient\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"waf-regional:GetWebACLForResource\",\"waf-regional:GetWebACL\",\"waf-regional:AssociateWebACL\",\"waf-regional:DisassociateWebACL\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"tag:GetResources\",\"tag:TagResources\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"waf:GetWebACL\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"wafv2:GetWebACL\",\"wafv2:GetWebACLForResource\",\"wafv2:AssociateWebACL\",\"wafv2:DisassociateWebACL\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"shield:DescribeProtection\",\"shield:GetSubscriptionState\",\"shield:DeleteProtection\",\"shield:CreateProtection\",\"shield:DescribeSubscription\",\"shield:ListProtections\"],\"Effect\":\"Allow\",\"Resource\":\"*\"}],\"Version\":\"2012-10-17\"}",
            "policy_id": "ANPA5FTZCXGO5MNGMEM55",
            "tags": {},
            "tags_all": {}
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "cluster_autoscaler_iam_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:role/dev-cluster-autoscaler",
            "assume_role_policy": "{\"Statement\":[{\"Action\":\"sts:AssumeRoleWithWebIdentity\",\"Condition\":{\"StringEquals\":{\"oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358:sub\":\"system:serviceaccount:kube-system:cluster-autoscaler\"}},\"Effect\":\"Allow\",\"Principal\":{\"Federated\":\"arn:aws:iam::905418291613:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358\"},\"Sid\":\"\"}],\"Version\":\"2012-10-17\"}",
            "create_date": "2024-12-20T14:17:27Z",
            "description": "",
            "force_detach_policies": false,
            "id": "dev-cluster-autoscaler",
            "inline_policy": [],
            "managed_policy_arns": [
              "arn:aws:iam::905418291613:policy/dev-AmazonEKSClusterAutoscalerPolicy"
            ],
            "max_session_duration": 3600,
            "name": "dev-cluster-autoscaler",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": "",
            "tags": {
              "tag-key": "cluster-autoscaler"
            },
            "tags_all": {
              "tag-key": "cluster-autoscaler"
            },
            "unique_id": "AROA5FTZCXGOVYQ7RCD4I"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_openid_connect_provider.oidc_provider",
            "data.aws_partition.current",
            "data.tls_certificate.authentication"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "ebs_iam_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:role/dev-amazonEks-ebs-iam-role",
            "assume_role_policy": "{\"Statement\":[{\"Action\":\"sts:AssumeRoleWithWebIdentity\",\"Condition\":{\"StringEquals\":{\"oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358:sub\":\"system:serviceaccount:kube-system:ebs-csi-controller-sa\"}},\"Effect\":\"Allow\",\"Principal\":{\"Federated\":\"arn:aws:iam::905418291613:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358\"},\"Sid\":\"\"}],\"Version\":\"2012-10-17\"}",
            "create_date": "2024-12-20T14:17:27Z",
            "description": "",
            "force_detach_policies": false,
            "id": "dev-amazonEks-ebs-iam-role",
            "inline_policy": [],
            "managed_policy_arns": [
              "arn:aws:iam::905418291613:policy/dev-amazonEks-ebs-iam-policy"
            ],
            "max_session_duration": 3600,
            "name": "dev-amazonEks-ebs-iam-role",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": "",
            "tags": {},
            "tags_all": {},
            "unique_id": "AROA5FTZCXGOXJI5JCNCQ"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_openid_connect_provider.oidc_provider",
            "data.aws_partition.current",
            "data.tls_certificate.authentication"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "eks_admin_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:role/eks-admin-role",
            "assume_role_policy": "{\"Statement\":[{\"Action\":\"sts:AssumeRole\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::905418291613:root\"},\"Sid\":\"\"}],\"Version\":\"2012-10-17\"}",
            "create_date": "2024-12-20T14:07:05Z",
            "description": "",
            "force_detach_policies": false,
            "id": "eks-admin-role",
            "inline_policy": [
              {
                "name": "eks-admin-full-access-policy",
                "policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Action\":[\"eks:*\",\"sts:AssumeRole\"],\"Effect\":\"Allow\",\"Resource\":\"*\"}]}"
              }
            ],
            "managed_policy_arns": [],
            "max_session_duration": 3600,
            "name": "eks-admin-role",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": "",
            "tags": {
              "Name": "eks-admin-full-access-role"
            },
            "tags_all": {
              "Name": "eks-admin-full-access-role"
            },
            "unique_id": "AROA5FTZCXGOXI2L4HCUW"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "data.aws_caller_identity.current"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "eks_cluster_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:role/dev-eks-cluster-role",
            "assume_role_policy": "{\"Statement\":[{\"Action\":\"sts:AssumeRole\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"eks.amazonaws.com\"}}],\"Version\":\"2012-10-17\"}",
            "create_date": "2024-12-20T14:07:05Z",
            "description": "",
            "force_detach_policies": false,
            "id": "dev-eks-cluster-role",
            "inline_policy": [],
            "managed_policy_arns": [
              "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy",
              "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController"
            ],
            "max_session_duration": 3600,
            "name": "dev-eks-cluster-role",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": "",
            "tags": {},
            "tags_all": {},
            "unique_id": "AROA5FTZCXGO7KQ2P3HXY"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "eks_dev_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:role/eks-dev-role",
            "assume_role_policy": "{\"Statement\":[{\"Action\":\"sts:AssumeRole\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::905418291613:root\"},\"Sid\":\"\"}],\"Version\":\"2012-10-17\"}",
            "create_date": "2024-12-20T14:07:05Z",
            "description": "",
            "force_detach_policies": false,
            "id": "eks-dev-role",
            "inline_policy": [
              {
                "name": "eks-dev-fullaccess-policy",
                "policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Action\":[\"iam:ListRoles\",\"ssm:GetParameter\",\"eks:DescribeNodegroup\",\"eks:ListNodegroups\",\"eks:DescribeCluster\",\"eks:ListClusters\",\"eks:AccessKubernetesApi\",\"eks:ListUpdates\",\"eks:ListFargateProfiles\",\"eks:ListIdentityProviderConfigs\",\"eks:ListAddons\",\"eks:DescribeAddonVersions\"],\"Effect\":\"Allow\",\"Resource\":\"*\"}]}"
              }
            ],
            "managed_policy_arns": [],
            "max_session_duration": 3600,
            "name": "eks-dev-role",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": "",
            "tags": {
              "Name": "eks-dev-role"
            },
            "tags_all": {
              "Name": "eks-dev-role"
            },
            "unique_id": "AROA5FTZCXGOYAJWTDRZB"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "data.aws_caller_identity.current"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "eks_nodegroup_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:role/dev-eks-nodegroup-role",
            "assume_role_policy": "{\"Statement\":[{\"Action\":\"sts:AssumeRole\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ec2.amazonaws.com\"}}],\"Version\":\"2012-10-17\"}",
            "create_date": "2024-12-20T14:07:05Z",
            "description": "",
            "force_detach_policies": false,
            "id": "dev-eks-nodegroup-role",
            "inline_policy": [],
            "managed_policy_arns": [
              "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
              "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
              "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
              "arn:aws:iam::aws:policy/AutoScalingFullAccess",
              "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"
            ],
            "max_session_duration": 3600,
            "name": "dev-eks-nodegroup-role",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": "",
            "tags": {},
            "tags_all": {},
            "unique_id": "AROA5FTZCXGOUQFFXIV6J"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "eks_readonly_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:role/eks-readonly-role",
            "assume_role_policy": "{\"Statement\":[{\"Action\":\"sts:AssumeRole\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::905418291613:root\"},\"Sid\":\"\"}],\"Version\":\"2012-10-17\"}",
            "create_date": "2024-12-20T14:07:05Z",
            "description": "",
            "force_detach_policies": false,
            "id": "eks-readonly-role",
            "inline_policy": [
              {
                "name": "eks-readonly-policy",
                "policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Action\":[\"iam:ListRoles\",\"ssm:GetParameter\",\"eks:DescribeNodegroup\",\"eks:ListNodegroups\",\"eks:DescribeCluster\",\"eks:ListClusters\",\"eks:AccessKubernetesApi\",\"eks:ListUpdates\",\"eks:ListFargateProfiles\",\"eks:ListIdentityProviderConfigs\",\"eks:ListAddons\",\"eks:DescribeAddonVersions\"],\"Effect\":\"Allow\",\"Resource\":\"*\"}]}"
              }
            ],
            "managed_policy_arns": [],
            "max_session_duration": 3600,
            "name": "eks-readonly-role",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": "",
            "tags": {
              "Name": "eks-readonly-role"
            },
            "tags_all": {
              "Name": "eks-readonly-role"
            },
            "unique_id": "AROA5FTZCXGOYMCMDDIQA"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "data.aws_caller_identity.current"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "ingress_controller_iam_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:role/dev-ingress-kong-controller-role",
            "assume_role_policy": "{\"Statement\":[{\"Action\":\"sts:AssumeRoleWithWebIdentity\",\"Condition\":{\"StringEquals\":{\"oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358:aud\":\"sts.amazonaws.com\",\"oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358:sub\":\"system:serviceaccount:kong:kong-ingress-controller\"}},\"Effect\":\"Allow\",\"Principal\":{\"Federated\":\"arn:aws:iam::905418291613:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/BD9B8CBB7DD9D935201147689578F358\"},\"Sid\":\"\"}],\"Version\":\"2012-10-17\"}",
            "create_date": "2024-12-20T14:17:27Z",
            "description": "",
            "force_detach_policies": false,
            "id": "dev-ingress-kong-controller-role",
            "inline_policy": [],
            "managed_policy_arns": [
              "arn:aws:iam::905418291613:policy/dev-ingress-kong-controller-policy"
            ],
            "max_session_duration": 3600,
            "name": "dev-ingress-kong-controller-role",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": "",
            "tags": {},
            "tags_all": {},
            "unique_id": "AROA5FTZCXGORSXGZKKMD"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_openid_connect_provider.oidc_provider",
            "data.aws_partition.current",
            "data.tls_certificate.authentication"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "cluster_autoscaler_iam_role_policy_attach",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-cluster-autoscaler-2024122014172833940000000e",
            "policy_arn": "arn:aws:iam::905418291613:policy/dev-AmazonEKSClusterAutoscalerPolicy",
            "role": "dev-cluster-autoscaler"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_openid_connect_provider.oidc_provider",
            "aws_iam_policy.cluster_autoscaler_iam_policy",
            "aws_iam_role.cluster_autoscaler_iam_role",
            "data.aws_partition.current",
            "data.tls_certificate.authentication"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "ebs_csi_iam_role_policy_attach",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-amazonEks-ebs-iam-role-20241220141728537800000010",
            "policy_arn": "arn:aws:iam::905418291613:policy/dev-amazonEks-ebs-iam-policy",
            "role": "dev-amazonEks-ebs-iam-role"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_openid_connect_provider.oidc_provider",
            "aws_iam_policy.ebs_iam_policy",
            "aws_iam_role.ebs_iam_role",
            "data.aws_partition.current",
            "data.http.iam_policy",
            "data.tls_certificate.authentication"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "eks-AmazonEC2ContainerRegistryReadOnly",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-eks-nodegroup-role-20241220140707697900000007",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
            "role": "dev-eks-nodegroup-role"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_nodegroup_role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "eks-AmazonEKSClusterPolicy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-eks-cluster-role-20241220140705701500000003",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy",
            "role": "dev-eks-cluster-role"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_cluster_role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "eks-AmazonEKSVPCResourceController",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-eks-cluster-role-20241220140705651800000002",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController",
            "role": "dev-eks-cluster-role"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_cluster_role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "eks-AmazonEKSWorkerNodePolicy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-eks-nodegroup-role-20241220140707931300000008",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
            "role": "dev-eks-nodegroup-role"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_nodegroup_role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "eks-AmazonEKS_CNI_Policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-eks-nodegroup-role-20241220140705763800000004",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
            "role": "dev-eks-nodegroup-role"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_nodegroup_role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "eks-autoscaling",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-eks-nodegroup-role-20241220140705781100000005",
            "policy_arn": "arn:aws:iam::aws:policy/AutoScalingFullAccess",
            "role": "dev-eks-nodegroup-role"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_nodegroup_role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "eks_cloudwatch_container_insights",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-eks-nodegroup-role-20241220140705824800000006",
            "policy_arn": "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy",
            "role": "dev-eks-nodegroup-role"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_nodegroup_role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "lbc_iam_role_policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev-ingress-kong-controller-role-2024122014172843170000000f",
            "policy_arn": "arn:aws:iam::905418291613:policy/dev-ingress-kong-controller-policy",
            "role": "dev-ingress-kong-controller-role"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_openid_connect_provider.oidc_provider",
            "aws_iam_policy.ingress_kong_controller_policy",
            "aws_iam_role.ingress_controller_iam_role",
            "data.aws_partition.current",
            "data.tls_certificate.authentication"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_user",
      "name": "eks_dev_user",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:user/eksdev",
            "force_destroy": true,
            "id": "eksdev",
            "name": "eksdev",
            "path": "/",
            "permissions_boundary": "",
            "tags": {
              "Name": "eksdev"
            },
            "tags_all": {
              "Name": "eksdev"
            },
            "unique_id": "AIDA5FTZCXGOZR7XDV3YZ"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_user",
      "name": "eks_readonly_user",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:user/eksreadonly",
            "force_destroy": true,
            "id": "eksreadonly",
            "name": "eksreadonly",
            "path": "/",
            "permissions_boundary": "",
            "tags": {
              "Name": "eksreadonly"
            },
            "tags_all": {
              "Name": "eksreadonly"
            },
            "unique_id": "AIDA5FTZCXGOY7T4QK4ZL"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_user",
      "name": "eksadmin_user",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::905418291613:user/eksadmin",
            "force_destroy": true,
            "id": "eksadmin",
            "name": "eksadmin",
            "path": "/",
            "permissions_boundary": "",
            "tags": {
              "Name": "eksadmin"
            },
            "tags_all": {
              "Name": "eksadmin"
            },
            "unique_id": "AIDA5FTZCXGOU3OCHUWQV"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_key_pair",
      "name": "generated_key",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:key-pair/keypair",
            "fingerprint": "c1:88:94:af:f8:96:43:8e:90:ef:53:fa:23:35:9b:9f",
            "id": "keypair",
            "key_name": "keypair",
            "key_name_prefix": "",
            "key_pair_id": "key-00efcfa4fc3b8e740",
            "key_type": "rsa",
            "public_key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDFGn3qkQiAUWwV+qSl5L3H75ab0xQ6qNMzBAYEAUK9ota+4hD/vrStjGZXNCnTWTA0WasWiPJCINw0OYMJBvhfo23g8pc8k6c6AnLV8DBj2UhFE7915BRMxkasBkuGJp/vusQztmETN6SKiaqTiy2komQAVou4xlVEt/3ih/0iX71NuiLJ+ax2/WdjfiURnT2KPZ30zjgGdoqmk4/qomeTixNohsRWq9WxGAeVxkXh6Z//acAp8VkvKzzNPCHSnekuQCDe3KtRUPN07PPRB5l4KmxyKK6Q7Us/SPlVKTKBXnk5b5hWkC2oCMYEe2tHYEjXkCqjNdnHCn/+GF67tT//",
            "tags": {},
            "tags_all": {}
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "tls_private_key.tls"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route53_record",
      "name": "kong-admin",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "alias": [
              {
                "evaluate_target_health": true,
                "name": "a8b9c9cc3041845bd87a67d112cb1498-1887389677.us-east-1.elb.amazonaws.com",
                "zone_id": "Z35SXDOTRQ7X7K"
              }
            ],
            "allow_overwrite": null,
            "cidr_routing_policy": [],
            "failover_routing_policy": [],
            "fqdn": "admin-kong.khoaluantotnghiep.click",
            "geolocation_routing_policy": [],
            "geoproximity_routing_policy": [],
            "health_check_id": "",
            "id": "Z09469042UWTUNGX6SBEY_admin-kong.khoaluantotnghiep.click_A",
            "latency_routing_policy": [],
            "multivalue_answer_routing_policy": false,
            "name": "admin-kong.khoaluantotnghiep.click",
            "records": [],
            "set_identifier": "",
            "ttl": 0,
            "type": "A",
            "weighted_routing_policy": [],
            "zone_id": "Z09469042UWTUNGX6SBEY"
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_openid_connect_provider.oidc_provider",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role.ingress_controller_iam_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "aws_route53_zone.route53_domain_name",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "data.aws_elb_hosted_zone_id.elb_hosted_zone",
            "data.aws_partition.current",
            "data.kubernetes_service_v1.admin_kong_ingress_controller",
            "data.tls_certificate.authentication",
            "helm_release.kong-ingress-controller",
            "kubernetes_namespace.kong_ns",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route53_record",
      "name": "kong-manager",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "alias": [
              {
                "evaluate_target_health": true,
                "name": "a97cafaba82c34d19a8ee5c86c47757f-1089699414.us-east-1.elb.amazonaws.com",
                "zone_id": "Z35SXDOTRQ7X7K"
              }
            ],
            "allow_overwrite": null,
            "cidr_routing_policy": [],
            "failover_routing_policy": [],
            "fqdn": "manager-kong.khoaluantotnghiep.click",
            "geolocation_routing_policy": [],
            "geoproximity_routing_policy": [],
            "health_check_id": "",
            "id": "Z09469042UWTUNGX6SBEY_manager-kong.khoaluantotnghiep.click_A",
            "latency_routing_policy": [],
            "multivalue_answer_routing_policy": false,
            "name": "manager-kong.khoaluantotnghiep.click",
            "records": [],
            "set_identifier": "",
            "ttl": 0,
            "type": "A",
            "weighted_routing_policy": [],
            "zone_id": "Z09469042UWTUNGX6SBEY"
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_openid_connect_provider.oidc_provider",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role.ingress_controller_iam_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "aws_route53_zone.route53_domain_name",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "data.aws_elb_hosted_zone_id.elb_hosted_zone",
            "data.aws_partition.current",
            "data.kubernetes_service_v1.manager_kong_ingress_controller",
            "data.tls_certificate.authentication",
            "helm_release.kong-ingress-controller",
            "kubernetes_namespace.kong_ns",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route53_record",
      "name": "kong-proxy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "alias": [
              {
                "evaluate_target_health": true,
                "name": "aa30f0335b39b43edaafd701d6f04607-351278094.us-east-1.elb.amazonaws.com",
                "zone_id": "Z35SXDOTRQ7X7K"
              }
            ],
            "allow_overwrite": null,
            "cidr_routing_policy": [],
            "failover_routing_policy": [],
            "fqdn": "www.khoaluantotnghiep.click",
            "geolocation_routing_policy": [],
            "geoproximity_routing_policy": [],
            "health_check_id": "",
            "id": "Z09469042UWTUNGX6SBEY_www.khoaluantotnghiep.click_A",
            "latency_routing_policy": [],
            "multivalue_answer_routing_policy": false,
            "name": "www.khoaluantotnghiep.click",
            "records": [],
            "set_identifier": "",
            "ttl": 0,
            "type": "A",
            "weighted_routing_policy": [],
            "zone_id": "Z09469042UWTUNGX6SBEY"
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_openid_connect_provider.oidc_provider",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role.ingress_controller_iam_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "aws_route53_zone.route53_domain_name",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "data.aws_elb_hosted_zone_id.elb_hosted_zone",
            "data.aws_partition.current",
            "data.kubernetes_service_v1.proxy_kong_ingress_controller",
            "data.tls_certificate.authentication",
            "helm_release.kong-ingress-controller",
            "kubernetes_namespace.kong_ns",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route53_zone",
      "name": "route53_domain_name",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:route53:::hostedzone/Z09469042UWTUNGX6SBEY",
            "comment": "Managed by Terraform",
            "delegation_set_id": "",
            "force_destroy": false,
            "id": "Z09469042UWTUNGX6SBEY",
            "name": "khoaluantotnghiep.click",
            "name_servers": [
              "ns-1082.awsdns-07.org",
              "ns-1567.awsdns-03.co.uk",
              "ns-63.awsdns-07.com",
              "ns-977.awsdns-58.net"
            ],
            "primary_name_server": "ns-63.awsdns-07.com",
            "tags": {
              "Environment": "dev"
            },
            "tags_all": {
              "Environment": "dev"
            },
            "vpc": [],
            "zone_id": "Z09469042UWTUNGX6SBEY"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group",
      "name": "allow_tls",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:security-group/sg-069cd5f52d5c13bb1",
            "description": "Allow TLS inbound traffic",
            "egress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "Allows outbound traffic",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              }
            ],
            "id": "sg-069cd5f52d5c13bb1",
            "ingress": [
              {
                "cidr_blocks": [
                  "10.0.1.0/24"
                ],
                "description": "TLS from VPC",
                "from_port": 22,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 22
              }
            ],
            "name": "allow_tls2024122014071737610000000a",
            "name_prefix": "allow_tls",
            "owner_id": "905418291613",
            "revoke_rules_on_delete": false,
            "tags": {},
            "tags_all": {},
            "timeouts": null,
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6OTAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "dependencies": [
            "module.vpc.aws_vpc.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "argocd",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "argo-cd",
            "cleanup_on_fail": true,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "argocd",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v2.7.6",
                "chart": "argo-cd",
                "first_deployed": 1734704256,
                "last_deployed": 1734704256,
                "name": "argocd",
                "namespace": "argocd",
                "notes": "\nDEPRECATED option server.extraArgs.\"--insecure\" - Use configs.params.server.insecure\n\nIn order to access the server UI you have the following options:\n\n1. kubectl port-forward service/argocd-server -n argocd 8080:443\n\n    and then open the browser on http://localhost:8080 and accept the certificate\n\n2. enable ingress in the values file `server.ingress.enabled` and either\n      - Add the annotation for ssl passthrough: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough\n      - Set the `configs.params.\"server.insecure\"` in the values file and terminate SSL at your ingress: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts\n\n\nAfter reaching the UI the first time you can login with username: admin and the random password generated during the installation. You can find the password by running:\n\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n\n(You should delete the initial secret afterwards as suggested by the Getting Started Guide: https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli)\n",
                "revision": 1,
                "values": "{\"dex\":{\"enabled\":false},\"global\":{\"image\":{\"tag\":\"v2.6.6\"}},\"server\":{\"extraArgs\":[\"--insecure\"]}}",
                "version": "5.36.7"
              }
            ],
            "name": "argocd",
            "namespace": "argocd",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://argoproj.github.io/argo-helm",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1500,
            "upgrade_install": null,
            "values": [
              "---\nglobal:\n  image:\n    tag: \"v2.6.6\"\n\ndex:\n  enabled: false\n\nserver:\n  extraArgs:\n    - --insecure"
            ],
            "verify": false,
            "version": "5.36.7",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "kubernetes_namespace.argocd_ns",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "argocd_rollout",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "argo-rollouts",
            "cleanup_on_fail": true,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "my-release",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v1.7.2",
                "chart": "argo-rollouts",
                "first_deployed": 1734704984,
                "last_deployed": 1734704984,
                "name": "my-release",
                "namespace": "argocd-rollout",
                "notes": "",
                "revision": 1,
                "values": "{\"apiVersionOverrides\":{\"ingress\":\"\"},\"clusterInstall\":true,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"controller\":{\"affinity\":{},\"component\":\"rollouts-controller\",\"containerPorts\":{\"healthz\":8080,\"metrics\":8090},\"createClusterRole\":true,\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"argoproj/argo-rollouts\",\"tag\":\"\"},\"initContainers\":[],\"livenessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/healthz\",\"port\":\"healthz\"},\"initialDelaySeconds\":30,\"periodSeconds\":20,\"successThreshold\":1,\"timeoutSeconds\":10},\"logging\":{\"format\":\"text\",\"kloglevel\":\"0\",\"level\":\"info\"},\"metricProviderPlugins\":[],\"metrics\":{\"enabled\":true,\"service\":{\"annotations\":{},\"port\":8090,\"portName\":\"metrics\"},\"serviceMonitor\":{\"additionalAnnotations\":{},\"additionalLabels\":{},\"enabled\":true,\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[]}},\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":null,\"minAvailable\":null},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/metrics\",\"port\":\"metrics\"},\"initialDelaySeconds\":15,\"periodSeconds\":5,\"successThreshold\":1,\"timeoutSeconds\":4},\"replicas\":2,\"resources\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"trafficRouterPlugins\":[],\"volumeMounts\":[],\"volumes\":[]},\"crdAnnotations\":{},\"createClusterAggregateRoles\":true,\"dashboard\":{\"affinity\":{},\"component\":\"rollouts-dashboard\",\"containerSecurityContext\":{},\"createClusterRole\":true,\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"enabled\":true,\"extraArgs\":[],\"extraEnv\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"argoproj/kubectl-argo-rollouts\",\"tag\":\"\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraPaths\":[],\"hosts\":[],\"ingressClassName\":\"\",\"labels\":{},\"pathType\":\"Prefix\",\"paths\":[\"/\"],\"tls\":[]},\"logging\":{\"kloglevel\":\"0\",\"level\":\"info\"},\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":null,\"minAvailable\":null},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"runAsNonRoot\":true},\"priorityClassName\":\"\",\"readonly\":false,\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"externalIPs\":[],\"labels\":{},\"loadBalancerClass\":\"\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":null,\"port\":3100,\"portName\":\"dashboard\",\"targetPort\":3100,\"type\":\"LoadBalancer\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]},\"extraObjects\":[],\"fullnameOverride\":null,\"global\":{\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"revisionHistoryLimit\":10},\"imagePullSecrets\":[],\"installCRDs\":true,\"keepCRDs\":true,\"kubeVersionOverride\":\"\",\"nameOverride\":null,\"notifications\":{\"configmap\":{\"create\":true},\"notifiers\":{},\"secret\":{\"annotations\":{},\"create\":false,\"items\":{}},\"subscriptions\":[],\"templates\":{},\"triggers\":{}},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"runAsNonRoot\":true},\"providerRBAC\":{\"additionalRules\":[],\"enabled\":true,\"providers\":{\"ambassador\":true,\"apisix\":true,\"awsAppMesh\":true,\"awsLoadBalancerController\":true,\"contour\":true,\"gatewayAPI\":true,\"glooPlatform\":true,\"istio\":true,\"smi\":true,\"traefik\":true}},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"serviceAnnotations\":{}}",
                "version": "2.38.0"
              }
            ],
            "name": "my-release",
            "namespace": "argocd-rollout",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://argoproj.github.io/argo-helm",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1500,
            "upgrade_install": null,
            "values": [
              "# -- Install and upgrade CRDs\ninstallCRDs: true\n# -- Keep CRD's on helm uninstall\nkeepCRDs: true\n\n# -- `false` runs controller in namespaced mode (does not require cluster RBAC)\nclusterInstall: true\n\n# -- flag to enable creation of cluster aggregate roles (requires cluster RBAC)\ncreateClusterAggregateRoles: true\n\n# -- String to partially override \"argo-rollouts.fullname\" template\nnameOverride:\n\n# -- String to fully override \"argo-rollouts.fullname\" template\nfullnameOverride:\n\n## Override APIVersions\n## If you want to template helm charts but cannot access k8s API server\n## you can set api versions here\napiVersionOverrides:\n  # -- String to override apiVersion of ingresses rendered by this helm chart\n  ingress: \"\" # networking.k8s.io/v1beta1\n\n# -- Override the Kubernetes version, which is used to evaluate certain manifests\nkubeVersionOverride: \"\"\n\n# -- Additional manifests to deploy within the chart. A list of objects.\n## Can be used to add secrets for Analysis with 3rd-party monitoring solutions.\nextraObjects: []\n  # - apiVersion: v1\n  #   kind: Secret\n  #   metadata:\n  #     name: datadog\n  #   type: Opaque\n  #   data:\n  #     address: https://api.datadoghq.com\n  #     api-key: \u003cdatadog-api-key\u003e\n  #     app-key: \u003cdatadog-app-key\u003e\n\nglobal:\n  # -- Annotations for all deployed Deployments\n  deploymentAnnotations: {}\n  # -- Labels for all deployed Deployments\n  deploymentLabels: {}\n  # -- Number of old deployment ReplicaSets to retain. The rest will be garbage collected.\n  revisionHistoryLimit: 10\n\ncontroller:\n  # -- Value of label `app.kubernetes.io/component`\n  component: rollouts-controller\n  # -- Annotations to be added to the controller deployment\n  deploymentAnnotations: {}\n  # -- Labels to be added to the controller deployment\n  deploymentLabels: {}\n  # -- Annotations to be added to application controller pods\n  podAnnotations: {}\n  # -- Labels to be added to the application controller pods\n  podLabels: {}\n  # -- [Node selector]\n  nodeSelector: {}\n  # -- [Tolerations] for use with node taints\n  tolerations: []\n  # -- Assign custom [affinity] rules to the deployment\n  affinity: {}\n  logging:\n    # -- Set the logging level (one of: `debug`, `info`, `warn`, `error`)\n    level: info\n    # -- Set the klog logging level\n    kloglevel: \"0\"\n    # -- Set the logging format (one of: `text`, `json`)\n    format: \"text\"\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the controller\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n  # - maxSkew: 1\n  #   topologyKey: topology.kubernetes.io/zone\n  #   whenUnsatisfiable: DoNotSchedule\n\n  # -- [priorityClassName] for the controller\n  priorityClassName: \"\"\n  # -- The number of controller pods to run\n  replicas: 2\n  image:\n    # -- Registry to use\n    registry: quay.io\n    # -- Repository to use\n    repository: argoproj/argo-rollouts\n    # -- Overrides the image tag (default is the chart appVersion)\n    tag: \"\"\n    # -- Image pull policy\n    pullPolicy: IfNotPresent\n\n  # -- Additional command line arguments to pass to rollouts-controller.  A list of flags.\n  extraArgs: []\n\n  # -- Additional environment variables for rollouts-controller. A list of name/value maps.\n  extraEnv: []\n    # - name: AWS_REGION\n    #   value: us-east-1\n\n  # -- Literal yaml for extra containers to be added to controller deployment.\n  ## Additional containers to add to the rollouts controller deployment\n  ## This will be rendered as the literal yaml\n  extraContainers: []\n\n  # -- Init containers to add to the rollouts controller pod\n  ## This will be rendered as the literal yaml\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3.8\n  #    command: [sh, -c]\n  #    args:\n  #      -  ls\n\n  # -- Resource limits and requests for the controller pods.\n  resources: {}\n  #  limits:\n  #    cpu: 100m\n  #    memory: 128Mi\n  #    ephemeral-storage: 1Gi\n  #  requests:\n  #    cpu: 50m\n  #    memory: 64Mi\n\n  # -- flag to enable creation of cluster controller role (requires cluster RBAC)\n  createClusterRole: true\n\n  # Controller container ports\n  containerPorts:\n    # -- Metrics container port\n    metrics: 8090\n    # -- Healthz container port\n    healthz: 8080\n\n  metrics:\n    # -- Deploy metrics service\n    enabled: true\n    service:\n      # -- Metrics service port name\n      portName: metrics\n      # -- Metrics service port\n      port: 8090\n      # -- Service annotations\n      annotations: {}\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: true\n      # -- Namespace to be used for the ServiceMonitor\n      namespace: \"\"\n      # -- Labels to be added to the ServiceMonitor\n      additionalLabels: {}\n      # -- Annotations to be added to the ServiceMonitor\n      additionalAnnotations: {}\n      # -- RelabelConfigs to apply to samples before scraping\n      relabelings: []\n      # -- MetricRelabelConfigs to apply to samples before ingestion\n      metricRelabelings: []\n\n  # -- Configure liveness [probe] for the controller\n  # @default -- See [values.yaml]\n  livenessProbe:\n    httpGet:\n      path: /healthz\n      port: healthz\n    initialDelaySeconds: 30\n    periodSeconds: 20\n    failureThreshold: 3\n    successThreshold: 1\n    timeoutSeconds: 10\n\n  # -- Configure readiness [probe] for the controller\n  # @default -- See [values.yaml]\n  readinessProbe:\n    httpGet:\n      path: /metrics\n      port: metrics\n    initialDelaySeconds: 15\n    periodSeconds: 5\n    failureThreshold: 3\n    successThreshold: 1\n    timeoutSeconds: 4\n\n  ## Configure Pod Disruption Budget for the controller\n  pdb:\n    # -- Labels to be added to controller [Pod Disruption Budget]\n    labels: {}\n    # -- Annotations to be added to controller [Pod Disruption Budget]\n    annotations: {}\n    # -- Deploy a [Pod Disruption Budget] for the controller\n    enabled: false\n    # -- Minimum number / percentage of pods that should remain scheduled\n    minAvailable: # 1\n    # -- Maximum number / percentage of pods that may be made unavailable\n    maxUnavailable: # 0\n\n  # -- Additional volumes to add to the controller pod\n  volumes: []\n    # - configMap:\n    #     name: my-certs-cm\n    #   name: my-certs\n\n  # -- Additional volumeMounts to add to the controller container\n  volumeMounts: []\n    # - mountPath: /etc/ssl/certs\n    #   name: my-certs\n\n  # -- Configures 3rd party metric providers for controller\n  ## Ref: https://argo-rollouts.readthedocs.io/en/stable/analysis/plugins/\n  metricProviderPlugins: []\n    # - name: \"argoproj-labs/sample-prometheus\" # name of the plugin, it must match the name required by the plugin so that it can find its configuration\n    #   location: \"file://./my-custom-plugin\" # supports http(s):// urls and file://\n\n  # -- Configures 3rd party traffic router plugins for controller\n  ## Ref: https://argo-rollouts.readthedocs.io/en/stable/features/traffic-management/plugins/\n  trafficRouterPlugins: []\n    # - name: \"argoproj-labs/sample-nginx\" # name of the plugin, it must match the name required by the plugin so it can find it's configuration\n    #   location: \"file://./my-custom-plugin\" # supports http(s):// urls and file://\n\nserviceAccount:\n  # -- Specifies whether a service account should be created\n  create: true\n  # -- Annotations to add to the service account\n  annotations: {}\n  # -- The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n\n# -- Annotations to be added to all CRDs\ncrdAnnotations: {}\n\n# -- Annotations for the all deployed pods\npodAnnotations: {}\n\n# -- Security Context to set on pod level\npodSecurityContext:\n  runAsNonRoot: true\n\n# -- Security Context to set on container level\ncontainerSecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: true\n  seccompProfile:\n    type: RuntimeDefault\n\n# -- Annotations to be added to the Rollout service\nserviceAnnotations: {}\n\n# -- Labels to be added to the Rollout pods\npodLabels: {}\n\n# -- Secrets with credentials to pull images from a private registry. Registry secret names as an array.\nimagePullSecrets: []\n# - name: argo-pull-secret\n\nproviderRBAC:\n  # -- Toggles addition of provider-specific RBAC rules to the controller Role and ClusterRole\n  enabled: true\n  # providerRBAC.enabled must be true in order to toggle the individual providers\n  providers:\n    # -- Adds RBAC rules for the Istio provider\n    istio: true\n    # -- Adds RBAC rules for the SMI provider\n    smi: true\n    # -- Adds RBAC rules for the Ambassador provider\n    ambassador: true\n    # -- Adds RBAC rules for the AWS Load Balancer Controller provider\n    awsLoadBalancerController: true\n    # -- Adds RBAC rules for the AWS App Mesh provider\n    awsAppMesh: true\n    # -- Adds RBAC rules for the Traefik provider\n    traefik: true\n    # -- Adds RBAC rules for the Apisix provider\n    apisix: true\n    # -- Adds RBAC rules for the Contour provider, see `https://github.com/argoproj-labs/rollouts-plugin-trafficrouter-contour/blob/main/README.md`\n    contour: true\n    # -- Adds RBAC rules for the Gloo Platform provider, see `https://github.com/argoproj-labs/rollouts-plugin-trafficrouter-glooplatform/blob/main/README.md`\n    glooPlatform: true\n    # -- Adds RBAC rules for the Gateway API provider\n    gatewayAPI: true\n  # -- Additional RBAC rules for others providers\n  additionalRules: []\n\ndashboard:\n  # -- Deploy dashboard server\n  enabled: true\n  # -- Set cluster role to readonly\n  readonly: false\n  # -- Value of label `app.kubernetes.io/component`\n  component: rollouts-dashboard\n  # -- Annotations to be added to the dashboard deployment\n  deploymentAnnotations: {}\n  # -- Labels to be added to the dashboard deployment\n  deploymentLabels: {}\n  # -- Annotations to be added to application dashboard pods\n  podAnnotations: {}\n  # -- Labels to be added to the application dashboard pods\n  podLabels: {}\n  # -- [Node selector]\n  nodeSelector: {}\n  # -- [Tolerations] for use with node taints\n  tolerations: []\n  # -- Assign custom [affinity] rules to the deployment\n  affinity: {}\n  logging:\n    # -- Set the logging level (one of: `debug`, `info`, `warn`, `error`)\n    level: info\n    # -- Set the klog logging level\n    kloglevel: \"0\"\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the dashboard server\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n  # - maxSkew: 1\n  #   topologyKey: topology.kubernetes.io/zone\n  #   whenUnsatisfiable: DoNotSchedule\n\n  # -- [priorityClassName] for the dashboard server\n  priorityClassName: \"\"\n\n  # -- flag to enable creation of dashbord cluster role (requires cluster RBAC)\n  createClusterRole: true\n\n  # -- The number of dashboard pods to run\n  replicas: 1\n  image:\n    # -- Registry to use\n    registry: quay.io\n    # --  Repository to use\n    repository: argoproj/kubectl-argo-rollouts\n    # -- Overrides the image tag (default is the chart appVersion)\n    tag: \"\"\n    # -- Image pull policy\n    pullPolicy: IfNotPresent\n  # -- Additional command line arguments to pass to rollouts-dashboard. A list of flags.\n  extraArgs: []\n  # -- Additional environment variables for rollouts-dashboard. A list of name/value maps.\n  extraEnv: []\n    # - name: FOO\n    #   value: bar\n  # -- Resource limits and requests for the dashboard pods.\n  resources: {}\n  # -- Security Context to set on pod level\n  podSecurityContext:\n    runAsNonRoot: true\n  # -- Security Context to set on container level\n  containerSecurityContext: {}\n  service:\n    # -- Sets the type of the Service\n    type: LoadBalancer\n    # -- The class of the load balancer implementation\n    loadBalancerClass: \"\"\n    # -- LoadBalancer will get created with the IP specified in this field\n    loadBalancerIP: \"\"\n    # -- Source IP ranges to allow access to service from\n    loadBalancerSourceRanges: []\n    # -- Dashboard service external IPs\n    externalIPs: []\n    # -- Service annotations\n    annotations: {}\n    # -- Service labels\n    labels: {}\n    # -- Service port name\n    portName: dashboard\n    # -- Service port\n    port: 3100\n    # -- Service target port\n    targetPort: 3100\n    # -- (int) Service nodePort\n    nodePort:\n  serviceAccount:\n    # -- Specifies whether a dashboard service account should be created\n    create: true\n    # -- Annotations to add to the dashboard service account\n    annotations: {}\n    # -- The name of the service account to use.\n    # If not set and create is true, a name is generated using the fullname template\n    name: \"\"\n\n  ## Configure Pod Disruption Budget for the dashboard\n  pdb:\n    # -- Labels to be added to dashboard [Pod Disruption Budget]\n    labels: {}\n    # -- Annotations to be added to dashboard [Pod Disruption Budget]\n    annotations: {}\n    # -- Deploy a [Pod Disruption Budget] for the dashboard\n    enabled: false\n    # -- Minimum number / percentage of pods that should remain scheduled\n    minAvailable: # 1\n    # -- Maximum number / percentage of pods that may be made unavailable\n    maxUnavailable: # 0\n\n  ## Ingress configuration.\n  ## ref: https://kubernetes.io/docs/user-guide/ingress/\n  ##\n  ingress:\n    # -- Enable dashboard ingress support\n    enabled: false\n    # -- Dashboard ingress annotations\n    annotations: {}\n    # -- Dashboard ingress labels\n    labels: {}\n    # -- Dashboard ingress class name\n    ingressClassName: \"\"\n\n    # -- Dashboard ingress hosts\n    ## Argo Rollouts Dashboard Ingress.\n    ## Hostnames must be provided if Ingress is enabled.\n    ## Secrets must be manually created in the namespace\n    hosts: []\n      # - argorollouts.example.com\n\n    # -- Dashboard ingress paths\n    paths:\n      - /\n    # -- Dashboard ingress path type\n    pathType: Prefix\n    # -- Dashboard ingress extra paths\n    extraPaths: []\n      # - path: /*\n      #   backend:\n      #     serviceName: ssl-redirect\n      #     servicePort: use-annotation\n      ## for Kubernetes \u003e=1.19 (when \"networking.k8s.io/v1\" is used)\n      # - path: /*\n      #   pathType: Prefix\n      #   backend:\n      #     service\n      #       name: ssl-redirect\n      #       port:\n      #         name: use-annotation\n\n    # -- Dashboard ingress tls\n    tls: []\n      # - secretName: argorollouts-example-tls\n      #   hosts:\n      #     - argorollouts.example.com\n\n  # -- Additional volumes to add to the dashboard pod\n  volumes: []\n\n  # -- Additional volumeMounts to add to the dashboard container\n  volumeMounts: []\n\nnotifications:\n  configmap:\n    # -- Whether to create notifications configmap\n    create: true\n\n  secret:\n    # -- Whether to create notifications secret.\n    ## If you want to manually create secret, do not forget to add proper label to it: \"app.kubernetes.io/component: {{ .Values.controller.component }}\".\n    create: false\n    # -- Generic key:value pairs to be inserted into the notifications secret\n    items: {}\n      # slack-token:\n    # -- Annotations to be added to the notifications secret\n    annotations: {}\n\n  # -- Configures notification services\n  notifiers: {}\n    # service.slack: |\n    #   token: $slack-token\n\n  # -- Notification templates\n  templates: {}\n    # template.my-purple-template: |\n    #   message: |\n    #     Rollout {{.rollout.metadata.name}} has purple image\n    #   slack:\n    #       attachments: |\n    #           [{\n    #             \"title\": \"{{ .rollout.metadata.name}}\",\n    #             \"color\": \"#800080\"\n    #           }]\n\n  # -- The trigger defines the condition when the notification should be sent\n  triggers: {}\n    # trigger.on-purple: |\n    #   - send: [my-purple-template]\n    #     when: rollout.spec.template.spec.containers[0].image == 'argoproj/rollouts-demo:purple'\n\n  # -- The subscriptions define the subscriptions to the triggers in a general way for all rollouts\n  subscriptions: []\n    # - recipients:\n    #   - slack:\u003cchannel\u003e\n    #   triggers:\n    #   - on-rollout-completed\n    #   - on-rollout-aborted\n\n"
            ],
            "verify": false,
            "version": "2.38.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "kubernetes_namespace.argocd_rollout_ns",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "cluster_autoscaler_release",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "cluster-autoscaler",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "dev-cluster-autoscaler",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.31.0",
                "chart": "cluster-autoscaler",
                "first_deployed": 1734704256,
                "last_deployed": 1734704256,
                "name": "dev-cluster-autoscaler",
                "namespace": "kube-system",
                "notes": "\n\nTo verify that cluster-autoscaler has started, run:\n\n  kubectl --namespace=kube-system get pods -l \"app.kubernetes.io/name=aws-cluster-autoscaler,app.kubernetes.io/instance=dev-cluster-autoscaler\"",
                "revision": 1,
                "values": "{\"autoDiscovery\":{\"clusterName\":\"ekscluster\"},\"awsRegion\":\"us-east-1\",\"cloudProvider\":\"aws\",\"rbac\":{\"serviceAccount\":{\"annotations\":{\"eks.amazonaws.com/role-arn\":\"arn:aws:iam::905418291613:role/dev-cluster-autoscaler\"},\"name\":\"cluster-autoscaler\"}}}",
                "version": "9.43.2"
              }
            ],
            "name": "dev-cluster-autoscaler",
            "namespace": "kube-system",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://kubernetes.github.io/autoscaler",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "autoDiscovery.clusterName",
                "type": "",
                "value": "ekscluster"
              },
              {
                "name": "awsRegion",
                "type": "",
                "value": "us-east-1"
              },
              {
                "name": "cloudProvider",
                "type": "",
                "value": "aws"
              },
              {
                "name": "rbac.serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn",
                "type": "",
                "value": "arn:aws:iam::905418291613:role/dev-cluster-autoscaler"
              },
              {
                "name": "rbac.serviceAccount.name",
                "type": "",
                "value": "cluster-autoscaler"
              }
            ],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": null,
            "verify": false,
            "version": "9.43.2",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_openid_connect_provider.oidc_provider",
            "aws_iam_role.cluster_autoscaler_iam_role",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "data.aws_partition.current",
            "data.tls_certificate.authentication",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "ebs_csi_driver",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "aws-ebs-csi-driver",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "dev-ebs-csi-driver",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.38.1",
                "chart": "aws-ebs-csi-driver",
                "first_deployed": 1734704257,
                "last_deployed": 1734704257,
                "name": "dev-ebs-csi-driver",
                "namespace": "kube-system",
                "notes": "To verify that aws-ebs-csi-driver has started, run:\n\n    kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=dev-ebs-csi-driver\"\n\n[ACTION REQUIRED] Update to the EBS CSI Driver IAM Policy\n\nDue to an upcoming change in handling of IAM polices for the CreateVolume API when creating a volume from an EBS snapshot, a change to your EBS CSI Driver policy may be needed. For more information and remediation steps, see GitHub issue #2190 (https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/2190). This change affects all versions of the EBS CSI Driver and action may be required even on clusters where the driver is not upgraded.\n",
                "revision": 1,
                "values": "{\"controller\":{\"serviceAccount\":{\"annotations\":{\"eks.amazonaws.com/role-arn\":\"arn:aws:iam::905418291613:role/dev-amazonEks-ebs-iam-role\"},\"name\":\"ebs-csi-controller-sa\"}}}",
                "version": "2.38.1"
              }
            ],
            "name": "dev-ebs-csi-driver",
            "namespace": "kube-system",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://kubernetes-sigs.github.io/aws-ebs-csi-driver",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "controller.serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn",
                "type": "string",
                "value": "arn:aws:iam::905418291613:role/dev-amazonEks-ebs-iam-role"
              },
              {
                "name": "controller.serviceAccount.name",
                "type": "",
                "value": "ebs-csi-controller-sa"
              }
            ],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": null,
            "verify": false,
            "version": "2.38.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_openid_connect_provider.oidc_provider",
            "aws_iam_role.ebs_iam_role",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "data.aws_partition.current",
            "data.tls_certificate.authentication",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "kong-ingress-controller",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "kong",
            "cleanup_on_fail": true,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "kong",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "3.8",
                "chart": "kong",
                "first_deployed": 1734704262,
                "last_deployed": 1734704262,
                "name": "kong",
                "namespace": "kong",
                "notes": "CHART NAME: postgresql\nCHART VERSION: 11.9.13\nAPP VERSION: 14.5.0\n\n** Please be patient while the chart is being deployed **\n\nPostgreSQL can be accessed via port 5432 on the following DNS names from within your cluster:\n\n    kong-postgresql.kong.svc.cluster.local - Read/Write connection\n\nTo get the password for \"postgres\" run:\n\n    export POSTGRES_ADMIN_PASSWORD=$(kubectl get secret --namespace kong kong-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 -d)\n\nTo get the password for \"kong\" run:\n\n    export POSTGRES_PASSWORD=$(kubectl get secret --namespace kong kong-postgresql -o jsonpath=\"{.data.password}\" | base64 -d)\n\nTo connect to your database run the following command:\n\n    kubectl run kong-postgresql-client --rm --tty -i --restart='Never' --namespace kong --image docker.io/bitnami/postgresql:13.11.0-debian-11-r20 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" \\\n      --command -- psql --host kong-postgresql -U kong -d kong -p 5432\n\n    \u003e NOTE: If you access the container using bash, make sure that you execute \"/opt/bitnami/scripts/postgresql/entrypoint.sh /bin/bash\" in order to avoid the error \"psql: local user with ID 1001} does not exist\"\n\nTo connect to your database from outside the cluster execute the following commands:\n\n    kubectl port-forward --namespace kong svc/kong-postgresql 5432:5432 \u0026\n    PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U kong -d kong -p 5432\n\nTo connect to Kong, please execute the following commands:\n\nHOST=$(kubectl get svc --namespace kong kong-kong-proxy -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nPORT=$(kubectl get svc --namespace kong kong-kong-proxy -o jsonpath='{.spec.ports[0].port}')\nexport PROXY_IP=${HOST}:${PORT}\ncurl $PROXY_IP\n\nOnce installed, please follow along the getting started guide to start using\nKong: https://docs.konghq.com/kubernetes-ingress-controller/latest/guides/getting-started/\n\n",
                "revision": 1,
                "values": "{\"admin\":{\"annotations\":{},\"enabled\":true,\"http\":{\"containerPort\":8001,\"enabled\":true,\"parameters\":[],\"servicePort\":8001},\"labels\":{},\"loadBalancerClass\":null,\"tls\":{\"client\":{\"caBundle\":\"\",\"secretName\":\"\"},\"containerPort\":8444,\"enabled\":true,\"ingress\":{\"annotations\":{},\"enabled\":false,\"hostname\":null,\"ingressClassName\":null,\"path\":\"/\",\"pathType\":\"ImplementationSpecific\"},\"parameters\":[\"http2\"],\"servicePort\":8444},\"type\":\"LoadBalancer\"},\"enterprise\":{\"enabled\":true,\"rbac\":{\"admin_gui_auth\":\"basic-auth\",\"admin_gui_auth_conf_secret\":\"kong-admin-auth\",\"enabled\":false,\"session_conf_secret\":\"kong-session-config\"}},\"env\":{\"KONG_ENFORCE_RBAC\":\"on\",\"KONG_STATUS_LISTEN\":\"0.0.0.0:8100\",\"admin_access_log\":\"/dev/stdout\",\"admin_error_log\":\"/dev/stderr\",\"admin_gui_access_log\":\"/dev/stdout\",\"admin_gui_api_url\":\"http://admin-kong.khoaluantotnghiep.click:8001\",\"admin_gui_error_log\":\"/dev/stderr\",\"admin_gui_url\":\"http://manager-kong.khoaluantotnghiep.click:8002\",\"database\":\"postgres\",\"nginx_worker_processes\":\"2\",\"password\":\"kong\",\"pg_database\":\"kong\",\"pg_host\":\"kong-postgresql.kong.svc.cluster.local\",\"pg_ssl\":\"off\",\"pg_ssl_verify\":\"off\",\"pg_user\":\"kong\",\"portal_api_access_log\":\"/dev/stdout\",\"portal_api_error_log\":\"/dev/stderr\",\"prefix\":\"/kong_prefix/\",\"proxy_access_log\":\"/dev/stdout\",\"proxy_error_log\":\"/dev/stderr\",\"router_flavor\":\"traditional\"},\"extraObjects\":[{\"apiVersion\":\"configuration.konghq.com/v1\",\"config\":{\"bandwidth_metrics\":true,\"latency_metrics\":true,\"per_consumer\":false,\"status_code_metrics\":true,\"upstream_health_metrics\":true},\"kind\":\"KongClusterPlugin\",\"metadata\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"kong\"},\"labels\":{\"global\":\"true\"},\"name\":\"prometheus\"},\"plugin\":\"prometheus\"}],\"image\":{\"effectiveSemver\":null,\"pullPolicy\":\"IfNotPresent\",\"repository\":\"kong/kong-gateway\",\"tag\":\"3.6\"},\"manager\":{\"annotations\":{},\"enabled\":true,\"http\":{\"containerPort\":8002,\"enabled\":true,\"parameters\":[],\"servicePort\":8002},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hostname\":null,\"ingressClassName\":null,\"path\":\"/\",\"pathType\":\"ImplementationSpecific\"},\"labels\":{},\"loadBalancerClass\":null,\"tls\":{\"containerPort\":8445,\"enabled\":true,\"parameters\":[\"http2\"],\"servicePort\":8445},\"type\":\"LoadBalancer\"},\"migrations\":{\"enabled\":true,\"postUpgrade\":true,\"preUpgrade\":true},\"namespace\":\"kong\",\"postgresql\":{\"auth\":{\"database\":\"kong\",\"username\":\"kong\"},\"enabled\":true,\"image\":{\"tag\":\"13.11.0-debian-11-r20\"},\"primary\":{\"persistence\":{\"enabled\":true,\"size\":\"10Gi\",\"storageClass\":\"gp2\"}},\"service\":{\"ports\":{\"postgresql\":\"5432\"}}},\"proxy\":{\"annotations\":{},\"enabled\":true,\"http\":{\"containerPort\":8000,\"enabled\":true,\"parameters\":[],\"servicePort\":8000},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hostname\":null,\"hosts\":[],\"ingressClassName\":null,\"labels\":{},\"path\":\"/\",\"pathType\":\"ImplementationSpecific\"},\"labels\":{\"enable-metrics\":\"true\"},\"loadBalancerClass\":null,\"nameOverride\":\"\",\"stream\":[],\"tls\":{\"appProtocol\":\"\",\"containerPort\":8443,\"enabled\":true,\"parameters\":[\"http2\"],\"servicePort\":443},\"type\":\"LoadBalancer\"},\"resources\":{\"limits\":{\"cpu\":\"500m\",\"memory\":\"512Mi\"},\"requests\":{\"cpu\":\"250m\",\"memory\":\"64Mi\"}}}",
                "version": "2.46.0"
              }
            ],
            "name": "kong",
            "namespace": "kong",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.konghq.com",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1500,
            "upgrade_install": null,
            "values": [
              "# Specify Kong's Docker image and repository details here\nimage:\n  repository: kong/kong-gateway\n  tag: \"3.6\"\n  # Kong Enterprise\n  # repository: kong/kong-gateway\n  # tag: \"3.5\"\n\n  # Specify a semver version if your image tag is not one (e.g. \"nightly\")\n  effectiveSemver:\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  # pullSecrets:n\n  #   - myRegistrKeySecretName\n\n# Specify Kong admin API service and listener configuration\n# Specify Kong admin API service and listener configuration\n\nenv:  \n  admin_gui_url: \"http://manager-kong.khoaluantotnghiep.click:8002\"\n  admin_gui_api_url: \"http://admin-kong.khoaluantotnghiep.click:8001\"\n  KONG_STATUS_LISTEN: \"0.0.0.0:8100\"\n  KONG_ENFORCE_RBAC: \"on\"\n  database: postgres\n  pg_database: kong\n  pg_ssl: \"off\"\n  pg_ssl_verify: \"off\"\n  password: kong\n  pg_host: kong-postgresql.kong.svc.cluster.local\n  pg_user: kong\n  router_flavor: \"traditional\"\n  nginx_worker_processes: \"2\"\n  proxy_access_log: /dev/stdout\n  admin_access_log: /dev/stdout\n  admin_gui_access_log: /dev/stdout\n  portal_api_access_log: /dev/stdout\n  proxy_error_log: /dev/stderr\n  admin_error_log: /dev/stderr\n  admin_gui_error_log: /dev/stderr\n  portal_api_error_log: /dev/stderr\n  prefix: /kong_prefix/\n\nresources:\n  requests:\n    memory: \"64Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"512Mi\"\n    cpu: \"500m\"\n\nmigrations:\n  enabled: true\n  postUpgrade: true\n  preUpgrade: true\nnamespace: kong\n\nadmin:\n  # Enable creating a Kubernetes service for the admin API\n  # Disabling this is recommended for most ingress controller configurations\n  # Enterprise users that wish to use Kong Manager with the controller should enable this\n  enabled: true\n  type: LoadBalancer\n  loadBalancerClass:\n  # To specify annotations or labels for the admin service, add them to the respective\n  # \"annotations\" or \"labels\" dictionaries below.\n  annotations: {}\n  #  service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \"*\"\n  labels: {}\n\n  http:\n    # Enable plaintext HTTP listen for the admin API\n    # Disabling this and using a TLS listen only is recommended for most configuration\n    enabled: true\n    servicePort: 8001\n    containerPort: 8001\n    # Set a nodePort which is available if service type is NodePort\n    # nodePort: 32080\n    # Additional listen parameters, e.g. \"reuseport\", \"backlog=16384\"\n    parameters: []\n\n  tls:\n    # Enable HTTPS listen for the admin API\n    enabled: true\n    servicePort: 8444\n    containerPort: 8444\n    # Set a target port for the TLS port in the admin API service, useful when using TLS\n    # termination on an ELB.\n    # overrideServiceTargetPort: 8000\n    # Set a nodePort which is available if service type is NodePort\n    # nodePort: 32443\n    # Additional listen parameters, e.g. \"reuseport\", \"backlog=16384\"\n    parameters:\n    - http2\n\n    # Specify the CA certificate to use for TLS verification of the Admin API client by:\n    # - secretName - the secret must contain a key named \"tls.crt\" with the PEM-encoded certificate.\n    # - caBundle (PEM-encoded certificate string).\n    # If both are set, caBundle takes precedence.\n    client:\n      caBundle: \"\"\n      secretName: \"\"\n    ingress:\n      # Enable/disable exposure using ingress.\n      enabled: false\n      ingressClassName:\n      # TLS secret name.\n      # tls: kong-admin.example.com-tls\n      # Ingress hostname\n      hostname:\n      # Map of ingress annotations.\n      annotations: {}\n      # Ingress path.\n      path: /\n      # Each path in an Ingress is required to have a corresponding path type. (ImplementationSpecific/Exact/Prefix)\n      pathType: ImplementationSpecific\n\nmanager:\n  # Enable creating a Kubernetes service for Kong Manager\n  enabled: true\n  type: LoadBalancer\n  loadBalancerClass:\n  # To specify annotations or labels for the Manager service, add them to the respective\n  # \"annotations\" or \"labels\" dictionaries below.\n  annotations: {}\n  #  service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \"*\"\n  labels: {}\n\n  http:\n    # Enable plaintext HTTP listen for Kong Manager\n    enabled: true\n    servicePort: 8002\n    containerPort: 8002\n    # Set a nodePort which is available if service type is NodePort\n    # nodePort: 32080\n    # Additional listen parameters, e.g. \"reuseport\", \"backlog=16384\"\n    parameters: []\n\n  tls:\n    # Enable HTTPS listen for Kong Manager\n    enabled: true\n    servicePort: 8445\n    containerPort: 8445\n    # Set a nodePort which is available if service type is NodePort\n    # nodePort: 32443\n    # Additional listen parameters, e.g. \"reuseport\", \"backlog=16384\"\n    parameters:\n    - http2\n\n  ingress:\n    # Enable/disable exposure using ingress.\n    enabled: false\n    ingressClassName:\n    # TLS secret name.\n    # tls: kong-manager.example.com-tls\n    # Ingress hostname\n    hostname:\n    # Map of ingress annotations.\n    annotations: {}\n    # Ingress path.\n    path: /\n    # Each path in an Ingress is required to have a corresponding path type. (ImplementationSpecific/Exact/Prefix)\n    pathType: ImplementationSpecific\n\nenterprise:\n  enabled: true\n  rbac:\n    enabled: false\n    admin_gui_auth: basic-auth\n    admin_gui_auth_conf_secret: kong-admin-auth\n    # If RBAC is enabled, this Secret must contain an admin_gui_session_conf key\n    # The key value must be a secret configuration, following the example at\n    # https://docs.konghq.com/enterprise/latest/kong-manager/authentication/sessions\n    # If using 3.6+ and OIDC, session configuration is instead handled in the auth configuration,\n    # and this field can be left empty.\n    session_conf_secret: \"kong-session-config\"\n\n# Specify Kong proxy service configuration\nproxy:\n  # Enable creating a Kubernetes service for the proxy\n  enabled: true\n  type: LoadBalancer\n  loadBalancerClass:\n  # Override proxy Service name\n  nameOverride: \"\"\n  # To specify annotations or labels for the proxy service, add them to the respective\n  # \"annotations\" or \"labels\" dictionaries below.\n  annotations: {}\n  # If terminating TLS at the ELB, the following annotations can be used\n  # \"service.beta.kubernetes.io/aws-load-balancer-backend-protocol\": \"*\",\n  # \"service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled\": \"true\",\n  # \"service.beta.kubernetes.io/aws-load-balancer-ssl-cert\": \"arn:aws:acm:REGION:ACCOUNT:certificate/XXXXXX-XXXXXXX-XXXXXXX-XXXXXXXX\",\n  # \"service.beta.kubernetes.io/aws-load-balancer-ssl-ports\": \"kong-proxy-tls\",\n  # \"service.beta.kubernetes.io/aws-load-balancer-type\": \"elb\"\n  labels:\n    enable-metrics: \"true\"\n\n  http:\n    # Enable plaintext HTTP listen for the proxy\n    enabled: true\n    # Set the servicePort: 0 to skip exposing in the service but still\n    # let the port open in container to allow https to http mapping for\n    # tls terminated at LB.\n    servicePort: 8000\n    containerPort: 8000\n    # Set a nodePort which is available if service type is NodePort\n    # nodePort: 32080\n    # Additional listen parameters, e.g. \"reuseport\", \"backlog=16384\"\n    parameters: []\n\n  tls:\n    # Enable HTTPS listen for the proxy\n    enabled: true\n    servicePort: 443\n    containerPort: 8443\n    # Set a target port for the TLS port in proxy service\n    # overrideServiceTargetPort: 8000\n    # Set a nodePort which is available if service type is NodePort\n    # nodePort: 32443\n    # Additional listen parameters, e.g. \"reuseport\", \"backlog=16384\"\n    parameters:\n    - http2\n\n    # Specify the Service's TLS port's appProtocol. This can be useful when integrating with\n    # external load balancers that require the `appProtocol` field to be set (e.g. GCP).\n    appProtocol: \"\"\n\n  # Define stream (TCP) listen\n  # To enable, remove \"[]\", uncomment the section below, and select your desired\n  # ports and parameters. Listens are dynamically named after their containerPort,\n  # e.g. \"stream-9000\" for the below.\n  # Note: although you can select the protocol here, you cannot set UDP if you\n  # use a LoadBalancer Service due to limitations in current Kubernetes versions.\n  # To proxy both TCP and UDP with LoadBalancers, you must enable the udpProxy Service\n  # in the next section and place all UDP stream listen configuration under it.\n  stream: []\n    #   # Set the container (internal) and service (external) ports for this listen.\n    #   # These values should normally be the same. If your environment requires they\n    #   # differ, note that Kong will match routes based on the containerPort only.\n    # - containerPort: 9000\n    #   servicePort: 9000\n    #   protocol: TCP\n    #   # Optionally set a static nodePort if the service type is NodePort\n    #   # nodePort: 32080\n    #   # Additional listen parameters, e.g. \"ssl\", \"reuseport\", \"backlog=16384\"\n    #   # \"ssl\" is required for SNI-based routes. It is not supported on versions \u003c2.0\n    #   parameters: []\n\n  # Kong proxy ingress settings.\n  # Note: You need this only if you are using another Ingress Controller\n  # to expose Kong outside the k8s cluster.\n  ingress:\n    # Enable/disable exposure using ingress.\n    enabled: false\n    ingressClassName:\n    # To specify annotations or labels for the ingress, add them to the respective\n    # \"annotations\" or \"labels\" dictionaries below.\n    annotations: {}\n    labels: {}\n    # Ingress hostname\n    hostname:\n    # Ingress path (when used with hostname above).\n    path: /\n    # Each path in an Ingress is required to have a corresponding path type (when used with hostname above). (ImplementationSpecific/Exact/Prefix)\n    pathType: ImplementationSpecific\n    # Ingress hosts. Use this instead of or in combination with hostname to specify multiple ingress host configurations\n    hosts: []\n\npostgresql:\n  enabled: true\n  primary:\n    persistence:\n      enabled: true\n      storageClass: \"gp2\"\n      size: \"10Gi\"\n  auth:\n    username: kong\n    database: kong\n  image:\n    tag: 13.11.0-debian-11-r20\n  service:\n    ports:\n      postgresql: \"5432\"\n\nextraObjects:\n- apiVersion: configuration.konghq.com/v1\n  kind: KongClusterPlugin\n  metadata:\n    name: prometheus\n    annotations:\n     kubernetes.io/ingress.class: kong\n    labels:\n      global: \"true\"\n  config:\n    status_code_metrics: true\n    bandwidth_metrics: true\n    upstream_health_metrics: true\n    latency_metrics: true\n    per_consumer: false\n  plugin: prometheus\n"
            ],
            "verify": false,
            "version": "2.46.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_openid_connect_provider.oidc_provider",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role.ingress_controller_iam_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "data.aws_partition.current",
            "data.tls_certificate.authentication",
            "kubernetes_namespace.kong_ns",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "metrics_server_release",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "metrics-server",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "dev-metrics-server",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "0.7.2",
                "chart": "metrics-server",
                "first_deployed": 1734704251,
                "last_deployed": 1734704251,
                "name": "dev-metrics-server",
                "namespace": "kube-system",
                "notes": "***********************************************************************\n* Metrics Server                                                      *\n***********************************************************************\n  Chart version: 3.12.2\n  App version:   0.7.2\n  Image tag:     registry.k8s.io/metrics-server/metrics-server:v0.7.2\n***********************************************************************\n",
                "revision": 1,
                "values": "{}",
                "version": "3.12.2"
              }
            ],
            "name": "dev-metrics-server",
            "namespace": "kube-system",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://kubernetes-sigs.github.io/metrics-server/",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": null,
            "verify": false,
            "version": "3.12.2",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus1",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "kube-prometheus-stack",
            "cleanup_on_fail": true,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "monitoring",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v0.79.2",
                "chart": "kube-prometheus-stack",
                "first_deployed": 1734704263,
                "last_deployed": 1734704263,
                "name": "monitoring",
                "namespace": "monitoring",
                "notes": "1. Get your 'admin' user password by running:\n\n   kubectl get secret --namespace monitoring monitoring-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n\n2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:\n\n   monitoring-grafana.monitoring.svc.cluster.local\n\n   Get the Grafana URL to visit by running these commands in the same shell:\n     export POD_NAME=$(kubectl get pods --namespace monitoring -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=monitoring\" -o jsonpath=\"{.items[0].metadata.name}\")\n     kubectl --namespace monitoring port-forward $POD_NAME 3000\n\n3. Login with the password from step 1 and the username: admin\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Grafana pod is terminated.                            #####\n#################################################################################\n\nkube-state-metrics is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects.\nThe exposed metrics can be found here:\nhttps://github.com/kubernetes/kube-state-metrics/blob/master/docs/README.md#exposed-metrics\n\nThe metrics are exported on the HTTP endpoint /metrics on the listening port.\nIn your case, monitoring-kube-state-metrics.monitoring.svc.cluster.local:8080/metrics\n\nThey are served either as plaintext or protobuf depending on the Accept header.\nThey are designed to be consumed either by Prometheus itself or by a scraper that is compatible with scraping a Prometheus client endpoint.\n\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace monitoring -l \"app.kubernetes.io/name=prometheus-node-exporter,app.kubernetes.io/instance=monitoring\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:9100 to use your application\"\n  kubectl port-forward --namespace monitoring $POD_NAME 9100\nkube-prometheus-stack has been installed. Check its status by running:\n  kubectl --namespace monitoring get pods -l \"release=monitoring\"\n\nVisit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026 configure Alertmanager and Prometheus instances using the Operator.\n",
                "revision": 1,
                "values": "{\"additionalPrometheusRulesMap\":{},\"alertmanager\":{\"alertmanagerSpec\":{\"additionalConfig\":{},\"additionalConfigString\":\"\",\"additionalPeers\":[],\"affinity\":{},\"alertmanagerConfigMatcherStrategy\":{},\"alertmanagerConfigNamespaceSelector\":{},\"alertmanagerConfigSelector\":{},\"alertmanagerConfiguration\":{},\"automountServiceAccountToken\":true,\"clusterAdvertiseAddress\":false,\"clusterGossipInterval\":\"\",\"clusterLabel\":\"\",\"clusterPeerTimeout\":\"\",\"clusterPushpullInterval\":\"\",\"configMaps\":[],\"containers\":[],\"externalUrl\":null,\"forceEnableClusterMode\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/alertmanager\",\"sha\":\"\",\"tag\":\"v0.27.0\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"minReadySeconds\":0,\"nodeSelector\":{},\"paused\":false,\"podAntiAffinity\":\"soft\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"http-web\",\"priorityClassName\":\"\",\"replicas\":2,\"resources\":{},\"retention\":\"120h\",\"routePrefix\":\"/\",\"scheme\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tlsConfig\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"useExistingSecret\":false,\"volumeMounts\":[],\"volumes\":[],\"web\":{}},\"annotations\":{},\"apiVersion\":\"v2\",\"config\":{\"global\":{\"resolve_timeout\":\"5m\"},\"receivers\":[{\"email_configs\":[{\"auth_identity\":\"ltphilong2001@gmail.com\",\"auth_password\":\"z45letranphilong@gmail.com\",\"auth_username\":\"ltphilong2001@gmail.com\",\"from\":\"ltphilong2001@gmail.com\",\"smarthost\":\"smtp.gmail.com:587\",\"to\":\"ltphilong2001@gmail.com\"}],\"name\":\"email-alert\"}],\"route\":{\"group_by\":[\"severity\"],\"receiver\":\"email-alert\",\"routes\":[{\"group_interval\":\"5m\",\"group_wait\":\"30s\",\"receiver\":\"email-alert\",\"repeat_interval\":\"12h\"}]}},\"enableFeatures\":[],\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"alertmanager\"}},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"route\":{\"main\":{\"additionalRules\":[],\"annotations\":{},\"apiVersion\":\"gateway.networking.k8s.io/v1\",\"enabled\":false,\"filters\":[],\"hostnames\":[],\"kind\":\"HTTPRoute\",\"labels\":{},\"matches\":[{\"path\":{\"type\":\"PathPrefix\",\"value\":\"/\"}}],\"parentRefs\":[]}},\"secret\":{\"annotations\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"ipFamilies\":[\"IPv4\"],\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30903,\"port\":9093,\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{\"clientIP\":{\"timeoutSeconds\":10800}},\"targetPort\":9093,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"enableHttp2\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerSourceRanges\":[],\"nodePort\":30904,\"port\":9093,\"targetPort\":9093,\"type\":\"ClusterIP\"},\"stringConfig\":\"\",\"templateFiles\":{},\"tplConfig\":false},\"cleanPrometheusOperatorObjectNames\":false,\"commonLabels\":{},\"coreDns\":{\"enabled\":true,\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":9153,\"targetPort\":9153},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLabels\":[],\"targetLimit\":0}},\"crds\":{\"enabled\":true},\"customRules\":{},\"defaultRules\":{\"additionalAggregationLabels\":[],\"additionalRuleAnnotations\":{},\"additionalRuleGroupAnnotations\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8sContainerCpuUsageSecondsTotal\":{},\"k8sContainerMemoryCache\":{},\"k8sContainerMemoryRss\":{},\"k8sContainerMemorySwap\":{},\"k8sContainerResource\":{},\"k8sPodOwner\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleGroupLabels\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8sContainerCpuUsageSecondsTotal\":{},\"k8sContainerMemoryCache\":{},\"k8sContainerMemoryRss\":{},\"k8sContainerMemorySwap\":{},\"k8sContainerResource\":{},\"k8sPodOwner\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleLabels\":{},\"annotations\":{},\"appNamespacesTarget\":\".*\",\"create\":true,\"disabled\":{},\"keepFiringFor\":\"\",\"labels\":{},\"node\":{\"fsSelector\":\"fstype!=\\\"\\\"\"},\"rules\":{\"alertmanager\":true,\"configReloaders\":true,\"etcd\":true,\"general\":true,\"k8sContainerCpuUsageSecondsTotal\":true,\"k8sContainerMemoryCache\":true,\"k8sContainerMemoryRss\":true,\"k8sContainerMemorySwap\":true,\"k8sContainerMemoryWorkingSetBytes\":true,\"k8sContainerResource\":true,\"k8sPodOwner\":true,\"kubeApiserverAvailability\":true,\"kubeApiserverBurnrate\":true,\"kubeApiserverHistogram\":true,\"kubeApiserverSlos\":true,\"kubeControllerManager\":true,\"kubePrometheusGeneral\":true,\"kubePrometheusNodeRecording\":true,\"kubeProxy\":true,\"kubeSchedulerAlerting\":true,\"kubeSchedulerRecording\":true,\"kubeStateMetrics\":true,\"kubelet\":true,\"kubernetesApps\":true,\"kubernetesResources\":true,\"kubernetesStorage\":true,\"kubernetesSystem\":true,\"network\":true,\"node\":true,\"nodeExporterAlerting\":true,\"nodeExporterRecording\":true,\"prometheus\":true,\"prometheusOperator\":true,\"windows\":true},\"runbookUrl\":\"https://runbooks.prometheus-operator.dev/runbooks\"},\"extraManifests\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"rbac\":{\"create\":true,\"createAggregateClusterRoles\":false,\"pspAnnotations\":{},\"pspEnabled\":false}},\"grafana\":{\"additionalDataSources\":[],\"adminPassword\":\"prom-operator\",\"defaultDashboardsEditable\":true,\"defaultDashboardsEnabled\":true,\"defaultDashboardsTimezone\":\"utc\",\"deleteDatasources\":[],\"enabled\":true,\"extraConfigmapMounts\":[],\"forceDeployDashboards\":false,\"forceDeployDatasources\":false,\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"path\":\"/\",\"tls\":[]},\"namespaceOverride\":\"\",\"prune\":false,\"rbac\":{\"pspEnabled\":false},\"service\":{\"ipFamilies\":[],\"ipFamilyPolicy\":\"\",\"portName\":\"http-web\"},\"serviceAccount\":{\"autoMount\":true,\"create\":true},\"serviceMonitor\":{\"enabled\":true,\"interval\":\"\",\"labels\":{},\"path\":\"/metrics\",\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":\"30s\",\"tlsConfig\":{}},\"sidecar\":{\"dashboards\":{\"annotations\":{},\"enableNewTablePanelSyntax\":false,\"enabled\":true,\"label\":\"grafana_dashboard\",\"labelValue\":\"1\",\"multicluster\":{\"etcd\":{\"enabled\":false},\"global\":{\"enabled\":false}},\"provider\":{\"allowUiUpdates\":false},\"searchNamespace\":\"ALL\"},\"datasources\":{\"alertmanager\":{\"enabled\":true,\"handleGrafanaManagedAlerts\":false,\"implementation\":\"prometheus\",\"name\":\"Alertmanager\",\"uid\":\"alertmanager\"},\"annotations\":{},\"createPrometheusReplicasDatasources\":false,\"defaultDatasourceEnabled\":true,\"enabled\":true,\"exemplarTraceIdDestinations\":{},\"httpMethod\":\"POST\",\"isDefaultDatasource\":true,\"label\":\"grafana_datasource\",\"labelValue\":\"1\",\"name\":\"Prometheus\",\"uid\":\"prometheus\"}}},\"kube-state-metrics\":{\"namespaceOverride\":\"\",\"prometheus\":{\"monitor\":{\"enabled\":true,\"honorLabels\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"create\":true},\"releaseLabel\":true,\"selfMonitor\":{\"enabled\":false}},\"kubeApiServer\":{\"enabled\":true,\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"jobLabel\":\"component\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[{\"action\":\"drop\",\"regex\":\"apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\",\"sourceLabels\":[\"__name__\",\"le\"]}],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{\"matchLabels\":{\"component\":\"apiserver\",\"provider\":\"kubernetes\"}},\"targetLabels\":[],\"targetLimit\":0},\"tlsConfig\":{\"insecureSkipVerify\":false,\"serverName\":\"kubernetes\"}},\"kubeControllerManager\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"serverName\":null,\"targetLabels\":[],\"targetLimit\":0}},\"kubeDns\":{\"enabled\":false,\"service\":{\"dnsmasq\":{\"port\":10054,\"targetPort\":10054},\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"skydns\":{\"port\":10055,\"targetPort\":10055}},\"serviceMonitor\":{\"additionalLabels\":{},\"dnsmasqMetricRelabelings\":[],\"dnsmasqRelabelings\":[],\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLabels\":[],\"targetLimit\":0}},\"kubeEtcd\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":2381,\"targetPort\":2381},\"serviceMonitor\":{\"additionalLabels\":{},\"caFile\":\"\",\"certFile\":\"\",\"enabled\":true,\"insecureSkipVerify\":false,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"keyFile\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"http\",\"selector\":{},\"serverName\":\"\",\"targetLabels\":[],\"targetLimit\":0}},\"kubeProxy\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":10249,\"targetPort\":10249},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":false,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLabels\":[],\"targetLimit\":0}},\"kubeScheduler\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"serverName\":null,\"targetLabels\":[],\"targetLimit\":0}},\"kubeStateMetrics\":{\"enabled\":true},\"kubeTargetVersionOverride\":\"\",\"kubeVersionOverride\":\"\",\"kubelet\":{\"enabled\":true,\"namespace\":\"kube-system\",\"serviceMonitor\":{\"additionalLabels\":{},\"attachMetadata\":{\"node\":false},\"cAdvisor\":true,\"cAdvisorMetricRelabelings\":[{\"action\":\"drop\",\"regex\":\"container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_memory_(mapped_file|swap)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_(file_descriptors|tasks_state|threads_max)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_spec.*\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\".+;\",\"sourceLabels\":[\"id\",\"pod\"]}],\"cAdvisorRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"honorLabels\":true,\"honorTimestamps\":true,\"https\":true,\"insecureSkipVerify\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"probes\":true,\"probesMetricRelabelings\":[],\"probesRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"proxyUrl\":\"\",\"relabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"resource\":false,\"resourcePath\":\"/metrics/resource/v1alpha1\",\"resourceRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"sampleLimit\":0,\"targetLabels\":[],\"targetLimit\":0}},\"kubernetesServiceMonitors\":{\"enabled\":true},\"nameOverride\":\"\",\"namespaceOverride\":\"\",\"nodeExporter\":{\"enabled\":true,\"forceDeployDashboards\":false,\"operatingSystems\":{\"aix\":{\"enabled\":true},\"darwin\":{\"enabled\":true},\"linux\":{\"enabled\":true}}},\"prometheus\":{\"additionalPodMonitors\":[],\"additionalRulesForClusterRole\":[],\"additionalServiceMonitors\":[],\"agentMode\":false,\"annotations\":{},\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"prometheus\"}},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"podSecurityPolicy\":{\"allowedCapabilities\":[],\"allowedHostPaths\":[],\"volumes\":[]},\"prometheusSpec\":{\"additionalAlertManagerConfigs\":[],\"additionalAlertManagerConfigsSecret\":{},\"additionalAlertRelabelConfigs\":[],\"additionalAlertRelabelConfigsSecret\":{},\"additionalArgs\":[],\"additionalConfig\":{},\"additionalConfigString\":\"\",\"additionalPrometheusSecretsAnnotations\":{},\"additionalRemoteRead\":[],\"additionalRemoteWrite\":[],\"additionalScrapeConfigs\":[{\"job_name\":\"kong-monitoring\",\"metrics_path\":\"/metrics\",\"static_configs\":[{\"targets\":[\"kong-kong-admin.kong.svc.cluster.local:8001\"]}]},{\"job_name\":\"argocd-rollout-monitoring\",\"metrics_path\":\"/metrics\",\"static_configs\":[{\"targets\":[\"my-release-argo-rollouts-metrics.argocd-rollout.svc.cluster.local.8090\"]}]}],\"additionalScrapeConfigsSecret\":{},\"affinity\":{},\"alertingEndpoints\":[],\"allowOverlappingBlocks\":false,\"apiserverConfig\":{},\"arbitraryFSAccessThroughSMs\":false,\"automountServiceAccountToken\":true,\"configMaps\":[],\"containers\":[],\"disableCompaction\":false,\"enableAdminAPI\":false,\"enableFeatures\":[],\"enableRemoteWriteReceiver\":false,\"enforcedKeepDroppedTargets\":0,\"enforcedLabelLimit\":false,\"enforcedLabelNameLengthLimit\":false,\"enforcedLabelValueLengthLimit\":false,\"enforcedNamespaceLabel\":\"\",\"enforcedSampleLimit\":false,\"enforcedTargetLimit\":false,\"evaluationInterval\":\"\",\"excludedFromEnforcement\":[],\"exemplars\":\"\",\"externalLabels\":{},\"externalUrl\":\"\",\"hostAliases\":[],\"hostNetwork\":false,\"ignoreNamespaceSelectors\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/prometheus\",\"sha\":\"\",\"tag\":\"v2.55.1\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"maximumStartupDurationSeconds\":0,\"minReadySeconds\":0,\"nodeSelector\":{},\"overrideHonorLabels\":false,\"overrideHonorTimestamps\":false,\"paused\":false,\"persistentVolumeClaimRetentionPolicy\":{},\"podAntiAffinity\":\"soft\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"podMonitorNamespaceSelector\":{},\"podMonitorSelector\":{},\"podMonitorSelectorNilUsesHelmValues\":true,\"portName\":\"http-web\",\"priorityClassName\":\"\",\"probeNamespaceSelector\":{},\"probeSelector\":{},\"probeSelectorNilUsesHelmValues\":true,\"prometheusExternalLabelName\":\"\",\"prometheusExternalLabelNameClear\":false,\"prometheusRulesExcludedFromEnforce\":[],\"query\":{},\"queryLogFile\":false,\"remoteRead\":[],\"remoteWrite\":[],\"remoteWriteDashboards\":false,\"replicaExternalLabelName\":\"\",\"replicaExternalLabelNameClear\":false,\"replicas\":1,\"resources\":{},\"retention\":\"10d\",\"retentionSize\":\"\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"sampleLimit\":false,\"scrapeClasses\":[],\"scrapeConfigNamespaceSelector\":{},\"scrapeConfigSelector\":{},\"scrapeConfigSelectorNilUsesHelmValues\":true,\"scrapeInterval\":\"\",\"scrapeTimeout\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"serviceDiscoveryRole\":\"\",\"serviceMonitorNamespaceSelector\":{},\"serviceMonitorSelector\":{},\"serviceMonitorSelectorNilUsesHelmValues\":true,\"shards\":1,\"storageSpec\":{\"volumeClaimTemplate\":{\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"10Gi\"}},\"storageClassName\":\"gp2\"}}},\"thanos\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"tracingConfig\":{},\"tsdb\":{\"outOfOrderTimeWindow\":\"0s\"},\"version\":\"\",\"volumeMounts\":[],\"volumes\":[],\"walCompression\":true,\"web\":{}},\"route\":{\"main\":{\"additionalRules\":[],\"annotations\":{},\"apiVersion\":\"gateway.networking.k8s.io/v1\",\"enabled\":false,\"filters\":[],\"hostnames\":[],\"kind\":\"HTTPRoute\",\"labels\":{},\"matches\":[{\"path\":{\"type\":\"PathPrefix\",\"value\":\"/\"}}],\"parentRefs\":[]}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30090,\"port\":9090,\"publishNotReadyAddresses\":false,\"reloaderWebPort\":8080,\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{\"clientIP\":{\"timeoutSeconds\":10800}},\"targetPort\":9090,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"loadBalancerSourceRanges\":[],\"nodePort\":30091,\"port\":9090,\"targetPort\":9090,\"type\":\"ClusterIP\"},\"thanosIngress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"nodePort\":30901,\"paths\":[],\"servicePort\":10901,\"tls\":[]},\"thanosService\":{\"annotations\":{},\"clusterIP\":\"\",\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"ClusterIP\"},\"thanosServiceExternal\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"ClusterIP\"},\"thanosServiceMonitor\":{\"additionalLabels\":{},\"bearerTokenFile\":null,\"enabled\":false,\"interval\":\"\",\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"tlsConfig\":{}}},\"prometheus-node-exporter\":{\"extraArgs\":[\"--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\",\"--collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"],\"namespaceOverride\":\"\",\"podLabels\":{\"jobLabel\":\"node-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"pspEnabled\":false},\"releaseLabel\":true,\"service\":{\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{\"jobLabel\":\"node-exporter\"},\"portName\":\"http-metrics\"}},\"prometheus-windows-exporter\":{\"config\":\"collectors:\\n  enabled: '[defaults],memory,container'\",\"podLabels\":{\"jobLabel\":\"windows-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"jobLabel\":\"jobLabel\"}},\"releaseLabel\":true},\"prometheusOperator\":{\"admissionWebhooks\":{\"annotations\":{},\"caBundle\":\"\",\"certManager\":{\"admissionCert\":{\"duration\":\"\"},\"enabled\":false,\"rootCert\":{\"duration\":\"\"}},\"createSecretJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"deployment\":{\"affinity\":{},\"annotations\":{},\"automountServiceAccountToken\":true,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"dnsConfig\":{},\"enabled\":false,\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/admission-webhook\",\"sha\":\"\",\"tag\":\"\"},\"labels\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{},\"podLabels\":{},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":10,\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":31080,\"nodePortTls\":31443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":false,\"create\":true,\"name\":\"\"},\"strategy\":{},\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[]},\"enabled\":true,\"failurePolicy\":\"\",\"mutatingWebhookConfiguration\":{\"annotations\":{}},\"namespaceSelector\":{},\"objectSelector\":{},\"patch\":{\"affinity\":{},\"annotations\":{},\"enabled\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"registry.k8s.io\",\"repository\":\"ingress-nginx/kube-webhook-certgen\",\"sha\":\"\",\"tag\":\"v20221220-controller-v1.5.1-58-g787ea74b6\"},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":\"\",\"resources\":{},\"securityContext\":{\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":2000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true},\"tolerations\":[],\"ttlSecondsAfterFinished\":60},\"patchWebhookJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"timeoutSeconds\":10,\"validatingWebhookConfiguration\":{\"annotations\":{}}},\"affinity\":{},\"alertmanagerConfigNamespaces\":[],\"alertmanagerInstanceNamespaces\":[],\"alertmanagerInstanceSelector\":\"\",\"annotations\":{},\"automountServiceAccountToken\":true,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"denyNamespaces\":[],\"dnsConfig\":{},\"enabled\":true,\"env\":{\"GOGC\":\"30\"},\"extraArgs\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-operator\",\"sha\":\"\",\"tag\":\"\"},\"kubeletEndpointSliceEnabled\":false,\"kubeletEndpointsEnabled\":true,\"kubeletService\":{\"enabled\":true,\"name\":\"\",\"namespace\":\"kube-system\",\"selector\":\"\"},\"labels\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"namespaces\":{},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"prometheusConfigReloader\":{\"enableProbe\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-config-reloader\",\"sha\":\"\",\"tag\":\"\"},\"resources\":{}},\"prometheusInstanceNamespaces\":[],\"prometheusInstanceSelector\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"revisionHistoryLimit\":10,\"secretFieldSelector\":\"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\",\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30080,\"nodePortTls\":30443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"selfMonitor\":true,\"targetLimit\":0},\"strategy\":{},\"thanosImage\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.36.1\"},\"thanosRulerInstanceNamespaces\":[],\"thanosRulerInstanceSelector\":\"\",\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[],\"verticalPodAutoscaler\":{\"controlledResources\":[],\"enabled\":false,\"maxAllowed\":{},\"minAllowed\":{},\"updatePolicy\":{\"updateMode\":\"Auto\"}}},\"thanosRuler\":{\"annotations\":{},\"enabled\":false,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"route\":{\"main\":{\"additionalRules\":[],\"annotations\":{},\"apiVersion\":\"gateway.networking.k8s.io/v1\",\"enabled\":false,\"filters\":[],\"hostnames\":[],\"kind\":\"HTTPRoute\",\"labels\":{},\"matches\":[{\"path\":{\"type\":\"PathPrefix\",\"value\":\"/\"}}],\"parentRefs\":[]}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30905,\"port\":10902,\"targetPort\":10902,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"thanosRulerSpec\":{\"additionalArgs\":[],\"additionalConfig\":{},\"additionalConfigString\":\"\",\"affinity\":{},\"alertDropLabels\":[],\"alertmanagersConfig\":{\"existingSecret\":{},\"secret\":{}},\"containers\":[],\"evaluationInterval\":\"\",\"externalPrefix\":null,\"externalPrefixNilUsesHelmValues\":true,\"image\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.36.1\"},\"initContainers\":[],\"labels\":{},\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"nodeSelector\":{},\"objectStorageConfig\":{\"existingSecret\":{},\"secret\":{}},\"paused\":false,\"podAntiAffinity\":\"soft\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"web\",\"priorityClassName\":\"\",\"queryConfig\":{\"existingSecret\":{},\"secret\":{}},\"queryEndpoints\":[],\"replicas\":1,\"resources\":{},\"retention\":\"24h\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[],\"web\":{}}},\"windowsMonitoring\":{\"enabled\":false}}",
                "version": "67.4.0"
              }
            ],
            "name": "monitoring",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 2000,
            "upgrade_install": null,
            "values": [
              "# Default values for kube-                                            etheus-stack.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n## Provide a name in place of kube-prometheus-stack for `app:` labels\n##\nnameOverride: \"\"\n\n## Override the deployment namespace\n##\nnamespaceOverride: \"\"\n\n## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6\n##\nkubeTargetVersionOverride: \"\"\n\n## Allow kubeVersion to be overridden while creating the ingress\n##\nkubeVersionOverride: \"\"\n\n## Provide a name to substitute for the full names of resources\n##\nfullnameOverride: \"\"\n\n## Labels to apply to all resources\n##\ncommonLabels: {}\n# scmhash: abc123\n# myLabel: aakkmd\n\n## Install Prometheus Operator CRDs\n##\ncrds:\n  enabled: true\n\n## custom Rules to override \"for\" and \"severity\" in defaultRules\n##\ncustomRules: {}\n  # AlertmanagerFailedReload:\n  #   for: 3m\n  # AlertmanagerMembersInconsistent:\n  #   for: 5m\n  #   severity: \"warning\"\n\n## Create default rules for monitoring the cluster\n##\ndefaultRules:\n  create: true\n  rules:\n    alertmanager: true\n    etcd: true\n    configReloaders: true\n    general: true\n    k8sContainerCpuUsageSecondsTotal: true\n    k8sContainerMemoryCache: true\n    k8sContainerMemoryRss: true\n    k8sContainerMemorySwap: true\n    k8sContainerResource: true\n    k8sContainerMemoryWorkingSetBytes: true\n    k8sPodOwner: true\n    kubeApiserverAvailability: true\n    kubeApiserverBurnrate: true\n    kubeApiserverHistogram: true\n    kubeApiserverSlos: true\n    kubeControllerManager: true\n    kubelet: true\n    kubeProxy: true\n    kubePrometheusGeneral: true\n    kubePrometheusNodeRecording: true\n    kubernetesApps: true\n    kubernetesResources: true\n    kubernetesStorage: true\n    kubernetesSystem: true\n    kubeSchedulerAlerting: true\n    kubeSchedulerRecording: true\n    kubeStateMetrics: true\n    network: true\n    node: true\n    nodeExporterAlerting: true\n    nodeExporterRecording: true\n    prometheus: true\n    prometheusOperator: true\n    windows: true\n\n  ## Reduce app namespace alert scope\n  appNamespacesTarget: \".*\"\n\n  ## Set keep_firing_for for all alerts\n  keepFiringFor: \"\"\n\n  ## Labels for default rules\n  labels: {}\n  ## Annotations for default rules\n  annotations: {}\n\n  ## Additional labels for PrometheusRule alerts\n  additionalRuleLabels: {}\n\n  ## Additional annotations for PrometheusRule alerts\n  additionalRuleAnnotations: {}\n\n  ## Additional labels for specific PrometheusRule alert groups\n  additionalRuleGroupLabels:\n    alertmanager: {}\n    etcd: {}\n    configReloaders: {}\n    general: {}\n    k8sContainerCpuUsageSecondsTotal: {}\n    k8sContainerMemoryCache: {}\n    k8sContainerMemoryRss: {}\n    k8sContainerMemorySwap: {}\n    k8sContainerResource: {}\n    k8sPodOwner: {}\n    kubeApiserverAvailability: {}\n    kubeApiserverBurnrate: {}\n    kubeApiserverHistogram: {}\n    kubeApiserverSlos: {}\n    kubeControllerManager: {}\n    kubelet: {}\n    kubeProxy: {}\n    kubePrometheusGeneral: {}\n    kubePrometheusNodeRecording: {}\n    kubernetesApps: {}\n    kubernetesResources: {}\n    kubernetesStorage: {}\n    kubernetesSystem: {}\n    kubeSchedulerAlerting: {}\n    kubeSchedulerRecording: {}\n    kubeStateMetrics: {}\n    network: {}\n    node: {}\n    nodeExporterAlerting: {}\n    nodeExporterRecording: {}\n    prometheus: {}\n    prometheusOperator: {}\n\n  ## Additional annotations for specific PrometheusRule alerts groups\n  additionalRuleGroupAnnotations:\n    alertmanager: {}\n    etcd: {}\n    configReloaders: {}\n    general: {}\n    k8sContainerCpuUsageSecondsTotal: {}\n    k8sContainerMemoryCache: {}\n    k8sContainerMemoryRss: {}\n    k8sContainerMemorySwap: {}\n    k8sContainerResource: {}\n    k8sPodOwner: {}\n    kubeApiserverAvailability: {}\n    kubeApiserverBurnrate: {}\n    kubeApiserverHistogram: {}\n    kubeApiserverSlos: {}\n    kubeControllerManager: {}\n    kubelet: {}\n    kubeProxy: {}\n    kubePrometheusGeneral: {}\n    kubePrometheusNodeRecording: {}\n    kubernetesApps: {}\n    kubernetesResources: {}\n    kubernetesStorage: {}\n    kubernetesSystem: {}\n    kubeSchedulerAlerting: {}\n    kubeSchedulerRecording: {}\n    kubeStateMetrics: {}\n    network: {}\n    node: {}\n    nodeExporterAlerting: {}\n    nodeExporterRecording: {}\n    prometheus: {}\n    prometheusOperator: {}\n\n  additionalAggregationLabels: []\n\n  ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.\n  runbookUrl: \"https://runbooks.prometheus-operator.dev/runbooks\"\n\n  node:\n    fsSelector: 'fstype!=\"\"'\n    # fsSelector: 'fstype=~\"ext[234]|btrfs|xfs|zfs\"'\n\n  ## Disabled PrometheusRule alerts\n  disabled: {}\n  # KubeAPIDown: true\n  # NodeRAIDDegraded: true\n\n## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.\n##\n# additionalPrometheusRules: []\n#  - name: my-rule-file\n#    groups:\n#      - name: my_group\n#        rules:\n#        - record: my_record\n#          expr: 100 * my_record\n\n## Provide custom recording or alerting rules to be deployed into the cluster.\n##\nadditionalPrometheusRulesMap: {}\n#  rule-name:\n#    groups:\n#    - name: my_group\n#      rules:\n#      - record: my_record\n#        expr: 100 * my_record\n\n##\nglobal:\n  rbac:\n    create: true\n\n    ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs\n    ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles\n    createAggregateClusterRoles: false\n    pspEnabled: false\n    pspAnnotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)\n  ##\n  imageRegistry: \"\"\n\n  ## Reference to one or more secrets to be used when pulling images\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  imagePullSecrets: []\n  # - name: \"image-pull-secret\"\n  # or\n  # - \"image-pull-secret\"\n\nwindowsMonitoring:\n  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')\n  enabled: false\n\n## Configuration for prometheus-windows-exporter\n## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter\n##\nprometheus-windows-exporter:\n  ## Enable ServiceMonitor and set Kubernetes label to use as a job label\n  ##\n  prometheus:\n    monitor:\n      enabled: true\n      jobLabel: jobLabel\n\n  releaseLabel: true\n\n  ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards\n  ##\n  podLabels:\n    jobLabel: windows-exporter\n\n  ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards\n  ##\n  config: |-\n    collectors:\n      enabled: '[defaults],memory,container'\n\n## Configuration for alertmanager\n## ref: https://prometheus.io/docs/alerting/alertmanager/\n##\nalertmanager:\n\n  ## Deploy alertmanager\n  ##\n  enabled: true\n\n  ## Annotations for Alertmanager\n  ##\n  annotations: {}\n\n  ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2\n  ##\n  apiVersion: v2\n\n  ## @param alertmanager.enableFeatures Enable access to Alertmanager disabled features.\n  ##\n  enableFeatures: []\n\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n    automountServiceAccountToken: true\n\n  ## Configure pod disruption budgets for Alertmanager\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ## Alertmanager configuration directives\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  config:\n    global:\n      resolve_timeout: 5m\n    route:\n      receiver: 'email-alert'\n      group_by: ['severity']\n\n      routes:\n      - receiver: 'email-alert'\n        group_wait: 30s\n        group_interval: 5m\n        repeat_interval: 12h  \n\n    receivers:\n    - name: email-alert\n      email_configs:\n      - to: ltphilong2001@gmail.com\n        from: ltphilong2001@gmail.com\n        smarthost: smtp.gmail.com:587\n        auth_username: ltphilong2001@gmail.com\n        auth_identity: ltphilong2001@gmail.com\n        auth_password: z45letranphilong@gmail.com\n\n  ## Alertmanager configuration directives (as string type, preferred over the config hash map)\n  ## stringConfig will be used only, if tplConfig is true\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  stringConfig: \"\"\n\n  ## Pass the Alertmanager configuration directives through Helm's templating\n  ## engine. If the Alertmanager configuration contains Alertmanager templates,\n  ## they'll need to be properly escaped so that they are not interpreted by\n  ## Helm\n  ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function\n  ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string\n  ##      https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  tplConfig: false\n\n  ## Alertmanager template files to format alerts\n  ## By default, templateFiles are placed in /etc/alertmanager/config/ and if\n  ## they have a .tmpl file suffix will be loaded. See config.templates above\n  ## to change, add other suffixes. If adding other suffixes, be sure to update\n  ## config.templates above to include those suffixes.\n  ## ref: https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  ##\n  templateFiles: {}\n  #\n  ## An example template:\n  #   template_1.tmpl: |-\n  #       {{ define \"cluster\" }}{{ .ExternalURL | reReplaceAll \".*alertmanager\\\\.(.*)\" \"$1\" }}{{ end }}\n  #\n  #       {{ define \"slack.myorg.text\" }}\n  #       {{- $root := . -}}\n  #       {{ range .Alerts }}\n  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`\n  #         *Cluster:* {{ template \"cluster\" $root }}\n  #         *Description:* {{ .Annotations.description }}\n  #         *Graph:* \u003c{{ .GeneratorURL }}|:chart_with_upwards_trend:\u003e\n  #         *Runbook:* \u003c{{ .Annotations.runbook }}|:spiral_note_pad:\u003e\n  #         *Details:*\n  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`\n  #           {{ end }}\n  #       {{ end }}\n  #       {{ end }}\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n\n    labels: {}\n\n    ## Override ingress to a different defined port on the service\n    # servicePort: 8081\n    ## Override ingress to a different service then the default, this is useful if you need to\n    ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)\n    # serviceName: kube-prometheus-stack-alertmanager-0\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - alertmanager.domain.com\n\n    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Alertmanager Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: alertmanager-general-tls\n    #   hosts:\n    #   - alertmanager.example.com\n\n  # -- BETA: Configure the gateway routes for the chart here.\n  # More routes can be added by adding a dictionary key like the 'main' route.\n  # Be aware that this is an early beta of this feature,\n  # kube-prometheus-stack does not guarantee this works and is subject to change.\n  # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk\n  # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)\n  route:\n    main:\n      # -- Enables or disables the route\n      enabled: false\n\n      # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2\n      apiVersion: gateway.networking.k8s.io/v1\n      # -- Set the route kind\n      # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute\n      kind: HTTPRoute\n\n      annotations: {}\n      labels: {}\n\n      hostnames: []\n      # - my-filter.example.com\n      parentRefs: []\n      # - name: acme-gw\n\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n\n      ## Filters define the filters that are applied to requests that match this rule.\n      filters: []\n\n      ## Additional custom rules that can be added to the route\n      additionalRules: []\n\n  ## Configuration for Alertmanager secret\n  ##\n  secret:\n    annotations: {}\n\n  ## Configuration for creating an Ingress that will map to each Alertmanager replica service\n  ## alertmanager.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for alertmanager per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"alertmanager\"\n\n  ## Configuration for Alertmanager service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    ipFamilies: [\"IPv4\"]\n    ## Port for Alertmanager Service to listen on\n    ##\n    port: 9093\n    ## To be used with a proxy extraContainer port\n    ##\n    targetPort: 9093\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30903\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n\n    ## Additional ports to open for Alertmanager service\n    ##\n    additionalPorts: []\n    # - name: oauth-proxy\n    #   port: 8081\n    #   targetPort: 8081\n    # - name: oauth-metrics\n    #   port: 8082\n    #   targetPort: 8082\n\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## If you want to make sure that connections from a particular client are passed to the same Pod each time\n    ## Accepts 'ClientIP' or 'None'\n    ##\n    sessionAffinity: None\n\n    ## If you want to modify the ClientIP sessionAffinity timeout\n    ## The value must be \u003e0 \u0026\u0026 \u003c=86400(for 1 day) if ServiceAffinity == \"ClientIP\"\n    ##\n    sessionAffinityConfig:\n      clientIP:\n        timeoutSeconds: 10800\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a separate Service for each statefulset Alertmanager replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Alertmanager Service per replica to listen on\n    ##\n    port: 9093\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9093\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30904\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a ServiceMonitor for AlertManager\n  ##\n  serviceMonitor:\n    ## If true, a ServiceMonitor will be created for the AlertManager service.\n    ##\n    selfMonitor: true\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## enableHttp2: Whether to enable HTTP2.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint\n    enableHttp2: true\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional Endpoints\n    ##\n    additionalEndpoints: []\n    # - port: oauth-metrics\n    #   path: /metrics\n\n  ## Settings affecting alertmanagerSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec\n  ##\n  alertmanagerSpec:\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.\n    ##\n    podMetadata: {}\n\n    ## Image of Alertmanager\n    ##\n    image:\n      registry: quay.io\n      repository: prometheus/alertmanager\n      tag: v0.27.0\n      sha: \"\"\n\n    ## If true then the user will be responsible to provide a secret with alertmanager configuration\n    ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used\n    ##\n    useExistingSecret: false\n\n    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the\n    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.\n    ##\n    secrets: []\n\n    ## If false then the user will opt out of automounting API credentials.\n    ##\n    automountServiceAccountToken: true\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.\n    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.\n    ##\n    configMaps: []\n\n    ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for\n    ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.\n    ##\n    # configSecret:\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec\n    web: {}\n\n    ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.\n    ##\n    alertmanagerConfigSelector: {}\n    ## Example which selects all alertmanagerConfig resources\n    ## with label \"alertconfig\" with values any of \"example-config\" or \"example-config-2\"\n    # alertmanagerConfigSelector:\n    #   matchExpressions:\n    #     - key: alertconfig\n    #       operator: In\n    #       values:\n    #         - example-config\n    #         - example-config-2\n    #\n    ## Example which selects all alertmanagerConfig resources with label \"role\" set to \"example-config\"\n    # alertmanagerConfigSelector:\n    #   matchLabels:\n    #     role: example-config\n\n    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.\n    ##\n    alertmanagerConfigNamespaceSelector: {}\n    ## Example which selects all namespaces\n    ## with label \"alertmanagerconfig\" with values any of \"example-namespace\" or \"example-namespace-2\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchExpressions:\n    #     - key: alertmanagerconfig\n    #       operator: In\n    #       values:\n    #         - example-namespace\n    #         - example-namespace-2\n\n    ## Example which selects all namespaces with label \"alertmanagerconfig\" set to \"enabled\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchLabels:\n    #     alertmanagerconfig: enabled\n\n    ## AlermanagerConfig to be used as top level configuration\n    ##\n    alertmanagerConfiguration: {}\n    ## Example with select a global alertmanagerconfig\n    # alertmanagerConfiguration:\n    #   name: global-alertmanager-Configuration\n\n    ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:\n    ##\n    alertmanagerConfigMatcherStrategy: {}\n    ## Example with use OnNamespace strategy\n    # alertmanagerConfigMatcherStrategy:\n    #   type: OnNamespace\n\n    ## Define Log Format\n    # Use logfmt (default) or json logging\n    logFormat: logfmt\n\n    ## Log level for Alertmanager to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 2\n\n    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 120h\n\n    ## Storage is the definition of how storage will be used by the Alertmanager instances.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #   selector: {}\n\n\n    ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false\n    ##\n    externalUrl:\n\n    ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## scheme: HTTP scheme to use. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when connect to the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"soft\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the alertmanager instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: alertmanager\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the Alertmanager UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.\n    ##\n    containers: []\n    # containers:\n    # - name: oauth-proxy\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1\n    #   args:\n    #   - --upstream=http://127.0.0.1:9093\n    #   - --http-address=0.0.0.0:8081\n    #   - --metrics-address=0.0.0.0:8082\n    #   - ...\n    #   ports:\n    #   - containerPort: 8081\n    #     name: oauth-proxy\n    #     protocol: TCP\n    #   - containerPort: 8082\n    #     name: oauth-metrics\n    #     protocol: TCP\n    #   resources: {}\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.\n    ##\n    additionalPeers: []\n\n    ## PortName to use for Alert Manager.\n    ##\n    portName: \"http-web\"\n\n    ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918\n    ##\n    clusterAdvertiseAddress: false\n\n    ## clusterGossipInterval determines interval between gossip attempts.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterGossipInterval: \"\"\n\n    ## clusterPeerTimeout determines timeout for cluster peering.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterPeerTimeout: \"\"\n\n    ## clusterPushpullInterval determines interval between pushpull attempts.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterPushpullInterval: \"\"\n\n    ## clusterLabel defines the identifier that uniquely identifies the Alertmanager cluster.\n    clusterLabel: \"\"\n\n    ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.\n    ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.\n    forceEnableClusterMode: false\n\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\n    minReadySeconds: 0\n\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\n    additionalConfig: {}\n\n    ## Additional configuration which is not covered by the properties above.\n    ## Useful, if you need advanced templating inside alertmanagerSpec.\n    ## Otherwise, use alertmanager.alertmanagerSpec.additionalConfig (passed through tpl)\n    additionalConfigString: \"\"\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml\n##\ngrafana:\n  enabled: true\n  namespaceOverride: \"\"\n\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDatasources: false\n\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n  ## Deploy default dashboards\n  ##\n  defaultDashboardsEnabled: true\n\n  ## Timezone for the default dashboards\n  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg\n  ##\n  defaultDashboardsTimezone: utc\n\n  ## Editable flag for the default dashboards\n  ##\n  defaultDashboardsEditable: true\n\n  adminPassword: prom-operator\n\n  rbac:\n    ## If true, Grafana PSPs will be created\n    ##\n    pspEnabled: false\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: false\n\n    ## IngressClassName for Grafana Ingress.\n    ## Should be provided if Ingress is enable.\n    ##\n    # ingressClassName: nginx\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations: {}\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  # # To make Grafana persistent (Using Statefulset)\n  # #\n  # persistence:\n  #   enabled: true\n  #   type: sts\n  #   storageClassName: \"storageClassName\"\n  #   accessModes:\n  #     - ReadWriteOnce\n  #   size: 20Gi\n  #   finalizers:\n  #     - kubernetes.io/pvc-protection\n\n  serviceAccount:\n    create: true\n    autoMount: true\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n      labelValue: \"1\"\n      # Allow discovery in all namespaces for dashboards\n      searchNamespace: ALL\n\n      # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels\n      enableNewTablePanelSyntax: false\n\n      ## Annotations for Grafana dashboard configmaps\n      ##\n      annotations: {}\n      multicluster:\n        global:\n          enabled: false\n        etcd:\n          enabled: false\n      provider:\n        allowUiUpdates: false\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n      isDefaultDatasource: true\n\n      name: Prometheus\n      uid: prometheus\n\n      ## URL of prometheus datasource\n      ##\n      # url: http://prometheus-stack-prometheus:9090/\n\n      ## Prometheus request timeout in seconds\n      # timeout: 30\n\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\n      # defaultDatasourceScrapeInterval: 15s\n\n      ## Annotations for Grafana datasource configmaps\n      ##\n      annotations: {}\n\n      ## Set method for HTTP to send query to datasource\n      httpMethod: POST\n\n      ## Create datasource for each Pod of Prometheus StatefulSet;\n      ## this uses headless service `prometheus-operated` which is\n      ## created by Prometheus Operator\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286\n      createPrometheusReplicasDatasources: false\n      label: grafana_datasource\n      labelValue: \"1\"\n\n      ## Field with internal link pointing to existing data source in Grafana.\n      ## Can be provisioned via additionalDataSources\n      exemplarTraceIdDestinations: {}\n        # datasourceUid: Jaeger\n        # traceIdLabelName: trace_id\n      alertmanager:\n        enabled: true\n        name: Alertmanager\n        uid: alertmanager\n        handleGrafanaManagedAlerts: false\n        implementation: prometheus\n\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  deleteDatasources: []\n  # - name: example-datasource\n  #   orgId: 1\n\n  ## Configure additional grafana datasources (passed through tpl)\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources: []\n  # - name: prometheus-sample\n  #   access: proxy\n  #   basicAuth: true\n  #   secureJsonData:\n  #       basicAuthPassword: pass\n  #   basicAuthUser: daco\n  #   editable: false\n  #   jsonData:\n  #       tlsSkipVerify: true\n  #   orgId: 1\n  #   type: prometheus\n  #   url: https://{{ printf \"%s-prometheus.svc\" .Release.Name }}:9090\n  #   version: 1\n\n  # Flag to mark provisioned data sources for deletion if they are no longer configured.\n  # It takes no effect if data sources are already listed in the deleteDatasources section.\n  # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-config-file\n  prune: false\n\n  ## Passed to grafana subchart and used by servicemonitor below\n  ##\n  service:\n    portName: http-web\n    ipFamilies: []\n    ipFamilyPolicy: \"\"\n\n  serviceMonitor:\n    # If true, a ServiceMonitor CRD is created for a prometheus operator\n    # https://github.com/coreos/prometheus-operator\n    #\n    enabled: true\n\n    # Path to use for scraping metrics. Might be different if server.root_url is set\n    # in grafana.ini\n    path: \"/metrics\"\n\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n\n    # labels for the ServiceMonitor\n    labels: {}\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    #\n    interval: \"\"\n    scheme: http\n    tlsConfig: {}\n    scrapeTimeout: 30s\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Flag to disable all the kubernetes component scrapers\n##\nkubernetesServiceMonitors:\n  enabled: true\n\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: true\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings:\n      # Drop excessively noisy apiserver buckets.\n      - action: drop\n        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\n        sourceLabels:\n          - __name__\n          - le\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels:\n    #     - __meta_kubernetes_namespace\n    #     - __meta_kubernetes_service_name\n    #     - __meta_kubernetes_endpoint_port_name\n    #   action: keep\n    #   regex: default;kubernetes;https\n    # - targetLabel: __address__\n    #   replacement: kubernetes.default.svc:443\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: true\n  namespace: kube-system\n\n  serviceMonitor:\n    ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.\n    ##\n    attachMetadata:\n      node: false\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## If true, Prometheus use (respect) labels provided by exporter.\n    ##\n    honorLabels: true\n\n    ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.\n    ##\n    honorTimestamps: true\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/prometheus-operator/prometheus-operator/issues/926\n    ##\n    https: true\n\n    ## Skip TLS certificate validation when scraping.\n    ## This is enabled by default because kubelet serving certificate deployed by kubeadm is by default self-signed\n    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs\n    ##\n    insecureSkipVerify: true\n\n    ## Enable scraping /metrics/cadvisor from kubelet's service\n    ##\n    cAdvisor: true\n\n    ## Enable scraping /metrics/probes from kubelet's service\n    ##\n    probes: true\n\n    ## Enable scraping /metrics/resource from kubelet's service\n    ## This is disabled by default because container metrics are already exposed by cAdvisor\n    ##\n    resource: false\n    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource\n    resourcePath: \"/metrics/resource/v1alpha1\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    cAdvisorMetricRelabelings:\n      # Drop less useful container CPU metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'\n      # Drop less useful container / always zero filesystem metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'\n      # Drop less useful / always zero container memory metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_memory_(mapped_file|swap)'\n      # Drop less useful container process metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_(file_descriptors|tasks_state|threads_max)'\n      # Drop container spec metrics that overlap with kube-state-metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_spec.*'\n      # Drop cgroup metrics with no pod.\n      - sourceLabels: [id, pod]\n        action: drop\n        regex: '.+;'\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    cAdvisorRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    resourceRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    relabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: true\n\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.22.\n    ##\n    port: null\n    targetPort: null\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   component: kube-controller-manager\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    component: kube-controller-manager\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    # Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    # Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: true\n  service:\n    enabled: true\n    port: 9153\n    targetPort: 9153\n\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    k8s-app: kube-dns\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    dnsmasq:\n      port: 10054\n      targetPort: 10054\n    skydns:\n      port: 10055\n      targetPort: 10055\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    k8s-app: kube-dns\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqMetricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqRelabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: true\n\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    port: 2381\n    targetPort: 2381\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   component: etcd\n\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    component: etcd\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping kube scheduler\n##\nkubeScheduler:\n  enabled: true\n\n  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeScheduler.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.23.\n    ##\n    port: null\n    targetPort: null\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   component: kube-scheduler\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    ## Enable scraping kube-scheduler over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    component: kube-scheduler\n\n    ## Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    ## Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping kube proxy\n##\nkubeProxy:\n  enabled: true\n\n  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  service:\n    enabled: true\n    port: 10249\n    targetPort: 10249\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   k8s-app: kube-proxy\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    k8s-app: kube-proxy\n\n    ## Enable scraping kube-proxy over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\n    ##\n    https: false\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n    ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor\n    targetLabels: []\n\n## Component scraping kube state metrics\n##\nkubeStateMetrics:\n  enabled: true\n\n## Configuration for kube-state-metrics subchart\n##\nkube-state-metrics:\n  namespaceOverride: \"\"\n  rbac:\n    create: true\n  releaseLabel: true\n  prometheus:\n    monitor:\n      enabled: true\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n      ##\n      sampleLimit: 0\n\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n      ##\n      targetLimit: 0\n\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelLimit: 0\n\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelNameLengthLimit: 0\n\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelValueLengthLimit: 0\n\n      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      # Keep labels from scraped data, overriding server-side labels\n      ##\n      honorLabels: true\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - action: keep\n      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n      #   sourceLabels: [__name__]\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n\n  selfMonitor:\n    enabled: false\n\n## Deploy node exporter as a daemonset to all nodes\n##\nnodeExporter:\n  enabled: true\n  operatingSystems:\n    linux:\n      enabled: true\n    aix:\n      enabled: true\n    darwin:\n      enabled: true\n\n  ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n## Configuration for prometheus-node-exporter subchart\n##\nprometheus-node-exporter:\n  namespaceOverride: \"\"\n  podLabels:\n    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards\n    ##\n    jobLabel: node-exporter\n  releaseLabel: true\n  extraArgs:\n    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\n  service:\n    portName: http-metrics\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    labels:\n      jobLabel: node-exporter\n\n  prometheus:\n    monitor:\n      enabled: true\n\n      jobLabel: jobLabel\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n      ##\n      sampleLimit: 0\n\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n      ##\n      targetLimit: 0\n\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelLimit: 0\n\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelNameLengthLimit: 0\n\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelValueLengthLimit: 0\n\n      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - sourceLabels: [__name__]\n      #   separator: ;\n      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+\n      #   replacement: $1\n      #   action: drop\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n  rbac:\n    ## If true, create PSPs for node-exporter\n    ##\n    pspEnabled: false\n\n## Manages Prometheus and Alertmanager components\n##\nprometheusOperator:\n  enabled: true\n\n  ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-operator' by default\n  fullnameOverride: \"\"\n\n  ## Number of old replicasets to retain ##\n  ## The default value is 10, 0 will garbage-collect old replicasets ##\n  revisionHistoryLimit: 10\n\n  ## Strategy of the deployment\n  ##\n  strategy: {}\n\n  ## Prometheus-Operator v0.39.0 and later support TLS natively.\n  ##\n  tls:\n    enabled: true\n    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\n    tlsMinVersion: VersionTLS13\n    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\n    internalPort: 10250\n\n  ## Liveness probe for the prometheusOperator deployment\n  ##\n  livenessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 0\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 1\n  ## Readiness probe for the prometheusOperator deployment\n  ##\n  readinessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 0\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 1\n\n  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted\n  ## rules from making their way into prometheus and potentially preventing the container from starting\n  admissionWebhooks:\n    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly\n    ## IgnoreOnInstallOnly - If Release.IsInstall returns \"true\", set \"Ignore\" otherwise \"Fail\"\n    failurePolicy: \"\"\n    ## The default timeoutSeconds is 10 and the maximum value is 30.\n    timeoutSeconds: 10\n    enabled: true\n    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.\n    ## If unspecified, system trust roots on the apiserver are used.\n    caBundle: \"\"\n    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.\n    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own\n    ## certs ahead of time if you wish.\n    ##\n    annotations: {}\n    #   argocd.argoproj.io/hook: PreSync\n    #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\n\n    namespaceSelector: {}\n    objectSelector: {}\n\n    mutatingWebhookConfiguration:\n      annotations: {}\n      #   argocd.argoproj.io/hook: PreSync\n\n    validatingWebhookConfiguration:\n      annotations: {}\n      #   argocd.argoproj.io/hook: PreSync\n\n    deployment:\n      enabled: false\n\n      ## Number of replicas\n      ##\n      replicas: 1\n\n      ## Strategy of the deployment\n      ##\n      strategy: {}\n\n      # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n      podDisruptionBudget: {}\n        # maxUnavailable: 1\n        # minAvailable: 1\n\n      ## Number of old replicasets to retain ##\n      ## The default value is 10, 0 will garbage-collect old replicasets ##\n      revisionHistoryLimit: 10\n\n      ## Prometheus-Operator v0.39.0 and later support TLS natively.\n      ##\n      tls:\n        enabled: true\n        # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\n        tlsMinVersion: VersionTLS13\n        # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\n        internalPort: 10250\n\n      ## Service account for Prometheus Operator Webhook to use.\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n      ##\n      serviceAccount:\n        annotations: {}\n        automountServiceAccountToken: false\n        create: true\n        name: \"\"\n\n      ## Configuration for Prometheus operator Webhook service\n      ##\n      service:\n        annotations: {}\n        labels: {}\n        clusterIP: \"\"\n        ipDualStack:\n          enabled: false\n          ipFamilies: [\"IPv6\", \"IPv4\"]\n          ipFamilyPolicy: \"PreferDualStack\"\n\n        ## Port to expose on each node\n        ## Only used if service.type is 'NodePort'\n        ##\n        nodePort: 31080\n\n        nodePortTls: 31443\n\n        ## Additional ports to open for Prometheus operator Webhook service\n        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n        ##\n        additionalPorts: []\n\n        ## Loadbalancer IP\n        ## Only use if service.type is \"LoadBalancer\"\n        ##\n        loadBalancerIP: \"\"\n        loadBalancerSourceRanges: []\n\n        ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n        ##\n        externalTrafficPolicy: Cluster\n\n        ## Service type\n        ## NodePort, ClusterIP, LoadBalancer\n        ##\n        type: ClusterIP\n\n        ## List of IP addresses at which the Prometheus server service is available\n        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n        ##\n        externalIPs: []\n\n      # ## Labels to add to the operator webhook deployment\n      # ##\n      labels: {}\n\n      ## Annotations to add to the operator webhook deployment\n      ##\n      annotations: {}\n\n      ## Labels to add to the operator webhook pod\n      ##\n      podLabels: {}\n\n      ## Annotations to add to the operator webhook pod\n      ##\n      podAnnotations: {}\n\n      ## Assign a PriorityClassName to pods if set\n      # priorityClassName: \"\"\n\n      ## Define Log Format\n      # Use logfmt (default) or json logging\n      # logFormat: logfmt\n\n      ## Decrease log verbosity to errors only\n      # logLevel: error\n\n      ## Prometheus-operator webhook image\n      ##\n      image:\n        registry: quay.io\n        repository: prometheus-operator/admission-webhook\n        # if not set appVersion field from Chart.yaml is used\n        tag: \"\"\n        sha: \"\"\n        pullPolicy: IfNotPresent\n\n      ## Define Log Format\n      # Use logfmt (default) or json logging\n      # logFormat: logfmt\n\n      ## Decrease log verbosity to errors only\n      # logLevel: error\n\n\n      ## Liveness probe\n      ##\n      livenessProbe:\n        enabled: true\n        failureThreshold: 3\n        initialDelaySeconds: 30\n        periodSeconds: 10\n        successThreshold: 1\n        timeoutSeconds: 1\n\n      ## Readiness probe\n      ##\n      readinessProbe:\n        enabled: true\n        failureThreshold: 3\n        initialDelaySeconds: 5\n        periodSeconds: 10\n        successThreshold: 1\n        timeoutSeconds: 1\n\n      ## Resource limits \u0026 requests\n      ##\n      resources: {}\n      # limits:\n      #   cpu: 200m\n      #   memory: 200Mi\n      # requests:\n      #   cpu: 100m\n      #   memory: 100Mi\n\n      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n      ##\n      hostNetwork: false\n\n      ## Define which Nodes the Pods are scheduled on.\n      ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n      ##\n      nodeSelector: {}\n\n      ## Tolerations for use with node taints\n      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n      ##\n      tolerations: []\n      # - key: \"key\"\n      #   operator: \"Equal\"\n      #   value: \"value\"\n      #   effect: \"NoSchedule\"\n\n      ## Assign custom affinity rules to the prometheus operator\n      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n      ##\n      affinity: {}\n        # nodeAffinity:\n        #   requiredDuringSchedulingIgnoredDuringExecution:\n        #     nodeSelectorTerms:\n        #     - matchExpressions:\n        #       - key: kubernetes.io/e2e-az-name\n        #         operator: In\n        #         values:\n        #         - e2e-az1\n      #         - e2e-az2\n      dnsConfig: {}\n        # nameservers:\n        #   - 1.2.3.4\n        # searches:\n        #   - ns1.svc.cluster-domain.example\n        #   - my.dns.search.suffix\n        # options:\n        #   - name: ndots\n        #     value: \"2\"\n        #   - name: edns0\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n\n      ## Container-specific security context configuration\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n      ##\n      containerSecurityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n            - ALL\n\n      ## If false then the user will opt out of automounting API credentials.\n      ##\n      automountServiceAccountToken: true\n\n    patch:\n      enabled: true\n      image:\n        registry: registry.k8s.io\n        repository: ingress-nginx/kube-webhook-certgen\n        tag: v20221220-controller-v1.5.1-58-g787ea74b6\n        sha: \"\"\n        pullPolicy: IfNotPresent\n      resources: {}\n      ## Provide a priority class name to the webhook patching job\n      ##\n      priorityClassName: \"\"\n      ttlSecondsAfterFinished: 60\n      annotations: {}\n      #   argocd.argoproj.io/hook: PreSync\n      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\n      podAnnotations: {}\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n\n      ## SecurityContext holds pod-level security attributes and common container settings.\n      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n      ##\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n      ## Service account for Prometheus Operator Webhook Job Patch to use.\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n      ##\n      serviceAccount:\n        create: true\n        annotations: {}\n        automountServiceAccountToken: true\n\n    # Security context for create job container\n    createSecretJob:\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n          - ALL\n\n      # Security context for patch job container\n    patchWebhookJob:\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n          - ALL\n\n    # Use certmanager to generate webhook certs\n    certManager:\n      enabled: false\n      # self-signed root certificate\n      rootCert:\n        duration: \"\"  # default to be 5y\n      admissionCert:\n        duration: \"\"  # default to be 1y\n      # issuerRef:\n      #   name: \"issuer\"\n      #   kind: \"ClusterIssuer\"\n\n  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).\n  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration\n  ##\n  namespaces: {}\n    # releaseNamespace: true\n    # additional:\n    # - kube-system\n\n  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).\n  ##\n  denyNamespaces: []\n\n  ## Filter namespaces to look for prometheus-operator custom resources\n  ##\n  alertmanagerInstanceNamespaces: []\n  alertmanagerConfigNamespaces: []\n  prometheusInstanceNamespaces: []\n  thanosRulerInstanceNamespaces: []\n\n  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.\n  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)\n  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094\n  ##\n  # clusterDomain: \"cluster.local\"\n\n  networkPolicy:\n    ## Enable creation of NetworkPolicy resources.\n    ##\n    enabled: false\n\n    ## Flavor of the network policy to use.\n    #  Can be:\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\n    flavor: kubernetes\n\n    # cilium:\n    #   egress:\n\n    ## match labels used in selector\n    # matchLabels: {}\n\n  ## Service account for Prometheus Operator to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    automountServiceAccountToken: true\n    annotations: {}\n\n  ## Configuration for Prometheus operator service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n  ## Port to expose on each node\n  ## Only used if service.type is 'NodePort'\n  ##\n    nodePort: 30080\n\n    nodePortTls: 30443\n\n  ## Additional ports to open for Prometheus operator service\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n  ##\n    additionalPorts: []\n\n  ## Loadbalancer IP\n  ## Only use if service.type is \"LoadBalancer\"\n  ##\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n  ## Service type\n  ## NodePort, ClusterIP, LoadBalancer\n  ##\n    type: ClusterIP\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n  # ## Labels to add to the operator deployment\n  # ##\n  labels: {}\n\n  ## Annotations to add to the operator deployment\n  ##\n  annotations: {}\n\n  ## Labels to add to the operator pod\n  ##\n  podLabels: {}\n\n  ## Annotations to add to the operator pod\n  ##\n  podAnnotations: {}\n\n  ## Assign a PriorityClassName to pods if set\n  # priorityClassName: \"\"\n\n  ## Define Log Format\n  # Use logfmt (default) or json logging\n  # logFormat: logfmt\n\n  ## Decrease log verbosity to errors only\n  # logLevel: error\n\n  kubeletService:\n    ## If true, the operator will create and maintain a service for scraping kubelets\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md\n    ##\n    enabled: true\n    namespace: kube-system\n    selector: \"\"\n    ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-kubelet' by default\n    name: \"\"\n\n  ## Create Endpoints objects for kubelet targets.\n  kubeletEndpointsEnabled: true\n  ## Create EndpointSlice objects for kubelet targets.\n  kubeletEndpointSliceEnabled: false\n\n  ## Extra arguments to pass to prometheusOperator\n  # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/operator.md\n  extraArgs: []\n  #  - --labels=\"cluster=talos-cluster\"\n\n  ## Create a servicemonitor for the operator\n  ##\n  serviceMonitor:\n    ## If true, create a serviceMonitor for prometheus operator\n    ##\n    selfMonitor: true\n\n    ## Labels for ServiceMonitor\n    additionalLabels: {}\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.\n    scrapeTimeout: \"\"\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Resource limits \u0026 requests\n  ##\n  resources: {}\n  # limits:\n  #   cpu: 200m\n  #   memory: 200Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 100Mi\n\n  ## Operator Environment\n  ##  env:\n  ##    VARIABLE: value\n  env:\n    GOGC: \"30\"\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  ## Define which Nodes the Pods are scheduled on.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Tolerations for use with node taints\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  # - key: \"key\"\n  #   operator: \"Equal\"\n  #   value: \"value\"\n  #   effect: \"NoSchedule\"\n\n  ## Assign custom affinity rules to the prometheus operator\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n  securityContext:\n    fsGroup: 65534\n    runAsGroup: 65534\n    runAsNonRoot: true\n    runAsUser: 65534\n    seccompProfile:\n      type: RuntimeDefault\n\n  ## Container-specific security context configuration\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ##\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n      - ALL\n\n  # Enable vertical pod autoscaler support for prometheus-operator\n  verticalPodAutoscaler:\n    enabled: false\n\n    # Recommender responsible for generating recommendation for the object.\n    # List should be empty (then the default recommender will generate the recommendation)\n    # or contain exactly one recommender.\n    # recommenders:\n    # - name: custom-recommender-performance\n\n    # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory\n    controlledResources: []\n    # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.\n    # controlledValues: RequestsAndLimits\n\n    # Define the max allowed resources for the pod\n    maxAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n    # Define the min allowed resources for the pod\n    minAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n\n    updatePolicy:\n      # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction\n      # minReplicas: 1\n      # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates\n      # are applied during the life of a Pod. Possible values are \"Off\", \"Initial\", \"Recreate\", and \"Auto\".\n      updateMode: Auto\n\n  ## Prometheus-operator image\n  ##\n  image:\n    registry: quay.io\n    repository: prometheus-operator/prometheus-operator\n    # if not set appVersion field from Chart.yaml is used\n    tag: \"\"\n    sha: \"\"\n    pullPolicy: IfNotPresent\n\n  ## Prometheus image to use for prometheuses managed by the operator\n  ##\n  # prometheusDefaultBaseImage: prometheus/prometheus\n\n  ## Prometheus image registry to use for prometheuses managed by the operator\n  ##\n  # prometheusDefaultBaseImageRegistry: quay.io\n\n  ## Alertmanager image to use for alertmanagers managed by the operator\n  ##\n  # alertmanagerDefaultBaseImage: prometheus/alertmanager\n\n  ## Alertmanager image registry to use for alertmanagers managed by the operator\n  ##\n  # alertmanagerDefaultBaseImageRegistry: quay.io\n\n  ## Prometheus-config-reloader\n  ##\n  prometheusConfigReloader:\n    image:\n      registry: quay.io\n      repository: prometheus-operator/prometheus-config-reloader\n      # if not set appVersion field from Chart.yaml is used\n      tag: \"\"\n      sha: \"\"\n\n    # add prometheus config reloader liveness and readiness probe. Default: false\n    enableProbe: false\n\n    # resource config for prometheusConfigReloader\n    resources: {}\n      # requests:\n      #   cpu: 200m\n      #   memory: 50Mi\n      # limits:\n      #   cpu: 200m\n      #   memory: 50Mi\n\n  ## Thanos side-car image when configured\n  ##\n  thanosImage:\n    registry: quay.io\n    repository: thanos/thanos\n    tag: v0.36.1\n    sha: \"\"\n\n  ## Set a Label Selector to filter watched prometheus and prometheusAgent\n  ##\n  prometheusInstanceSelector: \"\"\n\n  ## Set a Label Selector to filter watched alertmanager\n  ##\n  alertmanagerInstanceSelector: \"\"\n\n  ## Set a Label Selector to filter watched thanosRuler\n  thanosRulerInstanceSelector: \"\"\n\n  ## Set a Field Selector to filter watched secrets\n  ##\n  secretFieldSelector: \"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\"\n\n  ## If false then the user will opt out of automounting API credentials.\n  ##\n  automountServiceAccountToken: true\n\n  ## Additional volumes\n  ##\n  extraVolumes: []\n\n  ## Additional volume mounts\n  ##\n  extraVolumeMounts: []\n\n## Deploy a Prometheus instance\n##\nprometheus:\n  enabled: true\n\n  ## Toggle prometheus into agent mode\n  ## Note many of features described below (e.g. rules, query, alerting, remote read, thanos) will not work in agent mode.\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/designs/prometheus-agent.md\n  ##\n  agentMode: false\n\n  ## Annotations for Prometheus\n  ##\n  annotations: {}\n\n  ## Configure network policy for the prometheus\n  networkPolicy:\n    enabled: false\n\n    ## Flavor of the network policy to use.\n    #  Can be:\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\n    flavor: kubernetes\n\n    # cilium:\n    #   endpointSelector:\n    #   egress:\n    #   ingress:\n\n    # egress:\n    # - {}\n    # ingress:\n    # - {}\n    # podSelector:\n    #   matchLabels:\n    #     app: prometheus\n\n  ## Service account for Prometheuses to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n    automountServiceAccountToken: true\n\n  # Service for thanos service discovery on sidecar\n  # Enable this can make Thanos Query can use\n  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery\n  # Thanos sidecar on prometheus nodes\n  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)\n  thanosService:\n    enabled: false\n    annotations: {}\n    labels: {}\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Service dual stack\n    ##\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## ClusterIP to assign\n    # Default is to make this a headless service (\"None\")\n    clusterIP: \"\"\n\n    ## Port to expose on each node, if service type is NodePort\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  # ServiceMonitor to scrape Sidecar metrics\n  # Needs thanosService to be enabled as well\n  thanosServiceMonitor:\n    enabled: false\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    metricRelabelings: []\n\n    ## relabel configs to apply to samples before ingestion.\n    relabelings: []\n\n  # Service for external access to sidecar\n  # Enabling this creates a service to expose thanos-sidecar outside the cluster.\n  thanosServiceExternal:\n    enabled: false\n    annotations: {}\n    labels: {}\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Port to expose on each node\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  ## Configuration for Prometheus service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## Port for Prometheus Service to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## Port for Prometheus Reloader to listen on\n    ##\n    reloaderWebPort: 8080\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30090\n\n    ## Loadbalancer IP\n    ## Only use if service.type is \"LoadBalancer\"\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Additional ports to open for Prometheus service\n    ##\n    additionalPorts: []\n    # additionalPorts:\n    # - name: oauth-proxy\n    #   port: 8081\n    #   targetPort: 8081\n    # - name: oauth-metrics\n    #   port: 8082\n    #   targetPort: 8082\n\n    ## Consider that all endpoints are considered \"ready\" even if the Pods themselves are not\n    ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec\n    publishNotReadyAddresses: false\n\n    ## If you want to make sure that connections from a particular client are passed to the same Pod each time\n    ## Accepts 'ClientIP' or 'None'\n    ##\n    sessionAffinity: None\n\n    ## If you want to modify the ClientIP sessionAffinity timeout\n    ## The value must be \u003e0 \u0026\u0026 \u003c=86400(for 1 day) if ServiceAffinity == \"ClientIP\"\n    ##\n    sessionAffinityConfig:\n      clientIP:\n        timeoutSeconds: 10800\n\n  ## Configuration for creating a separate Service for each statefulset Prometheus replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Prometheus Service per replica to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30091\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Service dual stack\n    ##\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n  ## Configure pod disruption budgets for Prometheus\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  # Ingress exposes thanos sidecar outside the cluster\n  thanosIngress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n    servicePort: 10901\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30901\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - thanos-gateway.domain.com\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Thanos Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanos-gateway-tls\n    #   hosts:\n    #   - thanos-gateway.domain.com\n    #\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Redirect ingress to an additional defined port on the service\n    # servicePort: 8081\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enabled.\n    ##\n    # hosts:\n    #   - prometheus.domain.com\n    hosts: []\n\n    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Prometheus Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n      # - secretName: prometheus-general-tls\n      #   hosts:\n      #     - prometheus.example.com\n\n  # -- BETA: Configure the gateway routes for the chart here.\n  # More routes can be added by adding a dictionary key like the 'main' route.\n  # Be aware that this is an early beta of this feature,\n  # kube-prometheus-stack does not guarantee this works and is subject to change.\n  # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk\n  # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)\n  route:\n    main:\n      # -- Enables or disables the route\n      enabled: false\n\n      # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2\n      apiVersion: gateway.networking.k8s.io/v1\n      # -- Set the route kind\n      # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute\n      kind: HTTPRoute\n\n      annotations: {}\n      labels: {}\n\n      hostnames: []\n      # - my-filter.example.com\n      parentRefs: []\n      # - name: acme-gw\n\n      matches:\n      - path:\n          type: PathPrefix\n          value: /\n\n      ## Filters define the filters that are applied to requests that match this rule.\n      filters: []\n\n      ## Additional custom rules that can be added to the route\n      additionalRules: []\n\n  ## Configuration for creating an Ingress that will map to each Prometheus replica service\n  ## prometheus.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for Prometheus per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"prometheus\"\n\n  ## Configure additional options for default pod security policy for Prometheus\n  ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  podSecurityPolicy:\n    allowedCapabilities: []\n    allowedHostPaths: []\n    volumes: []\n\n  serviceMonitor:\n    ## If true, create a serviceMonitor for prometheus\n    ##\n    selfMonitor: true\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional Endpoints\n    ##\n    additionalEndpoints: []\n    # - port: oauth-metrics\n    #   path: /metrics\n\n  ## Settings affecting prometheusSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec\n  ##\n  prometheusSpec:\n    ## Statefulset's persistent volume claim retention policy\n    ## pvcDeleteOnStsDelete and pvcDeleteOnStsScale determine whether\n    ## statefulset's PVCs are deleted (true) or retained (false) on scaling down\n    ## and deleting statefulset, respectively. Requires 1.27.0+.\n    ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\n    persistentVolumeClaimRetentionPolicy: {}\n    #  whenDeleted: Retain\n    #  whenScaled: Retain\n\n    ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos\n    ##\n    ## AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in the pod,\n    ## If the field isn’t set, the operator mounts the service account token by default.\n    ## Warning: be aware that by default, Prometheus requires the service account token for Kubernetes service discovery,\n    ## It is possible to use strategic merge patch to project the service account token into the ‘prometheus’ container.\n    automountServiceAccountToken: true\n\n    disableCompaction: false\n    ## APIServerConfig\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#apiserverconfig\n    ##\n    apiserverConfig: {}\n\n    ## Allows setting additional arguments for the Prometheus container\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus\n    additionalArgs: []\n\n    ## Interval between consecutive scrapes.\n    ## Defaults to 30s.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183\n    ##\n    scrapeInterval: \"\"\n\n    ## Number of seconds to wait for target to respond before erroring\n    ##\n    scrapeTimeout: \"\"\n\n    ## List of scrape classes to expose to scraping objects such as\n    ## PodMonitors, ServiceMonitors, Probes and ScrapeConfigs.\n    ##\n    scrapeClasses: []\n    # - name: istio-mtls\n    #   default: false\n    #   tlsConfig:\n    #     caFile: /etc/prometheus/secrets/istio.default/root-cert.pem\n    #     certFile: /etc/prometheus/secrets/istio.default/cert-chain.pem\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.\n    ##\n    listenLocal: false\n\n    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.\n    ## This is disabled by default.\n    ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis\n    ##\n    enableAdminAPI: false\n\n    ## Sets version of Prometheus overriding the Prometheus version as derived\n    ## from the image tag. Useful in cases where the tag does not follow semver v2.\n    version: \"\"\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig\n    web: {}\n\n    ## Exemplars related settings that are runtime reloadable.\n    ## It requires to enable the exemplar storage feature to be effective.\n    exemplars: \"\"\n      ## Maximum number of exemplars stored in memory for all series.\n      ## If not set, Prometheus uses its default value.\n      ## A value of zero or less than zero disables the storage.\n      # maxSize: 100000\n\n    # EnableFeatures API enables access to Prometheus disabled features.\n    # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/\n    enableFeatures: []\n    # - exemplar-storage\n\n    ## Image of Prometheus.\n    ##\n    image:\n      registry: quay.io\n      repository: prometheus/prometheus\n      tag: v2.55.1\n      sha: \"\"\n\n    ## Tolerations for use with node taints\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    #  - key: \"key\"\n    #    operator: \"Equal\"\n    #    value: \"value\"\n    #    effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: prometheus\n\n    ## Alertmanagers to which alerts will be sent\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints\n    ##\n    ## Default configuration will connect to the alertmanager deployed as part of this release\n    ##\n    alertingEndpoints: []\n    # - name: \"\"\n    #   namespace: \"\"\n    #   port: http\n    #   scheme: http\n    #   pathPrefix: \"\"\n    #   tlsConfig: {}\n    #   bearerTokenFile: \"\"\n    #   apiVersion: v2\n\n    ## External labels to add to any time series or alerts when communicating with external systems\n    ##\n    externalLabels: {}\n\n    ## enable --web.enable-remote-write-receiver flag on prometheus-server\n    ##\n    enableRemoteWriteReceiver: false\n\n    ## Name of the external label used to denote replica name\n    ##\n    replicaExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote replica name\n    ##\n    replicaExternalLabelNameClear: false\n\n    ## Name of the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelNameClear: false\n\n    ## External URL at which Prometheus will be reachable.\n    ##\n    externalUrl: \"\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not\n    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated\n    ## with the new list of secrets.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.\n    ##\n    configMaps: []\n\n    ## QuerySpec defines the query command line flags when starting Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#queryspec\n    ##\n    query: {}\n\n    ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.\n    ruleNamespaceSelector: {}\n    ## Example which selects PrometheusRules in namespaces with label \"prometheus\" set to \"somelabel\"\n    # ruleNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the servicemonitors created\n    ##\n    serviceMonitorSelectorNilUsesHelmValues: true\n\n    ## ServiceMonitors to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    serviceMonitorSelector: {}\n    ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for ServiceMonitor discovery.\n    ##\n    serviceMonitorNamespaceSelector: {}\n    ## Example which selects ServiceMonitors in namespaces with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the podmonitors created\n    ##\n    podMonitorSelectorNilUsesHelmValues: true\n\n    ## PodMonitors to be selected for target discovery.\n    ## If {}, select all PodMonitors\n    ##\n    podMonitorSelector: {}\n    ## Example which selects PodMonitors with label \"prometheus\" set to \"somelabel\"\n    # podMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for PodMonitor discovery.\n    podMonitorNamespaceSelector: {}\n    ## Example which selects PodMonitor in namespaces with label \"prometheus\" set to \"somelabel\"\n    # podMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the probes created\n    ##\n    probeSelectorNilUsesHelmValues: true\n\n    ## Probes to be selected for target discovery.\n    ## If {}, select all Probes\n    ##\n    probeSelector: {}\n    ## Example which selects Probes with label \"prometheus\" set to \"somelabel\"\n    # probeSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for Probe discovery.\n    probeNamespaceSelector: {}\n    ## Example which selects Probe in namespaces with label \"prometheus\" set to \"somelabel\"\n    # probeNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the scrapeConfigs created\n    ##\n    ## If null and scrapeConfigSelector is also null, exclude field from the prometheusSpec\n    ## (keeping downward compatibility with older versions of CRD)\n    ##\n    scrapeConfigSelectorNilUsesHelmValues: true\n\n    ## scrapeConfigs to be selected for target discovery.\n    ## If {}, select all scrapeConfigs\n    ##\n    scrapeConfigSelector: {}\n    ## Example which selects scrapeConfigs with label \"prometheus\" set to \"somelabel\"\n    # scrapeConfigSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for scrapeConfig discovery.\n    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)\n    scrapeConfigNamespaceSelector: {}\n    ## Example which selects scrapeConfig in namespaces with label \"prometheus\" set to \"somelabel\"\n    # scrapeConfigNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## How long to retain metrics\n    ##\n    retention: 10d\n\n    ## Maximum size of metrics\n    ##\n    retentionSize: \"\"\n\n    ## Allow out-of-order/out-of-bounds samples ingested into Prometheus for a specified duration\n    ## See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb\n    tsdb:\n      outOfOrderTimeWindow: 0s\n\n    ## Enable compression of the write-ahead log using Snappy.\n    ##\n    walCompression: true\n\n    ## If true, the Operator won't process any Prometheus configuration changes\n    ##\n    paused: false\n\n    ## Number of replicas of each shard to deploy for a Prometheus deployment.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ##\n    replicas: 1\n\n    ## EXPERIMENTAL: Number of shards to distribute targets onto.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.\n    ## Increasing shards will not reshard data either but it will continue to be available from the same instances.\n    ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.\n    ## Sharding is done on the content of the `__address__` target meta-label.\n    ##\n    shards: 1\n\n    ## Log level for Prometheus be configured in\n    ##\n    logLevel: info\n\n    ## Log format for Prometheus be configured in\n    ##\n    logFormat: logfmt\n\n    ## Prefix used to register routes, overriding externalUrl route.\n    ## Useful for proxies that rewrite URLs.\n    ##\n    routePrefix: /\n\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the prometheus pods.\n    ##\n    podMetadata: {}\n    # labels:\n    #   app: prometheus\n    #   k8s-app: prometheus\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    podAntiAffinity: \"soft\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the prometheus instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## The remote_read spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec\n    remoteRead: []\n    # - url: http://remote1/read\n    ## additionalRemoteRead is appended to remoteRead\n    additionalRemoteRead: []\n\n    ## The remote_write spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec\n    remoteWrite: []\n    # - url: http://remote1/push\n    ## additionalRemoteWrite is appended to remoteWrite\n    additionalRemoteWrite: []\n\n    ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature\n    remoteWriteDashboards: false\n\n    ## Resource limits \u0026 requests\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Prometheus StorageSpec for persistent data\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storageSpec:\n    ## Using PersistentVolumeClaim\n    ##\n     volumeClaimTemplate:\n       spec:\n         storageClassName: gp2\n         accessModes: [\"ReadWriteOnce\"]\n         resources:\n           requests:\n             storage: 10Gi\n      #  selector: {}\n\n    ## Using tmpfs volume\n    ##\n    #  emptyDir:\n    #    medium: Memory\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations\n    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form\n    ## as specified in the official Prometheus documentation:\n    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are\n    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility\n    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible\n    ## scrape configs are going to break Prometheus after the upgrade.\n    ## AdditionalScrapeConfigs can be defined as a list or as a templated string.\n    ##\n    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the\n    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes\n    ##\n    additionalScrapeConfigs: \n    - job_name: kong-monitoring\n      metrics_path: /metrics\n      static_configs:\n        - targets:\n          - kong-kong-admin.kong.svc.cluster.local:8001\n    - job_name: argocd-rollout-monitoring\n      metrics_path: /metrics\n      static_configs:\n        - targets:\n          - my-release-argo-rollouts-metrics.argocd-rollout.svc.cluster.local.8090\n    # - job_name: kube-etcd\n    #   kubernetes_sd_configs:\n    #     - role: node\n    #   scheme: https\n    #   tls_config:\n    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n    #   relabel_configs:\n    #   - action: labelmap\n    #     regex: __meta_kubernetes_node_label_(.+)\n    #   - source_labels: [__address__]\n    #     action: replace\n    #     targetLabel: __address__\n    #     regex: ([^:;]+):(\\d+)\n    #     replacement: ${1}:2379\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: keep\n    #     regex: .*mst.*\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: replace\n    #     targetLabel: node\n    #     regex: (.*)\n    #     replacement: ${1}\n    #   metric_relabel_configs:\n    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n    #     action: labeldrop\n    #\n    ## If scrape config contains a repetitive section, you may want to use a template.\n    ## In the following example, you can see how to define `gce_sd_configs` for multiple zones\n    # additionalScrapeConfigs: |\n    #  - job_name: \"node-exporter\"\n    #    gce_sd_configs:\n    #    {{range $zone := .Values.gcp_zones}}\n    #    - project: \"project1\"\n    #      zone: \"{{$zone}}\"\n    #      port: 9100\n    #    {{end}}\n    #    relabel_configs:\n    #    ...\n\n\n    ## If additional scrape configurations are already deployed in a single secret file you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalScrapeConfigs\n    additionalScrapeConfigsSecret: {}\n      # enabled: false\n      # name:\n      # key:\n\n    ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful\n    ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'\n    additionalPrometheusSecretsAnnotations: {}\n\n    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified\n    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003calertmanager_config\u003e.\n    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.\n    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this\n    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release\n    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertManagerConfigs: []\n    # - consul_sd_configs:\n    #   - server: consul.dev.test:8500\n    #     scheme: http\n    #     datacenter: dev\n    #     tag_separator: ','\n    #     services:\n    #       - metrics-prometheus-alertmanager\n\n    ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertManagerConfigs\n    additionalAlertManagerConfigsSecret: {}\n      # name:\n      # key:\n      # optional: false\n\n    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended\n    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the\n    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.\n    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the\n    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel\n    ## configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertRelabelConfigs: []\n    # - separator: ;\n    #   regex: prometheus_replica\n    #   replacement: $1\n    #   action: labeldrop\n\n    ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertRelabelConfigs\n    additionalAlertRelabelConfigsSecret: {}\n      # name:\n      # key:\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.\n    ## This section is experimental, it may change significantly without deprecation notice in any release.\n    ## This is experimental and may change significantly without backward compatibility in any release.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec\n    ##\n    thanos: {}\n      # secretProviderClass:\n      #   provider: gcp\n      #   parameters:\n      #     secrets: |\n      #       - resourceName: \"projects/$PROJECT_ID/secrets/testsecret/versions/latest\"\n      #         fileName: \"objstore.yaml\"\n      ## ObjectStorageConfig configures object storage in Thanos.\n      # objectStorageConfig:\n      #   # use existing secret, if configured, objectStorageConfig.secret will not be used\n      #   existingSecret: {}\n      #     # name: \"\"\n      #     # key: \"\"\n      #   # will render objectStorageConfig secret data and configure it to be used by Thanos custom resource,\n      #   # ignored when prometheusspec.thanos.objectStorageConfig.existingSecret is set\n      #   # https://thanos.io/tip/thanos/storage.md/#s3\n      #   secret: {}\n      #     # type: S3\n      #     # config:\n      #     #   bucket: \"\"\n      #     #   endpoint: \"\"\n      #     #   region: \"\"\n      #     #   access_key: \"\"\n      #     #   secret_key: \"\"\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.\n    ## if using proxy extraContainer update targetPort with proxy container port\n    containers: []\n    # containers:\n    # - name: oauth-proxy\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1\n    #   args:\n    #   - --upstream=http://127.0.0.1:9090\n    #   - --http-address=0.0.0.0:8081\n    #   - --metrics-address=0.0.0.0:8082\n    #   - ...\n    #   ports:\n    #   - containerPort: 8081\n    #     name: oauth-proxy\n    #     protocol: TCP\n    #   - containerPort: 8082\n    #     name: oauth-metrics\n    #     protocol: TCP\n    #   resources: {}\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## PortName to use for Prometheus.\n    ##\n    portName: \"http-web\"\n\n    ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files\n    ## on the file system of the Prometheus container e.g. bearer token files.\n    arbitraryFSAccessThroughSMs: false\n\n    ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor\n    ## or PodMonitor to true, this overrides honor_labels to false.\n    overrideHonorLabels: false\n\n    ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.\n    overrideHonorTimestamps: false\n\n    ## When ignoreNamespaceSelectors is set to true, namespaceSelector from all PodMonitor, ServiceMonitor and Probe objects will be ignored,\n    ## they will only discover targets within the namespace of the PodMonitor, ServiceMonitor and Probe object,\n    ## and servicemonitors will be installed in the default service namespace.\n    ## Defaults to false.\n    ignoreNamespaceSelectors: false\n\n    ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.\n    ## The label value will always be the namespace of the object that is being created.\n    ## Disabled by default\n    enforcedNamespaceLabel: \"\"\n\n   ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.\n    ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair\n    ## Deprecated, use `excludedFromEnforcement` instead\n    prometheusRulesExcludedFromEnforce: []\n\n    ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects\n    ## to be excluded from enforcing a namespace label of origin.\n    ## Works only if enforcedNamespaceLabel set to true.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference\n    excludedFromEnforcement: []\n\n    ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,\n    ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such\n    ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions\n    ## of Prometheus \u003e= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)\n    queryLogFile: false\n\n    # Use to set global sample_limit for Prometheus. This act as default SampleLimit for ServiceMonitor or/and PodMonitor.\n    # Set to 'false' to disable global sample_limit. or set to a number to override the default value.\n    sampleLimit: false\n\n    # EnforcedKeepDroppedTargetsLimit defines on the number of targets dropped by relabeling that will be kept in memory.\n    # The value overrides any spec.keepDroppedTargets set by ServiceMonitor, PodMonitor, Probe objects unless spec.keepDroppedTargets\n    # is greater than zero and less than spec.enforcedKeepDroppedTargets. 0 means no limit.\n    enforcedKeepDroppedTargets: 0\n\n    ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit\n    ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall\n    ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.\n    enforcedSampleLimit: false\n\n    ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set\n    ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall\n    ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except\n    ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.\n    enforcedTargetLimit: false\n\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelLimit: false\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelNameLengthLimit: false\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this\n    ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus\n    ## versions 2.27.0 and newer.\n    enforcedLabelValueLengthLimit: false\n\n    ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental\n    ## in Prometheus so it may change in any upcoming release.\n    allowOverlappingBlocks: false\n\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\n    minReadySeconds: 0\n\n    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n    # Use the host's network namespace if true. Make sure to understand the security implications if you want to enable it.\n    # When hostNetwork is enabled, this will set dnsPolicy to ClusterFirstWithHostNet automatically.\n    hostNetwork: false\n\n    # HostAlias holds the mapping between IP and hostnames that will be injected\n    # as an entry in the pod’s hosts file.\n    hostAliases: []\n    #  - ip: 10.10.0.100\n    #    hostnames:\n    #      - a1.app.local\n    #      - b1.app.local\n\n    ## TracingConfig configures tracing in Prometheus.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheustracingconfig\n    tracingConfig: {}\n\n    ## Defines the service discovery role used to discover targets from ServiceMonitor objects and Alertmanager endpoints.\n    ## If set, the value should be either “Endpoints” or “EndpointSlice”. If unset, the operator assumes the “Endpoints” role.\n    serviceDiscoveryRole: \"\"\n\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\n    additionalConfig: {}\n\n    ## Additional configuration which is not covered by the properties above.\n    ## Useful, if you need advanced templating inside alertmanagerSpec.\n    ## Otherwise, use prometheus.prometheusSpec.additionalConfig (passed through tpl)\n    additionalConfigString: \"\"\n\n    ## Defines the maximum time that the `prometheus` container's startup probe\n    ## will wait before being considered failed. The startup probe will return\n    ## success after the WAL replay is complete. If set, the value should be\n    ## greater than 60 (seconds). Otherwise it will be equal to 900 seconds (15\n    ## minutes).\n    maximumStartupDurationSeconds: 0\n\n  additionalRulesForClusterRole: []\n  #  - apiGroups: [ \"\" ]\n  #    resources:\n  #      - nodes/proxy\n  #    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  additionalServiceMonitors: []\n  ## Name of the ServiceMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Service label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the service name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## labels to transfer from the kubernetes service to the target\n    ##\n    # targetLabels: []\n\n    ## labels to transfer from the kubernetes pods to the target\n    ##\n    # podTargetLabels: []\n\n    ## Label selector for services to which this ServiceMonitor applies\n    ##\n    # selector: {}\n      ## Example which selects all services to be monitored\n      ## with label \"monitoredby\" with values any of \"example-service-1\" or \"example-service-2\"\n      # matchExpressions:\n      #   - key: \"monitoredby\"\n      #     operator: In\n      #     values:\n      #       - example-service-1\n      #       - example-service-2\n\n      ## label selector for services\n      ##\n      # matchLabels: {}\n\n    ## Namespaces from which services are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected service to be monitored\n    ##\n    # endpoints: []\n      ## Name of the endpoint's service port\n      ## Mutually exclusive with targetPort\n      # - port: \"\"\n\n      ## Name or number of the endpoint's target port\n      ## Mutually exclusive with port\n      # - targetPort: \"\"\n\n      ## File containing bearer token to be used when scraping targets\n      ##\n      #   bearerTokenFile: \"\"\n\n      ## Interval at which metrics should be scraped\n      ##\n      #   interval: 30s\n\n      ## HTTP path to scrape for metrics\n      ##\n      #   path: /metrics\n\n      ## HTTP scheme to use for scraping\n      ##\n      #   scheme: http\n\n      ## TLS configuration to use when scraping the endpoint\n      ##\n      #   tlsConfig:\n\n          ## Path to the CA file\n          ##\n          # caFile: \"\"\n\n          ## Path to client certificate file\n          ##\n          # certFile: \"\"\n\n          ## Skip certificate verification\n          ##\n          # insecureSkipVerify: false\n\n          ## Path to client key file\n          ##\n          # keyFile: \"\"\n\n          ## Server name used to verify host name\n          ##\n          # serverName: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    # metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    # relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  additionalPodMonitors: []\n  ## Name of the PodMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Pod label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the pod endpoint name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## Label selector for pods to which this PodMonitor applies\n    ##\n    # selector: {}\n      ## Example which selects all Pods to be monitored\n      ## with label \"monitoredby\" with values any of \"example-pod-1\" or \"example-pod-2\"\n      # matchExpressions:\n      #   - key: \"monitoredby\"\n      #     operator: In\n      #     values:\n      #       - example-pod-1\n      #       - example-pod-2\n\n      ## label selector for pods\n      ##\n      # matchLabels: {}\n\n    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.\n    ##\n    # podTargetLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    # sampleLimit: 0\n\n    ## Namespaces from which pods are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected pods to be monitored\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint\n    ##\n    # podMetricsEndpoints: []\n\n## Configuration for thanosRuler\n## ref: https://thanos.io/tip/components/rule.md/\n##\nthanosRuler:\n\n  ## Deploy thanosRuler\n  ##\n  enabled: false\n\n  ## Annotations for ThanosRuler\n  ##\n  annotations: {}\n\n  ## Service account for ThanosRuler to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n\n  ## Configure pod disruption budgets for ThanosRuler\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n\n    labels: {}\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - thanosruler.domain.com\n\n    ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for ThanosRuler Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanosruler-general-tls\n    #   hosts:\n    #   - thanosruler.example.com\n\n  # -- BETA: Configure the gateway routes for the chart here.\n  # More routes can be added by adding a dictionary key like the 'main' route.\n  # Be aware that this is an early beta of this feature,\n  # kube-prometheus-stack does not guarantee this works and is subject to change.\n  # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk\n  # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)\n  route:\n    main:\n      # -- Enables or disables the route\n      enabled: false\n\n      # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2\n      apiVersion: gateway.networking.k8s.io/v1\n      # -- Set the route kind\n      # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute\n      kind: HTTPRoute\n\n      annotations: {}\n      labels: {}\n\n      hostnames: []\n      # - my-filter.example.com\n      parentRefs: []\n      # - name: acme-gw\n\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n\n      ## Filters define the filters that are applied to requests that match this rule.\n      filters: []\n\n      ## Additional custom rules that can be added to the route\n      additionalRules: []\n\n  ## Configuration for ThanosRuler service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## Port for ThanosRuler Service to listen on\n    ##\n    port: 10902\n    ## To be used with a proxy extraContainer port\n    ##\n    targetPort: 10902\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30905\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n\n    ## Additional ports to open for ThanosRuler service\n    additionalPorts: []\n\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a ServiceMonitor for the ThanosRuler service\n  ##\n  serviceMonitor:\n    ## If true, create a serviceMonitor for thanosRuler\n    ##\n    selfMonitor: true\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional Endpoints\n    ##\n    additionalEndpoints: []\n    # - port: oauth-metrics\n    #   path: /metrics\n\n  ## Settings affecting thanosRulerpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec\n  ##\n  thanosRulerSpec:\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods.\n    ##\n    podMetadata: {}\n\n    ## Image of ThanosRuler\n    ##\n    image:\n      registry: quay.io\n      repository: thanos/thanos\n      tag: v0.36.1\n      sha: \"\"\n\n    ## Namespaces to be selected for PrometheusRules discovery.\n    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage\n    ##\n    ruleNamespaceSelector: {}\n\n    ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## Define Log Format\n    # Use logfmt (default) or json logging\n    logFormat: logfmt\n\n    ## Log level for ThanosRuler to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n\n    ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 24h\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## Storage is the definition of how storage will be used by the ThanosRuler instances.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #   selector: {}\n\n    ## AlertmanagerConfig define configuration for connecting to alertmanager.\n    ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.\n    alertmanagersConfig:\n      # use existing secret, if configured, alertmanagersConfig.secret will not be used\n      existingSecret: {}\n        # name: \"\"\n        # key: \"\"\n      # will render alertmanagersConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when alertmanagersConfig.existingSecret is set\n      # https://thanos.io/tip/components/rule.md/#alertmanager\n      secret: {}\n        # alertmanagers:\n        # - api_version: v2\n        #   http_config:\n        #     basic_auth:\n        #       username: some_user\n        #       password: some_pass\n        #   static_configs:\n        #     - alertmanager.thanos.io\n        #   scheme: http\n        #   timeout: 10s\n\n    ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.\n    ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.\n    # alertmanagersUrl:\n\n    ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false\n    ##\n    externalPrefix:\n\n    ## If true, http://{{ template \"kube-prometheus-stack.thanosRuler.name\" . }}.{{ template \"kube-prometheus-stack.namespace\" . }}:{{ .Values.thanosRuler.service.port }}\n    ## will be used as value for externalPrefix\n    externalPrefixNilUsesHelmValues: true\n\n    ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## ObjectStorageConfig configures object storage in Thanos\n    objectStorageConfig:\n      # use existing secret, if configured, objectStorageConfig.secret will not be used\n      existingSecret: {}\n        # name: \"\"\n        # key: \"\"\n      # will render objectStorageConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when objectStorageConfig.existingSecret is set\n      # https://thanos.io/tip/thanos/storage.md/#s3\n      secret: {}\n        # type: S3\n        # config:\n        #   bucket: \"\"\n        #   endpoint: \"\"\n        #   region: \"\"\n        #   access_key: \"\"\n        #   secret_key: \"\"\n\n    ## Labels by name to drop before sending to alertmanager\n    ## Maps to the --alert.label-drop flag of thanos ruler.\n    alertDropLabels: []\n\n    ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.\n    ## Maps to the --query flag of thanos ruler.\n    queryEndpoints: []\n\n    ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.\n    ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.\n    queryConfig:\n      # use existing secret, if configured, queryConfig.secret will not be used\n      existingSecret: {}\n        # name: \"\"\n        # key: \"\"\n      # render queryConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when queryConfig.existingSecret is set\n      # https://thanos.io/tip/components/rule.md/#query-api\n      secret: {}\n        # - http_config:\n        #     basic_auth:\n        #       username: some_user\n        #       password: some_pass\n        #   static_configs:\n        #     - URL\n        #   scheme: http\n        #   timeout: 10s\n\n    ## Labels configure the external label pairs to ThanosRuler. A default replica\n    ## label `thanos_ruler_replica` will be always added as a label with the value\n    ## of the pod's name and it will be dropped in the alerts.\n    labels: {}\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Allows setting additional arguments for the ThanosRuler container\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosruler\n    ##\n    additionalArgs: []\n      # - name: remote-write.config\n      #   value: |-\n      #     \"remote_write\":\n      #     - \"name\": \"receiver-0\"\n      #       \"remote_timeout\": \"30s\"\n      #       \"url\": \"http://thanos-receiver-0.thanos-receiver:8081/api/v1/receive\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"soft\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the thanosRuler instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: thanos-ruler\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the ThanosRuler UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.\n    ##\n    containers: []\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## PortName to use for ThanosRuler.\n    ##\n    portName: \"web\"\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerwebspec\n    web: {}\n\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\n    additionalConfig: {}\n\n    ## Additional configuration which is not covered by the properties above.\n    ## Useful, if you need advanced templating\n    additionalConfigString: \"\"\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.\n##\ncleanPrometheusOperatorObjectNames: false\n\n## Extra manifests to deploy as an array\nextraManifests: []\n  # - apiVersion: v1\n  #   kind: ConfigMap\n  #   metadata:\n  #   labels:\n  #     name: prometheus-extra\n  #   data:\n  #     extra-data: \"value\"\n\n"
            ],
            "verify": false,
            "version": "67.4.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_cluster_role_binding_v1",
      "name": "eks_dev_cluster_role_binding",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "eks-dev-cluster-role-binding",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "eks-dev-cluster-role-binding",
                "resource_version": "992",
                "uid": "f59b43eb-b7e6-4e08-99e0-b1b681cd4b29"
              }
            ],
            "role_ref": [
              {
                "api_group": "rbac.authorization.k8s.io",
                "kind": "ClusterRole",
                "name": "eks-dev-cluster-role"
              }
            ],
            "subject": [
              {
                "api_group": "rbac.authorization.k8s.io",
                "kind": "Group",
                "name": "eks-dev-group",
                "namespace": "default"
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "kubernetes_cluster_role_v1.eks_dev_cluster_role",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_cluster_role_binding_v1",
      "name": "eks_readonly_cluster_role_binding",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "eks-readonly-cluster-role-binding",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "eks-readonly-cluster-role-binding",
                "resource_version": "991",
                "uid": "78b15372-64e3-41fb-bb7b-2fc7bb2d1d6d"
              }
            ],
            "role_ref": [
              {
                "api_group": "rbac.authorization.k8s.io",
                "kind": "ClusterRole",
                "name": "eks-readonly-cluster-role"
              }
            ],
            "subject": [
              {
                "api_group": "rbac.authorization.k8s.io",
                "kind": "Group",
                "name": "eks-readonly-group",
                "namespace": "default"
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "kubernetes_cluster_role_v1.eks_readonly_cluster_role",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_cluster_role_v1",
      "name": "eks_dev_cluster_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "aggregation_rule": [],
            "id": "eks-dev-cluster-role",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "eks-dev-cluster-role",
                "resource_version": "977",
                "uid": "d68cc286-7afa-46d4-a226-eadb21db4d8d"
              }
            ],
            "rule": [
              {
                "api_groups": [
                  ""
                ],
                "non_resource_urls": [],
                "resource_names": [],
                "resources": [
                  "namespaces",
                  "pods",
                  "nodes",
                  "eevnts",
                  "services"
                ],
                "verbs": [
                  "get",
                  "list"
                ]
              },
              {
                "api_groups": [
                  "apps"
                ],
                "non_resource_urls": [],
                "resource_names": [],
                "resources": [
                  "replicasets",
                  "statefulsets",
                  "deployments",
                  "daemonsets"
                ],
                "verbs": [
                  "get",
                  "list"
                ]
              },
              {
                "api_groups": [
                  "batch"
                ],
                "non_resource_urls": [],
                "resource_names": [],
                "resources": [
                  "jobs"
                ],
                "verbs": [
                  "get",
                  "list"
                ]
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_cluster_role_v1",
      "name": "eks_readonly_cluster_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "aggregation_rule": [],
            "id": "eks-readonly-cluster-role",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "eks-readonly-cluster-role",
                "resource_version": "978",
                "uid": "79077806-4ec1-4ff6-ba37-5bcd6515d335"
              }
            ],
            "rule": [
              {
                "api_groups": [
                  ""
                ],
                "non_resource_urls": [],
                "resource_names": [],
                "resources": [
                  "namespaces",
                  "pods",
                  "nodes",
                  "events",
                  "services"
                ],
                "verbs": [
                  "get",
                  "list"
                ]
              },
              {
                "api_groups": [
                  "apps"
                ],
                "non_resource_urls": [],
                "resource_names": [],
                "resources": [
                  "replicasets",
                  "statefulsets",
                  "deployments",
                  "daemonsets"
                ],
                "verbs": [
                  "get",
                  "list"
                ]
              },
              {
                "api_groups": [
                  "batch"
                ],
                "non_resource_urls": [],
                "resource_names": [],
                "resources": [
                  "jobs"
                ],
                "verbs": [
                  "get",
                  "list"
                ]
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_config_map_v1",
      "name": "aws-auth",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "binary_data": {},
            "data": {
              "mapRoles": "- \"groups\":\n  - \"system:bootstrappers\"\n  - \"system:nodes\"\n  \"rolearn\": \"arn:aws:iam::905418291613:role/dev-eks-nodegroup-role\"\n  \"username\": \"system:node:{{EC2PrivateDNSName}}\"\n- \"groups\":\n  - \"system:masters\"\n  \"rolearn\": \"arn:aws:iam::905418291613:role/eks-admin-role\"\n  \"username\": \"eks-admin\"\n- \"groups\":\n  - \"eks-readonly-group\"\n  \"rolearn\": \"arn:aws:iam::905418291613:role/eks-readonly-role\"\n  \"username\": \"eks-readonly\"\n- \"groups\":\n  - \"eks-dev-group\"\n  \"rolearn\": \"arn:aws:iam::905418291613:role/eks-dev-role\"\n  \"username\": \"eks-dev\"\n"
            },
            "id": "kube-system/aws-auth",
            "immutable": false,
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "aws-auth",
                "namespace": "kube-system",
                "resource_version": "995",
                "uid": "e1a6e3ec-f5bb-4a04-887a-a75d9f8ca395"
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_admin_role",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role.eks_dev_role",
            "aws_iam_role.eks_nodegroup_role",
            "aws_iam_role.eks_readonly_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_caller_identity.current",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "kubernetes_cluster_role_binding_v1.eks_dev_cluster_role_binding",
            "kubernetes_cluster_role_binding_v1.eks_readonly_cluster_role_binding",
            "kubernetes_cluster_role_v1.eks_dev_cluster_role",
            "kubernetes_cluster_role_v1.eks_readonly_cluster_role",
            "kubernetes_namespace.k8s_dev",
            "kubernetes_role_binding_v1.eksdev_role_binding",
            "kubernetes_role_v1.eksdev_role",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "argocd_ns",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "argocd",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "argocd",
                "resource_version": "979",
                "uid": "f82b6cec-6bfe-4ef9-b7a6-53a86f1a9007"
              }
            ],
            "timeouts": null,
            "wait_for_default_service_account": false
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "argocd_rollout_ns",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "argocd-rollout",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "argocd-rollout",
                "resource_version": "982",
                "uid": "05ecbad6-28dc-4af5-abd6-decb340f8ea2"
              }
            ],
            "timeouts": null,
            "wait_for_default_service_account": false
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "k8s_dev",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "dev",
                "resource_version": "980",
                "uid": "fbaf6c1a-e865-473d-be6d-763f7a2c69b0"
              }
            ],
            "timeouts": null,
            "wait_for_default_service_account": false
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "kong_ns",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "kong",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "kong",
                "resource_version": "981",
                "uid": "2a70ea86-85e1-4772-b8af-b374370ce973"
              }
            ],
            "timeouts": null,
            "wait_for_default_service_account": false
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_role_binding_v1",
      "name": "eksdev_role_binding",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev/eksdev-rolebinding",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "eksdev-rolebinding",
                "namespace": "dev",
                "resource_version": "994",
                "uid": "1f5f48a2-cf83-46a4-adb1-84eb6d2cdc8f"
              }
            ],
            "role_ref": [
              {
                "api_group": "rbac.authorization.k8s.io",
                "kind": "Role",
                "name": "eksdev-role"
              }
            ],
            "subject": [
              {
                "api_group": "rbac.authorization.k8s.io",
                "kind": "Group",
                "name": "eks-dev-group",
                "namespace": "default"
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "kubernetes_namespace.k8s_dev",
            "kubernetes_role_v1.eksdev_role",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_role_v1",
      "name": "eksdev_role",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "dev/eksdev-role",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "eksdev-role",
                "namespace": "dev",
                "resource_version": "993",
                "uid": "92e72fec-9be8-4e8b-8f7e-16a4f56bb7bf"
              }
            ],
            "rule": [
              {
                "api_groups": [
                  "",
                  "apps",
                  "extensions"
                ],
                "resource_names": [],
                "resources": [
                  "*"
                ],
                "verbs": [
                  "*"
                ]
              },
              {
                "api_groups": [
                  "batch"
                ],
                "resource_names": [],
                "resources": [
                  "cronjobs",
                  "jobs"
                ],
                "verbs": [
                  "*"
                ]
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.eks_cluster_role",
            "aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController",
            "data.aws_availability_zones.available",
            "data.aws_eks_cluster_auth.eks_cluster_auth",
            "kubernetes_namespace.k8s_dev",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "tls_private_key",
      "name": "tls",
      "provider": "provider[\"registry.terraform.io/hashicorp/tls\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "algorithm": "RSA",
            "ecdsa_curve": "P224",
            "id": "909f29f07a932e17d744f68aa99094b152e392c2",
            "private_key_openssh": "-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABFwAAAAdz\nc2gtcnNhAAAAAwEAAQAAAQEAxRp96pEIgFFsFfqkpeS9x++Wm9MUOqjTMwQGBAFC\nvaLWvuIQ/760rYxmVzQp01kwNFmrFojyQiDcNDmDCQb4X6Nt4PKXPJOnOgJy1fAw\nY9lIRRO/deQUTMZGrAZLhiaf77rEM7ZhEzekiomqk4stpKJkAFaLuMZVRLf94of9\nIl+9Tboiyfmsdv1nY34lEZ09ij2d9M44BnaKppOP6qJnk4sTaIbEVqvVsRgHlcZF\n4emf/2nAKfFZLys8zTwh0p3pLkAg3tyrUVDzdOzz0QeZeCpsciiukO1LP0j5VSky\ngV55OW+YVpAtqAjGBHtrR2BI15AqozXZxwp//hheu7U//wAAA7ilcj54pXI+eAAA\nAAdzc2gtcnNhAAABAQDFGn3qkQiAUWwV+qSl5L3H75ab0xQ6qNMzBAYEAUK9ota+\n4hD/vrStjGZXNCnTWTA0WasWiPJCINw0OYMJBvhfo23g8pc8k6c6AnLV8DBj2UhF\nE7915BRMxkasBkuGJp/vusQztmETN6SKiaqTiy2komQAVou4xlVEt/3ih/0iX71N\nuiLJ+ax2/WdjfiURnT2KPZ30zjgGdoqmk4/qomeTixNohsRWq9WxGAeVxkXh6Z//\nacAp8VkvKzzNPCHSnekuQCDe3KtRUPN07PPRB5l4KmxyKK6Q7Us/SPlVKTKBXnk5\nb5hWkC2oCMYEe2tHYEjXkCqjNdnHCn/+GF67tT//AAAAAwEAAQAAAQBtjsgmte8P\nxDJcFTGntHl4xMfmXXGFGYordTUSzNCYMAdFmsrwHMS5Iq1QE4KcI5sTvjPHFt/x\nueosbTtUrTcpkeHlg3JlDX2KVQoXuKZtGb1+B/gIQ3q1IhDuVaFtM9L6ocmlJN7Y\nm7kt+E+EK3x3YnxLY8/OKRAEXnEUlZoynCKg7kDc+erovd4R14N7PIXlwv8CgyAK\np30Im8YnU9aoCZ6vcs/1NU4JWpoX9n3ReKr0T03pD7kmXmDJ9LVRj5Pk5zwwgDSQ\naBbOWZJrjn6JU4uFrRKhyZ0AyAUuRrNpJjbCYoZ1PIqzHNSXnj8Yge1JBhbSE+0u\nRU5yfU9YG85hAAAAgQCrroJCXmldFZ1SKN5iURzTImMypI37fF3cCDCui+4uGLTb\nmmGrvixzwN8HIraLDaITQOdQy6XDKltQfOwbdNRf37+uCqT1+4ujqv6IKdinKyeh\n5RW0JVy5N2NUVnkulUKRVdqWJu9HKHC93KaRnydv+7cUzRdC2y14RI9tpxv6EgAA\nAIEA7tLxOkPs3uQGWrUb6u+oki7zHVkoKmg5YAdpLlM5BxT7h8QeJ026ur2m3Q/F\nnXjiipIXUGGmD6tl4/RsiVpa6KUcHdEzpiEFGiKyiw0G0t1+NaUisuWBwWvhX5kF\nfrg725fI7vQNlWwXLq9j9Hvmr3AbSPJ0nMNaqT/Ul4ThmmUAAACBANNHa9KaYd2M\n1OA2e30n/9lpyiVEaFVQmguAxc+71Bj7l9jTUGonveTKu+MTyVo8ZjStw0/Rh+fJ\n7/fWC6waVs9BTAiCM9lqrTpz/GSDV9GKU3bMoCpll64SOVE48yTfzwttBQWWjJo0\nTRBfODGDBxf2vSd1eqiiEhQ/95Ym3biTAAAAAAEC\n-----END OPENSSH PRIVATE KEY-----\n",
            "private_key_pem": "-----BEGIN RSA PRIVATE KEY-----\nMIIEpAIBAAKCAQEAxRp96pEIgFFsFfqkpeS9x++Wm9MUOqjTMwQGBAFCvaLWvuIQ\n/760rYxmVzQp01kwNFmrFojyQiDcNDmDCQb4X6Nt4PKXPJOnOgJy1fAwY9lIRRO/\ndeQUTMZGrAZLhiaf77rEM7ZhEzekiomqk4stpKJkAFaLuMZVRLf94of9Il+9Tboi\nyfmsdv1nY34lEZ09ij2d9M44BnaKppOP6qJnk4sTaIbEVqvVsRgHlcZF4emf/2nA\nKfFZLys8zTwh0p3pLkAg3tyrUVDzdOzz0QeZeCpsciiukO1LP0j5VSkygV55OW+Y\nVpAtqAjGBHtrR2BI15AqozXZxwp//hheu7U//wIDAQABAoIBAG2OyCa17w/EMlwV\nMae0eXjEx+ZdcYUZiit1NRLM0JgwB0WayvAcxLkirVATgpwjmxO+M8cW3/G56ixt\nO1StNymR4eWDcmUNfYpVChe4pm0ZvX4H+AhDerUiEO5VoW0z0vqhyaUk3tibuS34\nT4QrfHdifEtjz84pEARecRSVmjKcIqDuQNz56ui93hHXg3s8heXC/wKDIAqnfQib\nxidT1qgJnq9yz/U1Tglamhf2fdF4qvRPTekPuSZeYMn0tVGPk+TnPDCANJBoFs5Z\nkmuOfolTi4WtEqHJnQDIBS5Gs2kmNsJihnU8irMc1JeePxiB7UkGFtIT7S5FTnJ9\nT1gbzmECgYEA7tLxOkPs3uQGWrUb6u+oki7zHVkoKmg5YAdpLlM5BxT7h8QeJ026\nur2m3Q/FnXjiipIXUGGmD6tl4/RsiVpa6KUcHdEzpiEFGiKyiw0G0t1+NaUisuWB\nwWvhX5kFfrg725fI7vQNlWwXLq9j9Hvmr3AbSPJ0nMNaqT/Ul4ThmmUCgYEA00dr\n0pph3YzU4DZ7fSf/2WnKJURoVVCaC4DFz7vUGPuX2NNQaie95Mq74xPJWjxmNK3D\nT9GH58nv99YLrBpWz0FMCIIz2WqtOnP8ZINX0YpTdsygKmWXrhI5UTjzJN/PC20F\nBZaMmjRNEF84MYMHF/a9J3V6qKISFD/3libduJMCgYA14b2vCZgtiYCtgmL0FHIR\n2SncmzrmpnEba1CdtQUOxfsh3Gt5Lp5Md3FoOqC5MIbcAK3l4sIWkvMABStfqdUM\n3AOF7qcaeiSuitmBacT6WiKZc9JqTkGCJBbK/Lkyp9pJZutcjg9qgOKSE9wXUsTv\nG/nr8VfB7olL2izaqo5DyQKBgQDSU/V40QvAHKUHF/XTYHCJJZGBjBo5ZGYWMXm7\nFYdN35kI10TkiO+3xkNJCbeXRy2QH7Oh0Dt1ekiT9tfj2sy0wpJIniWzuUj+Odz2\nyvIvvX7dc1O3IS/gsG+y9fjHqq5Y94zaRUdvc8WMGN6+G2yCZLVlhMQ5AC7zTcLS\nDZsIVQKBgQCrroJCXmldFZ1SKN5iURzTImMypI37fF3cCDCui+4uGLTbmmGrvixz\nwN8HIraLDaITQOdQy6XDKltQfOwbdNRf37+uCqT1+4ujqv6IKdinKyeh5RW0JVy5\nN2NUVnkulUKRVdqWJu9HKHC93KaRnydv+7cUzRdC2y14RI9tpxv6Eg==\n-----END RSA PRIVATE KEY-----\n",
            "private_key_pem_pkcs8": "-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDFGn3qkQiAUWwV\n+qSl5L3H75ab0xQ6qNMzBAYEAUK9ota+4hD/vrStjGZXNCnTWTA0WasWiPJCINw0\nOYMJBvhfo23g8pc8k6c6AnLV8DBj2UhFE7915BRMxkasBkuGJp/vusQztmETN6SK\niaqTiy2komQAVou4xlVEt/3ih/0iX71NuiLJ+ax2/WdjfiURnT2KPZ30zjgGdoqm\nk4/qomeTixNohsRWq9WxGAeVxkXh6Z//acAp8VkvKzzNPCHSnekuQCDe3KtRUPN0\n7PPRB5l4KmxyKK6Q7Us/SPlVKTKBXnk5b5hWkC2oCMYEe2tHYEjXkCqjNdnHCn/+\nGF67tT//AgMBAAECggEAbY7IJrXvD8QyXBUxp7R5eMTH5l1xhRmKK3U1EszQmDAH\nRZrK8BzEuSKtUBOCnCObE74zxxbf8bnqLG07VK03KZHh5YNyZQ19ilUKF7imbRm9\nfgf4CEN6tSIQ7lWhbTPS+qHJpSTe2Ju5LfhPhCt8d2J8S2PPzikQBF5xFJWaMpwi\noO5A3Pnq6L3eEdeDezyF5cL/AoMgCqd9CJvGJ1PWqAmer3LP9TVOCVqaF/Z90Xiq\n9E9N6Q+5Jl5gyfS1UY+T5Oc8MIA0kGgWzlmSa45+iVOLha0SocmdAMgFLkazaSY2\nwmKGdTyKsxzUl54/GIHtSQYW0hPtLkVOcn1PWBvOYQKBgQDu0vE6Q+ze5AZatRvq\n76iSLvMdWSgqaDlgB2kuUzkHFPuHxB4nTbq6vabdD8WdeOKKkhdQYaYPq2Xj9GyJ\nWlropRwd0TOmIQUaIrKLDQbS3X41pSKy5YHBa+FfmQV+uDvbl8ju9A2VbBcur2P0\ne+avcBtI8nScw1qpP9SXhOGaZQKBgQDTR2vSmmHdjNTgNnt9J//ZacolRGhVUJoL\ngMXPu9QY+5fY01BqJ73kyrvjE8laPGY0rcNP0Yfnye/31gusGlbPQUwIgjPZaq06\nc/xkg1fRilN2zKAqZZeuEjlROPMk388LbQUFloyaNE0QXzgxgwcX9r0ndXqoohIU\nP/eWJt24kwKBgDXhva8JmC2JgK2CYvQUchHZKdybOuamcRtrUJ21BQ7F+yHca3ku\nnkx3cWg6oLkwhtwAreXiwhaS8wAFK1+p1QzcA4Xupxp6JK6K2YFpxPpaIplz0mpO\nQYIkFsr8uTKn2klm61yOD2qA4pIT3BdSxO8b+evxV8HuiUvaLNqqjkPJAoGBANJT\n9XjRC8AcpQcX9dNgcIklkYGMGjlkZhYxebsVh03fmQjXROSI77fGQ0kJt5dHLZAf\ns6HQO3V6SJP21+PazLTCkkieJbO5SP453PbK8i+9ft1zU7chL+Cwb7L1+Meqrlj3\njNpFR29zxYwY3r4bbIJktWWExDkALvNNwtINmwhVAoGBAKuugkJeaV0VnVIo3mJR\nHNMiYzKkjft8XdwIMK6L7i4YtNuaYau+LHPA3wcitosNohNA51DLpcMqW1B87Bt0\n1F/fv64KpPX7i6Oq/ogp2KcrJ6HlFbQlXLk3Y1RWeS6VQpFV2pYm70cocL3cppGf\nJ2/7txTNF0LbLXhEj22nG/oS\n-----END PRIVATE KEY-----\n",
            "public_key_fingerprint_md5": "47:ff:88:c7:42:e8:34:cc:61:b5:00:d6:d7:ff:e6:7f",
            "public_key_fingerprint_sha256": "SHA256:a8kF0wGo2EVRKDcur4nRVukEILg2dFTHf990up9CmTM",
            "public_key_openssh": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDFGn3qkQiAUWwV+qSl5L3H75ab0xQ6qNMzBAYEAUK9ota+4hD/vrStjGZXNCnTWTA0WasWiPJCINw0OYMJBvhfo23g8pc8k6c6AnLV8DBj2UhFE7915BRMxkasBkuGJp/vusQztmETN6SKiaqTiy2komQAVou4xlVEt/3ih/0iX71NuiLJ+ax2/WdjfiURnT2KPZ30zjgGdoqmk4/qomeTixNohsRWq9WxGAeVxkXh6Z//acAp8VkvKzzNPCHSnekuQCDe3KtRUPN07PPRB5l4KmxyKK6Q7Us/SPlVKTKBXnk5b5hWkC2oCMYEe2tHYEjXkCqjNdnHCn/+GF67tT//\n",
            "public_key_pem": "-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxRp96pEIgFFsFfqkpeS9\nx++Wm9MUOqjTMwQGBAFCvaLWvuIQ/760rYxmVzQp01kwNFmrFojyQiDcNDmDCQb4\nX6Nt4PKXPJOnOgJy1fAwY9lIRRO/deQUTMZGrAZLhiaf77rEM7ZhEzekiomqk4st\npKJkAFaLuMZVRLf94of9Il+9Tboiyfmsdv1nY34lEZ09ij2d9M44BnaKppOP6qJn\nk4sTaIbEVqvVsRgHlcZF4emf/2nAKfFZLys8zTwh0p3pLkAg3tyrUVDzdOzz0QeZ\neCpsciiukO1LP0j5VSkygV55OW+YVpAtqAjGBHtrR2BI15AqozXZxwp//hheu7U/\n/wIDAQAB\n-----END PUBLIC KEY-----\n",
            "rsa_bits": 2048
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_default_network_acl",
      "name": "this",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:network-acl/acl-0d91261f9d7c5ab71",
            "default_network_acl_id": "acl-0d91261f9d7c5ab71",
            "egress": [
              {
                "action": "allow",
                "cidr_block": "",
                "from_port": 0,
                "icmp_code": 0,
                "icmp_type": 0,
                "ipv6_cidr_block": "::/0",
                "protocol": "-1",
                "rule_no": 101,
                "to_port": 0
              },
              {
                "action": "allow",
                "cidr_block": "0.0.0.0/0",
                "from_port": 0,
                "icmp_code": 0,
                "icmp_type": 0,
                "ipv6_cidr_block": "",
                "protocol": "-1",
                "rule_no": 100,
                "to_port": 0
              }
            ],
            "id": "acl-0d91261f9d7c5ab71",
            "ingress": [
              {
                "action": "allow",
                "cidr_block": "",
                "from_port": 0,
                "icmp_code": 0,
                "icmp_type": 0,
                "ipv6_cidr_block": "::/0",
                "protocol": "-1",
                "rule_no": 101,
                "to_port": 0
              },
              {
                "action": "allow",
                "cidr_block": "0.0.0.0/0",
                "from_port": 0,
                "icmp_code": 0,
                "icmp_type": 0,
                "ipv6_cidr_block": "",
                "protocol": "-1",
                "rule_no": 100,
                "to_port": 0
              }
            ],
            "owner_id": "905418291613",
            "subnet_ids": [
              "subnet-00c603222b651f8da",
              "subnet-01085b2d3412f3ebf",
              "subnet-089c4360de112d825",
              "subnet-0a00bb84a17478e9a"
            ],
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-default"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-default"
            },
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.vpc.aws_vpc.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_default_route_table",
      "name": "default",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:route-table/rtb-0af4dfcbeb35b8fc7",
            "default_route_table_id": "rtb-0af4dfcbeb35b8fc7",
            "id": "rtb-0af4dfcbeb35b8fc7",
            "owner_id": "905418291613",
            "propagating_vgws": [],
            "route": [],
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-default"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-default"
            },
            "timeouts": {
              "create": "5m",
              "update": "5m"
            },
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsInVwZGF0ZSI6MzAwMDAwMDAwMDAwfX0=",
          "dependencies": [
            "module.vpc.aws_vpc.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_default_security_group",
      "name": "this",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:security-group/sg-0d32169a2d9196be7",
            "description": "default VPC security group",
            "egress": [],
            "id": "sg-0d32169a2d9196be7",
            "ingress": [],
            "name": "default",
            "name_prefix": "",
            "owner_id": "905418291613",
            "revoke_rules_on_delete": false,
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-default"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-default"
            },
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "module.vpc.aws_vpc.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_eip",
      "name": "nat",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "address": null,
            "allocation_id": "eipalloc-090762d6a8b2a6e95",
            "arn": "arn:aws:ec2:us-east-1:905418291613:elastic-ip/eipalloc-090762d6a8b2a6e95",
            "associate_with_private_ip": null,
            "association_id": "eipassoc-07bbcca718b3b919a",
            "carrier_ip": "",
            "customer_owned_ip": "",
            "customer_owned_ipv4_pool": "",
            "domain": "vpc",
            "id": "eipalloc-090762d6a8b2a6e95",
            "instance": "",
            "ipam_pool_id": null,
            "network_border_group": "us-east-1",
            "network_interface": "eni-08d0d85727a5534e8",
            "private_dns": "ip-10-0-1-159.ec2.internal",
            "private_ip": "10.0.1.159",
            "ptr_record": "",
            "public_dns": "ec2-3-219-224-152.compute-1.amazonaws.com",
            "public_ip": "3.219.224.152",
            "public_ipv4_pool": "amazon",
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-us-east-1a"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-us-east-1a"
            },
            "timeouts": null,
            "vpc": true
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjoxODAwMDAwMDAwMDAsInJlYWQiOjkwMDAwMDAwMDAwMCwidXBkYXRlIjozMDAwMDAwMDAwMDB9fQ==",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_internet_gateway.this",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_internet_gateway",
      "name": "this",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:internet-gateway/igw-09c0d725dc3fae3fe",
            "id": "igw-09c0d725dc3fae3fe",
            "owner_id": "905418291613",
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc"
            },
            "timeouts": null,
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH19",
          "dependencies": [
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_nat_gateway",
      "name": "this",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "allocation_id": "eipalloc-090762d6a8b2a6e95",
            "association_id": "eipassoc-07bbcca718b3b919a",
            "connectivity_type": "public",
            "id": "nat-03207e3190b737a48",
            "network_interface_id": "eni-08d0d85727a5534e8",
            "private_ip": "10.0.1.159",
            "public_ip": "3.219.224.152",
            "secondary_allocation_ids": [],
            "secondary_private_ip_address_count": 0,
            "secondary_private_ip_addresses": [],
            "subnet_id": "subnet-089c4360de112d825",
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-us-east-1a"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-us-east-1a"
            },
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTgwMDAwMDAwMDAwMCwidXBkYXRlIjo2MDAwMDAwMDAwMDB9fQ==",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_eip.nat",
            "module.vpc.aws_internet_gateway.this",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_route",
      "name": "private_nat_gateway",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "carrier_gateway_id": "",
            "core_network_arn": "",
            "destination_cidr_block": "0.0.0.0/0",
            "destination_ipv6_cidr_block": "",
            "destination_prefix_list_id": "",
            "egress_only_gateway_id": "",
            "gateway_id": "",
            "id": "r-rtb-0fd9a529bf5727fd51080289494",
            "instance_id": "",
            "instance_owner_id": "",
            "local_gateway_id": "",
            "nat_gateway_id": "nat-03207e3190b737a48",
            "network_interface_id": "",
            "origin": "CreateRoute",
            "route_table_id": "rtb-0fd9a529bf5727fd5",
            "state": "active",
            "timeouts": {
              "create": "5m",
              "delete": null,
              "update": null
            },
            "transit_gateway_id": "",
            "vpc_endpoint_id": "",
            "vpc_peering_connection_id": ""
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_eip.nat",
            "module.vpc.aws_internet_gateway.this",
            "module.vpc.aws_nat_gateway.this",
            "module.vpc.aws_route_table.private",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_route",
      "name": "public_internet_gateway",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "carrier_gateway_id": "",
            "core_network_arn": "",
            "destination_cidr_block": "0.0.0.0/0",
            "destination_ipv6_cidr_block": "",
            "destination_prefix_list_id": "",
            "egress_only_gateway_id": "",
            "gateway_id": "igw-09c0d725dc3fae3fe",
            "id": "r-rtb-0ff69f77bee200b8a1080289494",
            "instance_id": "",
            "instance_owner_id": "",
            "local_gateway_id": "",
            "nat_gateway_id": "",
            "network_interface_id": "",
            "origin": "CreateRoute",
            "route_table_id": "rtb-0ff69f77bee200b8a",
            "state": "active",
            "timeouts": {
              "create": "5m",
              "delete": null,
              "update": null
            },
            "transit_gateway_id": "",
            "vpc_endpoint_id": "",
            "vpc_peering_connection_id": ""
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "module.vpc.aws_internet_gateway.this",
            "module.vpc.aws_route_table.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_route_table",
      "name": "private",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:route-table/rtb-0fd9a529bf5727fd5",
            "id": "rtb-0fd9a529bf5727fd5",
            "owner_id": "905418291613",
            "propagating_vgws": [],
            "route": [
              {
                "carrier_gateway_id": "",
                "cidr_block": "0.0.0.0/0",
                "core_network_arn": "",
                "destination_prefix_list_id": "",
                "egress_only_gateway_id": "",
                "gateway_id": "",
                "ipv6_cidr_block": "",
                "local_gateway_id": "",
                "nat_gateway_id": "nat-03207e3190b737a48",
                "network_interface_id": "",
                "transit_gateway_id": "",
                "vpc_endpoint_id": "",
                "vpc_peering_connection_id": ""
              }
            ],
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-private"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-private"
            },
            "timeouts": null,
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_route_table",
      "name": "public",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:route-table/rtb-0ff69f77bee200b8a",
            "id": "rtb-0ff69f77bee200b8a",
            "owner_id": "905418291613",
            "propagating_vgws": [],
            "route": [
              {
                "carrier_gateway_id": "",
                "cidr_block": "0.0.0.0/0",
                "core_network_arn": "",
                "destination_prefix_list_id": "",
                "egress_only_gateway_id": "",
                "gateway_id": "igw-09c0d725dc3fae3fe",
                "ipv6_cidr_block": "",
                "local_gateway_id": "",
                "nat_gateway_id": "",
                "network_interface_id": "",
                "transit_gateway_id": "",
                "vpc_endpoint_id": "",
                "vpc_peering_connection_id": ""
              }
            ],
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-public"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-public"
            },
            "timeouts": null,
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "private",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-031280db144d40dba",
            "route_table_id": "rtb-0fd9a529bf5727fd5",
            "subnet_id": "subnet-00c603222b651f8da",
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_route_table.private",
            "module.vpc.aws_subnet.private",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        },
        {
          "index_key": 1,
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-059e4424bddfb2a26",
            "route_table_id": "rtb-0fd9a529bf5727fd5",
            "subnet_id": "subnet-0a00bb84a17478e9a",
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_route_table.private",
            "module.vpc.aws_subnet.private",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "public",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-03b4732dd3249a973",
            "route_table_id": "rtb-0ff69f77bee200b8a",
            "subnet_id": "subnet-089c4360de112d825",
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_route_table.public",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        },
        {
          "index_key": 1,
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-0b146323781114a22",
            "route_table_id": "rtb-0ff69f77bee200b8a",
            "subnet_id": "subnet-01085b2d3412f3ebf",
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_route_table.public",
            "module.vpc.aws_subnet.public",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_subnet",
      "name": "private",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:subnet/subnet-00c603222b651f8da",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "us-east-1a",
            "availability_zone_id": "use1-az1",
            "cidr_block": "10.0.101.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_lni_at_device_index": 0,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-00c603222b651f8da",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": false,
            "outpost_arn": "",
            "owner_id": "905418291613",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-private-us-east-1a",
              "Type": "Private Subnets",
              "kubernetes.io/cluster/ekscluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-private-us-east-1a",
              "Type": "Private Subnets",
              "kubernetes.io/cluster/ekscluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "timeouts": null,
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        },
        {
          "index_key": 1,
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:subnet/subnet-0a00bb84a17478e9a",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "us-east-1b",
            "availability_zone_id": "use1-az2",
            "cidr_block": "10.0.102.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_lni_at_device_index": 0,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-0a00bb84a17478e9a",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": false,
            "outpost_arn": "",
            "owner_id": "905418291613",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-private-us-east-1b",
              "Type": "Private Subnets",
              "kubernetes.io/cluster/ekscluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-private-us-east-1b",
              "Type": "Private Subnets",
              "kubernetes.io/cluster/ekscluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "timeouts": null,
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_subnet",
      "name": "public",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:subnet/subnet-089c4360de112d825",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "us-east-1a",
            "availability_zone_id": "use1-az1",
            "cidr_block": "10.0.1.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_lni_at_device_index": 0,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-089c4360de112d825",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": true,
            "outpost_arn": "",
            "owner_id": "905418291613",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-public-us-east-1a",
              "Type": "Public Subnets",
              "kubernetes.io/cluster/ekscluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-public-us-east-1a",
              "Type": "Public Subnets",
              "kubernetes.io/cluster/ekscluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "timeouts": null,
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        },
        {
          "index_key": 1,
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:subnet/subnet-01085b2d3412f3ebf",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "us-east-1b",
            "availability_zone_id": "use1-az2",
            "cidr_block": "10.0.2.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_lni_at_device_index": 0,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-01085b2d3412f3ebf",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": true,
            "outpost_arn": "",
            "owner_id": "905418291613",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc-public-us-east-1b",
              "Type": "Public Subnets",
              "kubernetes.io/cluster/ekscluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc-public-us-east-1b",
              "Type": "Public Subnets",
              "kubernetes.io/cluster/ekscluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "timeouts": null,
            "vpc_id": "vpc-008c185615b74dff3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "data.aws_availability_zones.available",
            "module.vpc.aws_vpc.this",
            "module.vpc.aws_vpc_ipv4_cidr_block_association.this"
          ]
        }
      ]
    },
    {
      "module": "module.vpc",
      "mode": "managed",
      "type": "aws_vpc",
      "name": "this",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:905418291613:vpc/vpc-008c185615b74dff3",
            "assign_generated_ipv6_cidr_block": false,
            "cidr_block": "10.0.0.0/16",
            "default_network_acl_id": "acl-0d91261f9d7c5ab71",
            "default_route_table_id": "rtb-0af4dfcbeb35b8fc7",
            "default_security_group_id": "sg-0d32169a2d9196be7",
            "dhcp_options_id": "dopt-07c84c7610c052b8d",
            "enable_dns_hostnames": true,
            "enable_dns_support": true,
            "enable_network_address_usage_metrics": false,
            "id": "vpc-008c185615b74dff3",
            "instance_tenancy": "default",
            "ipv4_ipam_pool_id": null,
            "ipv4_netmask_length": null,
            "ipv6_association_id": "",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_network_border_group": "",
            "ipv6_ipam_pool_id": "",
            "ipv6_netmask_length": 0,
            "main_route_table_id": "rtb-0af4dfcbeb35b8fc7",
            "owner_id": "905418291613",
            "tags": {
              "Environment": "dev",
              "Name": "dev-vpc"
            },
            "tags_all": {
              "Environment": "dev",
              "Name": "dev-vpc"
            }
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    }
  ],
  "check_results": []
}
